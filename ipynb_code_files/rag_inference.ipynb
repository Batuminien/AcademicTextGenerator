{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f96eefb71bfa458f860df782ddb1383d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e198974641d4818997393ccc554913f","IPY_MODEL_e0c503180f76401db6ec62f1a22786f0","IPY_MODEL_a1e4448e14c940d5a6d1a1fd490ec80f"],"layout":"IPY_MODEL_04c74636dba0412eb5705d13be075e58"}},"8e198974641d4818997393ccc554913f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7dd1d92f09d40fbbb5757910dcc8007","placeholder":"​","style":"IPY_MODEL_3e397761839c427bb78ff6f520cdd93c","value":"Fetching 4 files: 100%"}},"e0c503180f76401db6ec62f1a22786f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef1165bf2a224649880a2057f1d68716","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d862ffd9fa9e4e8b92b29ea8633140c9","value":4}},"a1e4448e14c940d5a6d1a1fd490ec80f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3211c392f764cd89559b2a999901217","placeholder":"​","style":"IPY_MODEL_b713846c20064d149077200bdb8d3cd3","value":" 4/4 [00:47&lt;00:00, 47.83s/it]"}},"04c74636dba0412eb5705d13be075e58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7dd1d92f09d40fbbb5757910dcc8007":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e397761839c427bb78ff6f520cdd93c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef1165bf2a224649880a2057f1d68716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d862ffd9fa9e4e8b92b29ea8633140c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3211c392f764cd89559b2a999901217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b713846c20064d149077200bdb8d3cd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02541a9b6fca4ee597877cb4c4916ced":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa2247e47fd04c8a9448a6abf2bb89eb","IPY_MODEL_9c27508cc9eb46e2933d98c5ff0c4473","IPY_MODEL_be11e4978c414ffd8ad3884acc66828e"],"layout":"IPY_MODEL_56ba9fa881124b3683fc581951c805b8"}},"aa2247e47fd04c8a9448a6abf2bb89eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c17e78d5825499ab8884a53bc1502e1","placeholder":"​","style":"IPY_MODEL_6c244671018e45298102449421ed5e62","value":"model-00003-of-00004.safetensors: 100%"}},"9c27508cc9eb46e2933d98c5ff0c4473":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b67a519d7f6480398aef26da47a550e","max":3864726424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c000ad2c5ca4a3e8447095aa4592ccf","value":3864726424}},"be11e4978c414ffd8ad3884acc66828e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11f0ba061a544fcea377bc4478b83748","placeholder":"​","style":"IPY_MODEL_3b84365e813e4699a2fbe0d8f59cc7aa","value":" 3.86G/3.86G [00:36&lt;00:00, 141MB/s]"}},"56ba9fa881124b3683fc581951c805b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c17e78d5825499ab8884a53bc1502e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c244671018e45298102449421ed5e62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b67a519d7f6480398aef26da47a550e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c000ad2c5ca4a3e8447095aa4592ccf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"11f0ba061a544fcea377bc4478b83748":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b84365e813e4699a2fbe0d8f59cc7aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36f61d00ad194c74913081afd4d2d5ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f0390f1544b4204b64cb064c02060ac","IPY_MODEL_0701a592c33148bf8466aa0d75a25e8a","IPY_MODEL_31c55b1053cf4d32bdd2322e8f9fd97a"],"layout":"IPY_MODEL_7c86297973344673ba0eb205457ed3d6"}},"1f0390f1544b4204b64cb064c02060ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62bb374932ab4c8c90b7258706643daa","placeholder":"​","style":"IPY_MODEL_c00a0e9fcdde423aa24f23ce46bea9a2","value":"model-00004-of-00004.safetensors: 100%"}},"0701a592c33148bf8466aa0d75a25e8a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0d5589790074f12a436e53ae37c72f8","max":3556377672,"min":0,"orientation":"horizontal","style":"IPY_MODEL_135c4fc411244a578f61db3c34c53bb3","value":3556377672}},"31c55b1053cf4d32bdd2322e8f9fd97a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee7e3db6e4854c07a05c545222dc78a1","placeholder":"​","style":"IPY_MODEL_a81f0fafe59649ae9775ef7419b18b11","value":" 3.56G/3.56G [00:39&lt;00:00, 172MB/s]"}},"7c86297973344673ba0eb205457ed3d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62bb374932ab4c8c90b7258706643daa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c00a0e9fcdde423aa24f23ce46bea9a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0d5589790074f12a436e53ae37c72f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"135c4fc411244a578f61db3c34c53bb3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee7e3db6e4854c07a05c545222dc78a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a81f0fafe59649ae9775ef7419b18b11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca4aa907617e4ad984cc79e29c673072":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbad8675efc742af9efa575ed73ea7b9","IPY_MODEL_9024ecc8d329481a942c59baa2970187","IPY_MODEL_94f148ddea2b4926aed0fcc863d58c8d"],"layout":"IPY_MODEL_c76c97ec03ab4909b492338f92ce3e15"}},"fbad8675efc742af9efa575ed73ea7b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd1a6fa912f04405afcfa4c01bf080ab","placeholder":"​","style":"IPY_MODEL_f4638cdcca51413b9ae0ae7bafa0d7e0","value":"model-00002-of-00004.safetensors: 100%"}},"9024ecc8d329481a942c59baa2970187":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d202563c7584484ba2df79251fba9128","max":3864726352,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c8c837bfac34736be16e48e85bdd9dc","value":3864726352}},"94f148ddea2b4926aed0fcc863d58c8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1ad481d775b4d5f8a943325464ec9c3","placeholder":"​","style":"IPY_MODEL_6bf07c0352924cc7894f1f855115e622","value":" 3.86G/3.86G [00:32&lt;00:00, 121MB/s]"}},"c76c97ec03ab4909b492338f92ce3e15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd1a6fa912f04405afcfa4c01bf080ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4638cdcca51413b9ae0ae7bafa0d7e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d202563c7584484ba2df79251fba9128":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c8c837bfac34736be16e48e85bdd9dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1ad481d775b4d5f8a943325464ec9c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bf07c0352924cc7894f1f855115e622":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e8a1780a7af4b5e9e546c7f128bb4e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a7c2659d85e4dcaa2241b4dcd187fcf","IPY_MODEL_30f1f31f1cfe4a4dac6a7da9bd4557d7","IPY_MODEL_4e23c29160a3434b92e326eaf77429e0"],"layout":"IPY_MODEL_2b572fbad5424420b77c4a8e15ac2cd5"}},"8a7c2659d85e4dcaa2241b4dcd187fcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d781a56c106d40258b17f4eac51dce3e","placeholder":"​","style":"IPY_MODEL_3f1e1969d0574ed893870d634f9df227","value":"model-00001-of-00004.safetensors: 100%"}},"30f1f31f1cfe4a4dac6a7da9bd4557d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92402e95a227442880a1f9fdc04a3157","max":3945441440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b849c9916b943798a8b0b7c62f52869","value":3945441440}},"4e23c29160a3434b92e326eaf77429e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcf0c9e666ed4f53a4673bd03f91689c","placeholder":"​","style":"IPY_MODEL_ddaa5b9f0b9f4daaa8a898676384ca5c","value":" 3.95G/3.95G [00:47&lt;00:00, 411MB/s]"}},"2b572fbad5424420b77c4a8e15ac2cd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d781a56c106d40258b17f4eac51dce3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f1e1969d0574ed893870d634f9df227":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92402e95a227442880a1f9fdc04a3157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b849c9916b943798a8b0b7c62f52869":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bcf0c9e666ed4f53a4673bd03f91689c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddaa5b9f0b9f4daaa8a898676384ca5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d9a21e65a5c42369f762db814a166cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2d9f2379a724dffb92ea1a475c12676","IPY_MODEL_56052ec8c7e84d9684281d061b68e7bb","IPY_MODEL_613a018f09ec4de3801e64455c1c461a"],"layout":"IPY_MODEL_a7f77fee35fd4ee2aa91fcb60af59743"}},"a2d9f2379a724dffb92ea1a475c12676":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17eb070b1c1a4eba8c3442586ffe4b63","placeholder":"​","style":"IPY_MODEL_26d4df2eafbe42e8826ae11f66129f85","value":"Loading checkpoint shards: 100%"}},"56052ec8c7e84d9684281d061b68e7bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd1f639e10ec4b55bbe616a2f8220917","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab196f508a73403f86850dd43074d7fb","value":4}},"613a018f09ec4de3801e64455c1c461a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_012b68968f98433d9dbb97f684ccf461","placeholder":"​","style":"IPY_MODEL_d3bd9679925d4fad837319beeba0e15d","value":" 4/4 [00:14&lt;00:00,  3.83s/it]"}},"a7f77fee35fd4ee2aa91fcb60af59743":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17eb070b1c1a4eba8c3442586ffe4b63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26d4df2eafbe42e8826ae11f66129f85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd1f639e10ec4b55bbe616a2f8220917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab196f508a73403f86850dd43074d7fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"012b68968f98433d9dbb97f684ccf461":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3bd9679925d4fad837319beeba0e15d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e031e77e82b473e8dd6b0499ddfa652":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8b6b89a4a784463a27f1c63d30de208","IPY_MODEL_12bd38680e6c4cd6bf219de2d6664290","IPY_MODEL_c297f62d6a0d45e98c9541ab0a48f60c"],"layout":"IPY_MODEL_303ab6d81a654696bb8d8d79a2f3e6e6"}},"d8b6b89a4a784463a27f1c63d30de208":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0856acc1a09142099186a3d8bb3b3a7a","placeholder":"​","style":"IPY_MODEL_e10c8c920fea4c1d8802bd6c89cd0851","value":"generation_config.json: 100%"}},"12bd38680e6c4cd6bf219de2d6664290":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05bb9e69a97f449e9a2db22d832c0dea","max":243,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26eb69aa91c24d449bbb1d1eeb9dc176","value":243}},"c297f62d6a0d45e98c9541ab0a48f60c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cafd258d6dc45059024b927485df6dd","placeholder":"​","style":"IPY_MODEL_06342f0f1a1546bd813d5ce341590140","value":" 243/243 [00:00&lt;00:00, 29.8kB/s]"}},"303ab6d81a654696bb8d8d79a2f3e6e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0856acc1a09142099186a3d8bb3b3a7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e10c8c920fea4c1d8802bd6c89cd0851":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05bb9e69a97f449e9a2db22d832c0dea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26eb69aa91c24d449bbb1d1eeb9dc176":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2cafd258d6dc45059024b927485df6dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06342f0f1a1546bd813d5ce341590140":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install -q sentence-transformers hnswlib"],"metadata":{"id":"-2MCjQVo_A1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PR7WZhu1waj","executionInfo":{"status":"ok","timestamp":1766063145201,"user_tz":-180,"elapsed":2739,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"outputId":"efbeb3f2-5ae1-4278-de27-247105b67533"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["#**CONFIGURATION**"],"metadata":{"id":"qtkthM_Y15HK"}},{"cell_type":"code","source":["import os\n","\n","BASE_DIR = \"/content/drive/MyDrive/NLP/codes/data\"\n","INDEX_DIR = f\"{BASE_DIR}/index\"\n","INDEX_PATH = f\"{INDEX_DIR}/hnsw_index.bin\"\n","META_PATH = f\"{INDEX_DIR}/metadatas.jsonl\"\n","\n","EMBEDDING_MODEL_ID = \"BAAI/bge-m3\"\n","EMBEDDING_DIM = 1024\n","MAX_SEQ_LENGTH = 8192\n","\n","#LLM_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","LLM_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n","\n","print(f\"Paths configured:\\n Index: {INDEX_PATH}\\n Meta: {META_PATH}\")"],"metadata":{"id":"4QW_TeX31Df4","executionInfo":{"status":"ok","timestamp":1766063145209,"user_tz":-180,"elapsed":7,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b253a99f-a81d-4a20-f4d1-16e3507a33c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Paths configured:\n"," Index: /content/drive/MyDrive/NLP/codes/data/index/hnsw_index.bin\n"," Meta: /content/drive/MyDrive/NLP/codes/data/index/metadatas.jsonl\n"]}]},{"cell_type":"markdown","source":["#**RETRIEVAL ENGINE LOADING**"],"metadata":{"id":"ES0euIA617Nd"}},{"cell_type":"code","source":["import json\n","import hnswlib\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","\n","rag_components = {\n","    \"hnsw_index\": None,\n","    \"metadatas\": None,\n","    \"emb_model\": None\n","}\n","\n","def init_retrieval_system():\n","    \"\"\"\n","    Vektör veritabanını ve embedding modelini yükle.\n","    \"\"\"\n","\n","    #Metadata Yükle (JSONL)\n","    print(\"Loading metadata...\")\n","    metas = []\n","    if not os.path.exists(META_PATH):\n","        raise FileNotFoundError(f\"Metadata file not found at {META_PATH}\")\n","\n","    with open(META_PATH, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip():\n","                metas.append(json.loads(line))\n","\n","    rag_components[\"metadatas\"] = metas\n","    print(f\"-> Loaded {len(metas)} metadata entries\")\n","\n","    #HNSW Index Yükle\n","    print(f\"Loading HNSW index (Dim={EMBEDDING_DIM})...\")\n","    if not os.path.exists(INDEX_PATH):\n","        raise FileNotFoundError(f\"Index file not found at {INDEX_PATH}\")\n","\n","    index = hnswlib.Index(space=\"cosine\", dim=EMBEDDING_DIM)\n","    index.load_index(INDEX_PATH)\n","    index.set_ef(128)\n","    rag_components[\"hnsw_index\"] = index\n","    print(f\"-> Index loaded with {index.get_current_count()} elements.\")\n","\n","    #Embedding modelini yükle\n","    print(f\"Loading embedding model: {EMBEDDING_MODEL_ID}...\")\n","    model = SentenceTransformer(EMBEDDING_MODEL_ID, device=\"cuda\", trust_remote_code=True)\n","    model.max_seq_length = MAX_SEQ_LENGTH\n","    rag_components[\"emb_model\"] = model\n","    print(\"-> Embedding model ready.\")\n","\n","    print(\"\\n RETRIEVAL SYSTEM READY!\")\n"],"metadata":{"id":"BpihItCx1z7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as genai\n","from google.colab import userdata\n","\n","# Key'i güvenli kutudan çeker, kodda görünmez\n","API_KEY = userdata.get('GEMINI_API_KEY')\n","genai.configure(api_key=API_KEY)\n","\n","def optimize_query_with_gemini(user_query):\n","    # Model konfigürasyonu (Global ayarı kullanır)\n","    model = genai.GenerativeModel('gemini-1.5-flash')\n","\n","    prompt = f\"\"\"\n","    Sen uzman bir araştırmacısın. Kullanıcının şu sorusu için vektör veritabanında arama yapacağız:\n","    Soru: \"{user_query}\"\n","\n","    Bu soruya en iyi cevabı bulabilmek için veritabanında aratabileceğimiz 3 farklı, daha teknik varyasyon yaz.\n","    Sadece maddeleri yaz.\n","    \"\"\"\n","\n","    response = model.generate_content(prompt)\n","    return response.text"],"metadata":{"id":"pH346CwGy9CZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def retrieve_documents(query: str, top_k: int = 5):\n","    \"\"\"\n","    Query'e en yakın dökümanları getirir.\n","    \"\"\"\n","\n","    model = rag_components[\"emb_model\"]\n","    index = rag_components[\"hnsw_index\"]\n","    metadatas = rag_components[\"metadatas\"]\n","\n","    q_emb = model.encode([query], normalize_embeddings=True)\n","\n","    labels, distances = index.knn_query(q_emb, k=top_k)\n","\n","    results = []\n","    for label, dist in zip(labels[0], distances[0]):\n","        meta = metadatas[int(label)]\n","        results.append({\n","            \"idx\": int(label),\n","            \"distance\": float(dist),\n","            \"chunk_id\": meta.get(\"chunk_id\"),\n","            \"text\": meta.get(\"text\"),\n","            \"title\": meta.get(\"title\", \"Unknown Title\"),\n","            \"authors\": meta.get(\"authors\", []),\n","            \"year\": meta.get(\"year\", \"\"),\n","            \"url\": meta.get(\"url\", \"\"),\n","            \"references\": meta.get(\"references\", []),\n","            \"section\": meta.get(\"section_title\", \"\")\n","        })\n","\n","    return results"],"metadata":{"id":"QM-WNWZb3U10"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_retrieval_system()"],"metadata":{"id":"NKH5TW3e4Dol","executionInfo":{"status":"ok","timestamp":1766063190949,"user_tz":-180,"elapsed":29345,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3d36b7b-3bfb-4c88-8449-03bfc7efb4e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading metadata...\n","-> Loaded 602123 metadata entries\n","Loading HNSW index (Dim=1024)...\n","-> Index loaded with 602123 elements.\n","Loading embedding model: BAAI/bge-m3...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["-> Embedding model ready.\n","\n"," RETRIEVAL SYSTEM READY!\n"]}]},{"cell_type":"markdown","source":["#**LLM LOADING**"],"metadata":{"id":"gx4eSc8V4Gjl"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","llm_components = {\n","    \"model\": None,\n","    \"tokenizer\": None\n","}\n","\n","def init_llm_system():\n","    print(f\"Loading LLM: {LLM_MODEL_ID}...\")\n","\n","    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, trust_remote_code=True)\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        LLM_MODEL_ID,\n","        dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        trust_remote_code=True\n","    )\n","\n","    llm_components[\"model\"] = model\n","    llm_components[\"tokenizer\"] = tokenizer\n","\n","    print(f\"\\n {LLM_MODEL_ID} READY\")"],"metadata":{"id":"DYIbrbnM4Far"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_llm_system()"],"metadata":{"id":"6F9yo6iR4jVd","executionInfo":{"status":"ok","timestamp":1766063255555,"user_tz":-180,"elapsed":64550,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"colab":{"base_uri":"https://localhost:8080/","height":293,"referenced_widgets":["f96eefb71bfa458f860df782ddb1383d","8e198974641d4818997393ccc554913f","e0c503180f76401db6ec62f1a22786f0","a1e4448e14c940d5a6d1a1fd490ec80f","04c74636dba0412eb5705d13be075e58","a7dd1d92f09d40fbbb5757910dcc8007","3e397761839c427bb78ff6f520cdd93c","ef1165bf2a224649880a2057f1d68716","d862ffd9fa9e4e8b92b29ea8633140c9","e3211c392f764cd89559b2a999901217","b713846c20064d149077200bdb8d3cd3","02541a9b6fca4ee597877cb4c4916ced","aa2247e47fd04c8a9448a6abf2bb89eb","9c27508cc9eb46e2933d98c5ff0c4473","be11e4978c414ffd8ad3884acc66828e","56ba9fa881124b3683fc581951c805b8","5c17e78d5825499ab8884a53bc1502e1","6c244671018e45298102449421ed5e62","7b67a519d7f6480398aef26da47a550e","1c000ad2c5ca4a3e8447095aa4592ccf","11f0ba061a544fcea377bc4478b83748","3b84365e813e4699a2fbe0d8f59cc7aa","36f61d00ad194c74913081afd4d2d5ef","1f0390f1544b4204b64cb064c02060ac","0701a592c33148bf8466aa0d75a25e8a","31c55b1053cf4d32bdd2322e8f9fd97a","7c86297973344673ba0eb205457ed3d6","62bb374932ab4c8c90b7258706643daa","c00a0e9fcdde423aa24f23ce46bea9a2","d0d5589790074f12a436e53ae37c72f8","135c4fc411244a578f61db3c34c53bb3","ee7e3db6e4854c07a05c545222dc78a1","a81f0fafe59649ae9775ef7419b18b11","ca4aa907617e4ad984cc79e29c673072","fbad8675efc742af9efa575ed73ea7b9","9024ecc8d329481a942c59baa2970187","94f148ddea2b4926aed0fcc863d58c8d","c76c97ec03ab4909b492338f92ce3e15","cd1a6fa912f04405afcfa4c01bf080ab","f4638cdcca51413b9ae0ae7bafa0d7e0","d202563c7584484ba2df79251fba9128","9c8c837bfac34736be16e48e85bdd9dc","b1ad481d775b4d5f8a943325464ec9c3","6bf07c0352924cc7894f1f855115e622","7e8a1780a7af4b5e9e546c7f128bb4e3","8a7c2659d85e4dcaa2241b4dcd187fcf","30f1f31f1cfe4a4dac6a7da9bd4557d7","4e23c29160a3434b92e326eaf77429e0","2b572fbad5424420b77c4a8e15ac2cd5","d781a56c106d40258b17f4eac51dce3e","3f1e1969d0574ed893870d634f9df227","92402e95a227442880a1f9fdc04a3157","9b849c9916b943798a8b0b7c62f52869","bcf0c9e666ed4f53a4673bd03f91689c","ddaa5b9f0b9f4daaa8a898676384ca5c","2d9a21e65a5c42369f762db814a166cb","a2d9f2379a724dffb92ea1a475c12676","56052ec8c7e84d9684281d061b68e7bb","613a018f09ec4de3801e64455c1c461a","a7f77fee35fd4ee2aa91fcb60af59743","17eb070b1c1a4eba8c3442586ffe4b63","26d4df2eafbe42e8826ae11f66129f85","bd1f639e10ec4b55bbe616a2f8220917","ab196f508a73403f86850dd43074d7fb","012b68968f98433d9dbb97f684ccf461","d3bd9679925d4fad837319beeba0e15d","8e031e77e82b473e8dd6b0499ddfa652","d8b6b89a4a784463a27f1c63d30de208","12bd38680e6c4cd6bf219de2d6664290","c297f62d6a0d45e98c9541ab0a48f60c","303ab6d81a654696bb8d8d79a2f3e6e6","0856acc1a09142099186a3d8bb3b3a7a","e10c8c920fea4c1d8802bd6c89cd0851","05bb9e69a97f449e9a2db22d832c0dea","26eb69aa91c24d449bbb1d1eeb9dc176","2cafd258d6dc45059024b927485df6dd","06342f0f1a1546bd813d5ce341590140"]},"outputId":"cd5f8253-c8f5-4fde-a1db-ae641005bc53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading LLM: Qwen/Qwen2.5-7B-Instruct...\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96eefb71bfa458f860df782ddb1383d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02541a9b6fca4ee597877cb4c4916ced"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f61d00ad194c74913081afd4d2d5ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca4aa907617e4ad984cc79e29c673072"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8a1780a7af4b5e9e546c7f128bb4e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d9a21e65a5c42369f762db814a166cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e031e77e82b473e8dd6b0499ddfa652"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"," Qwen/Qwen2.5-7B-Instruct READY\n"]}]},{"cell_type":"markdown","source":["#**GENERATION LOGIC WITH CITATIONS**"],"metadata":{"id":"ozaPH4Ls4lMl"}},{"cell_type":"code","source":["def get_full_author_list(authors_list):\n","    \"\"\"\n","    Yazarları 'Ad Soyad, Ad Soyad' şeklinde tam liste olarak döndürür.\n","    \"\"\"\n","\n","    if not authors_list:\n","        return \"Unknown Authors\"\n","\n","    names = []\n","    for author in authors_list:\n","        first = author.get(\"firstname\", \"\").strip()\n","        last = author.get(\"surname\", \"\").strip()\n","        full_name = f\"{first} {last}\".strip()\n","        if full_name:\n","            names.append(full_name)\n","\n","    return \", \".join(names)"],"metadata":{"id":"lRX5oflF4nLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_ref_text(text):\n","    \"\"\"\n","    JSON'daki ham referans metnini (tab/newline dolu) temizler.\n","    \"\"\"\n","\n","    if not text: return \"\"\n","    text = text.replace('\\n', ' ').replace('\\t', ' ')\n","    return re.sub(r'\\s+', ' ', text).strip()"],"metadata":{"id":"E3IdsI489r50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","def generate_academic_paper(user_query: str, top_k: int = 6):\n","    \"\"\"\n","    - Dökümanları getirir.\n","    - Prompt'u hazırlar.\n","    - Mistral ile akademik metni üretir.\n","    \"\"\"\n","\n","# --- YENİ EKLENEN KISIM: GEMINI OPTIMIZATION ---\n","    print(f\"Original Query: {user_query}\")\n","    print(\"Optimizing query with Gemini...\")\n","\n","    try:\n","        # Gemini fonksiyonunu çağır (Bu fonksiyonu tanımlamış olmalısın)\n","        optimized_search_query = optimize_query_with_gemini(user_query)\n","        print(f\"Search Query Optimized: {optimized_search_query}\")\n","    except Exception as e:\n","        print(f\"Gemini Optimization failed, using original query. Error: {e}\")\n","        optimized_search_query = user_query\n","    # -----------------------------------------------\n","\n","    # DİKKAT: Retrieve ederken 'optimized_search_query' kullanıyoruz\n","    print(f\"Retrieving top {top_k} contexts...\")\n","    contexts = retrieve_documents(optimized_search_query, top_k=top_k)\n","    context_block = \"\"\n","    for i, ctx in enumerate(contexts, 1):\n","        auth_str = get_full_author_list(ctx['authors'])\n","        year = ctx.get('year') or \"n.d.\"\n","        title = ctx.get('title', 'Unknown Title')\n","        section = ctx.get('section', 'General')\n","\n","        internal_refs_text = \"\"\n","        raw_refs = ctx.get('references', [])\n","\n","        if raw_refs:\n","            internal_refs_text = \"\\n    > Studies cited within this text:\\n\"\n","\n","            for ref in raw_refs:\n","                rid = ref.get('id')\n","                clean_text = clean_ref_text(ref.get('text'))\n","                if clean_text:\n","                    internal_refs_text += f\"    * [Ref ID: {rid}] {clean_text}\\n\"\n","\n","        context_block += f\"--- {i} ---\\n\"\n","        context_block += f\"Primary Work: {title}\\n\"\n","        context_block += f\"Authors: {auth_str} ({year})\\n\"\n","        context_block += f\"Content (from {section}):\\n{ctx['text']}\\n\"\n","        context_block += f\"{internal_refs_text}\\n\"\n","\n","    system_prompt = f\"\"\"[INST] You are an expert Academic Literature Reviewer and Research Assistant.\n","    Your goal is to synthesize the provided academic papers into a coherent, objective, scientifically accurate, and highly readable review.\n","\n","    ### I. CITATION & INDEXING PROTOCOLS (STRICTLY FOLLOW):\n","    1.  **SEQUENTIAL RE-INDEXING RULE (CRITICAL):**\n","        * You will receive sources labeled with various IDs (e.g., `--- SOURCE 5 ---`, `--- SOURCE 12 ---`).\n","        * **IGNORE** these original numbers for your citations.\n","        * **RE-NUMBER** them based on their order of appearance in the provided context:\n","            * The **1st** source listed in the context becomes **[1]**.\n","            * The **2nd** source listed in the context becomes **[2]**.\n","            * And so on.\n","        * *Example:* If the context shows `Source 10` followed by `Source 5`, cite the first one as [1] and the second as [2].\n","\n","    2.  **QUALITY FILTER:**\n","        * If a provided source is empty, irrelevant, or lacks specific findings, **DO NOT USE IT**. Do not force a citation just to fill a quota. Only cite sources that contribute meaningful information.\n","\n","    3.  **SECONDARY SOURCES:**\n","        * If referencing a study cited *within* a source (e.g., Smith, 2020), state: \"Smith (2020, cited in [1])...\"\n","\n","    ### II. FORMATTING & STYLE GUIDELINES:\n","    * **Tone:** Objective, formal, and academic. No conversational filler.\n","    * **Structure:** Use **Headings (`##`)** for themes, **Bolding** for key terms, and **Bullet Points** for lists.\n","    * **LaTeX:** Use `$...$` for inline math (e.g., $p < 0.05$) and `$$...$$` for block equations. Do NOT use LaTeX for simple units (e.g., write \"15%\", not $15\\%$).\n","\n","    ### III. CRITICAL NEGATIVE CONSTRAINTS:\n","    1.  **NO HALLUCINATIONS:** If the answer is not in the sources, do not invent it.\n","    2.  **NO SOURCE CONFLATION:** Keep findings distinct.\n","    3.  **NO META-TALK:** Do not write \"The provided text says...\". Start the review directly.\n","    4.  **NO REFERENCE LIST:** DO NOT generate a \"References\" section at the end.\n","\n","    ### IV. ONE-SHOT EXAMPLE (EMULATE THIS STYLE):\n","\n","    **Context Provided:**\n","    --- SOURCE 25 --- (First in list)\n","    Content: Method A achieves 90% accuracy.\n","    --- SOURCE 8 --- (Second in list)\n","    Content: Method B is faster but less accurate.\n","\n","    **Ideal Response:**\n","    ## Performance Comparison\n","    Recent studies highlight a trade-off between accuracy and speed. Method A demonstrates superior precision, achieving **90% accuracy** [1]. In contrast, Method B prioritizes computational efficiency over raw performance [2].\n","\n","    ### V. EXECUTION:\n","    AVAILABLE CONTEXT SOURCES:\n","    {context_block}\n","    ---\n","\n","    USER QUERY:\n","    {user_query}\n","\n","    Generate the academic review response body now.\n","    At the very end, add a single, short, italicized \"Next Step\" asking if the user wants to explore a specific aspect further. [/INST]\n","    \"\"\"\n","\n","    tokenizer = llm_components[\"tokenizer\"]\n","    model = llm_components[\"model\"]\n","\n","    inputs = tokenizer(system_prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","    input_length = inputs.input_ids.shape[1]\n","\n","    print(\"Generating academic text...\")\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=8192,\n","            temperature=0.2,\n","            top_p=0.9,\n","            do_sample=True,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","\n","    generated_tokens = outputs[0][input_length:]\n","    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n","\n","    return answer.strip(), contexts"],"metadata":{"id":"5IbLyiVF49RE","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1766046963410,"user_tz":-180,"elapsed":144,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"outputId":"6a9842aa-954d-4b2f-adfc-ab771dec09bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:123: SyntaxWarning: invalid escape sequence '\\%'\n","<>:123: SyntaxWarning: invalid escape sequence '\\%'\n","/tmp/ipython-input-4037575112.py:123: SyntaxWarning: invalid escape sequence '\\%'\n","  answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n"]}]},{"cell_type":"code","source":["def query_qwen(user_query: str, use_rag: bool = True, top_k: int = 6):\n","    \"\"\"\n","    Qwen modeli için Baseline vs RAG karşılaştırma fonksiyonu.\n","    Orijinal 'generate_academic_paper' mantığını birebir korur.\n","    \"\"\"\n","\n","    tokenizer = llm_components[\"tokenizer\"]\n","    model = llm_components[\"model\"]\n","\n","    contexts = []\n","    context_block = \"\"\n","\n","    # --- RAG MODU: Retrieval ve Context İşleme (Senin Orijinal Mantığın) ---\n","    if use_rag:\n","        print(f\"Retrieving top {top_k} contexts for: '{user_query}'...\")\n","        contexts = retrieve_documents(user_query, top_k=top_k)\n","\n","        for i, ctx in enumerate(contexts, 1):\n","            # Helper fonksiyonları kullandığından emin oluyoruz\n","            auth_str = get_full_author_list(ctx['authors'])\n","            year = ctx.get('year') or \"n.d.\"\n","            title = ctx.get('title', 'Unknown Title')\n","            section = ctx.get('section', 'General')\n","\n","            # İç referansları senin mantığınla işle\n","            internal_refs_text = \"\"\n","            raw_refs = ctx.get('references', [])\n","\n","            if raw_refs:\n","                internal_refs_text = \"\\n    > Studies cited within this text:\\n\"\n","                for ref in raw_refs:\n","                    rid = ref.get('id')\n","                    clean_text = clean_ref_text(ref.get('text')) # Senin helper fonksiyonun\n","                    if clean_text:\n","                        internal_refs_text += f\"    * [Ref ID: {rid}] {clean_text}\\n\"\n","\n","            # Context bloğunu inşa et\n","            context_block += f\"--- SOURCE {i} ---\\n\"\n","            context_block += f\"Primary Work: {title}\\n\"\n","            context_block += f\"Authors: {auth_str} ({year})\\n\"\n","            context_block += f\"Content (from {section}):\\n{ctx['text']}\\n\"\n","            context_block += f\"{internal_refs_text}\\n\"\n","\n","        # RAG için System Prompt (Senin kuralların)\n","        \"\"\"system_instruction = f[INST] You are an expert Academic Literature Reviewer and Research Assistant.\n","    Your goal is to synthesize the provided academic papers into a coherent, objective, scientifically accurate, and highly readable review.\n","\n","    ### I. CITATION PROTOCOLS (STRICTLY FOLLOW):\n","    1.  **PRIMARY SOURCES (The Main Text):**\n","        * These are the main papers provided in the context, labeled as `--- SOURCE X ---`.\n","        * **Rule:** Any information taken directly from the text of Source X must be cited using **ONLY the number** in the brackets: `[X]`.\n","        * **DO NOT** use `[Source 1]`, `(Author, Year)`, or `[Ref 1]`. Just use `[1]`, `[2]`, etc.\n","        * *Example:* \"Yoon et al. propose a new benchmark [1].\"\n","\n","    2.  **SECONDARY SOURCES (The \"Cited In\" Rule):**\n","        * You will see a list labeled `> Studies cited within this text` under some sources.\n","        * **Rule:** If you refer to these inner studies (e.g., Smith, 2020), you MUST explicitly state that they are cited in the primary source.\n","        * *Correct:* \"According to Smith (2020, cited in [1]), the error rate is...\"\n","        * *Incorrect:* \"Smith (2020) states that...\" (Do not imply you read Smith's paper directly).\n","\n","    ### II. FORMATTING & STYLE GUIDELINES:\n","    * **Tone:** Objective, formal, and academic. No conversational filler (\"I\", \"We\", \"Let's look at\").\n","    * **Structure & Scannability:**\n","        * Use **Headings (`##`, `###`)** to organize findings by theme or methodology (not just by source).\n","        * Use **Bolding (`**...**`)** to highlight key concepts, methodologies, or significant results.\n","        * Use **Bullet Points** to list specific metrics or comparative features for clarity.\n","    * **LaTeX Usage:**\n","        * Use LaTeX ONLY for formal math/science (equations, formulas).\n","        * Enclose inline math in single dollar signs: $E = mc^2$.\n","        * Enclose standalone equations in double dollar signs: $$...$$.\n","        * **Strictly Avoid** LaTeX for simple numbers or units (e.g., write \"15%\" or \"200 km\", NOT $15\\%$).\n","    * **Synthesis:** Do not just list summaries. Connect ideas (e.g., \"While [1] focuses on accuracy, [3] prioritizes efficiency.\").\n","\n","    ### III. CRITICAL NEGATIVE CONSTRAINTS:\n","    1.  **NO HALLUCINATIONS:** If the answer is not in the sources, do not invent it.\n","    2.  **NO SOURCE CONFLATION:** Keep findings distinct (e.g., do not attribute [1]'s findings to [2]).\n","    3.  **NO META-TALK:** Do not write \"The provided text says...\" or \"Based on the context...\". Start the review directly.\n","    4.  **NO REFERENCE LIST:** DO NOT generate a \"References\" section at the end.\n","    5.  **IGNORE ORIGINAL CITATIONS:** Do not copy citations like [12] or [45] found inside the source text.\n","\n","    ### IV. ONE-SHOT EXAMPLE (EMULATE THIS STYLE):\n","\n","    **User Query:** \"How does method X improve accuracy?\"\n","\n","    **Context Provided:**\n","    --- SOURCE 1 ---\n","    Title: Study of X\n","    Content: Method X increases accuracy by 15% compared to Y.\n","    >   Studies cited within this text:\n","        * [Ref ID: 10] Smith (2020) introduced Method Y.\n","\n","    --- SOURCE 2 ---\n","    Title: Analysis of Latency\n","    Content: While X is accurate, it suffers from high latency ($t > 500ms$).\n","\n","    **Ideal Response:**\n","    ## Accuracy Gains\n","    Method X has been shown to significantly enhance performance, achieving a **15% increase in accuracy** over Method Y [1]. As noted by Smith (2020, cited in [1]), Method Y remains a common baseline, but X outperforms it in raw precision.\n","\n","    ## Latency Concerns\n","    Despite these gains, recent analyses indicate that Method X is prone to high latency issues, specifically where $t > 500ms$ [2]. This suggests a trade-off between computational speed and predictive power.\n","\n","    ### Conclusion\n","    While Method X offers superior accuracy, its application may be limited by latency constraints.\n","\n","    ---\n","\n","    ### V. EXECUTION:\n","    AVAILABLE CONTEXT SOURCES:\n","    {context_block}\n","    ---\n","\n","    USER QUERY:\n","    {user_query}\n","\n","    Generate the academic review response body now.\n","    At the very end, add a single, short, italicized \"Next Step\" asking if the user wants to explore a specific aspect further. [/INST]\n","    \"\"\"\n","\n","        system_instruction = f\"\"\"[INST] You are an expert Academic Literature Reviewer and Research Assistant.\n","    Your goal is to synthesize the provided academic papers into a coherent, objective, scientifically accurate, and highly readable review.\n","\n","    ### I. CITATION & INDEXING PROTOCOLS (STRICTLY FOLLOW):\n","    1.  **SEQUENTIAL RE-INDEXING RULE (CRITICAL):**\n","        * You will receive sources labeled with various IDs (e.g., `--- SOURCE 5 ---`, `--- SOURCE 12 ---`).\n","        * **IGNORE** these original numbers for your citations.\n","        * **RE-NUMBER** them based on their order of appearance in the provided context:\n","            * The **1st** source listed in the context becomes **[1]**.\n","            * The **2nd** source listed in the context becomes **[2]**.\n","            * And so on.\n","        * *Example:* If the context shows `Source 10` followed by `Source 5`, cite the first one as [1] and the second as [2].\n","\n","    2.  **QUALITY FILTER:**\n","        * If a provided source is empty, irrelevant, or lacks specific findings, **DO NOT USE IT**. Do not force a citation just to fill a quota. Only cite sources that contribute meaningful information.\n","\n","    3.  **SECONDARY SOURCES:**\n","        * If referencing a study cited *within* a source (e.g., Smith, 2020), state: \"Smith (2020, cited in [1])...\"\n","\n","    ### II. FORMATTING & STYLE GUIDELINES:\n","    * **Tone:** Objective, formal, and academic. No conversational filler.\n","    * **Structure:** Use **Headings (`##`)** for themes, **Bolding** for key terms, and **Bullet Points** for lists.\n","    * **LaTeX:** Use `$...$` for inline math (e.g., $p < 0.05$) and `$$...$$` for block equations. Do NOT use LaTeX for simple units (e.g., write \"15%\", not $15\\%$).\n","\n","    ### III. CRITICAL NEGATIVE CONSTRAINTS:\n","    1.  **NO HALLUCINATIONS:** If the answer is not in the sources, do not invent it.\n","    2.  **NO SOURCE CONFLATION:** Keep findings distinct.\n","    3.  **NO META-TALK:** Do not write \"The provided text says...\". Start the review directly.\n","    4.  **NO REFERENCE LIST:** DO NOT generate a \"References\" section at the end.\n","\n","    ### IV. ONE-SHOT EXAMPLE (EMULATE THIS STYLE):\n","\n","    **Context Provided:**\n","    --- SOURCE 25 --- (First in list)\n","    Content: Method A achieves 90% accuracy.\n","    --- SOURCE 8 --- (Second in list)\n","    Content: Method B is faster but less accurate.\n","\n","    **Ideal Response:**\n","    ## Performance Comparison\n","    Recent studies highlight a trade-off between accuracy and speed. Method A demonstrates superior precision, achieving **90% accuracy** [1]. In contrast, Method B prioritizes computational efficiency over raw performance [2].\n","\n","    ### V. EXECUTION:\n","    AVAILABLE CONTEXT SOURCES:\n","    {context_block}\n","    ---\n","\n","    USER QUERY:\n","    {user_query}\n","\n","    Generate the academic review response body now.\n","    At the very end, add a single, short, italicized \"Next Step\" asking if the user wants to explore a specific aspect further. [/INST]\n","    \"\"\"\n","    else:\n","        # --- BASELINE MODU: Context Yok ---\n","        system_instruction = \"\"\"You are an expert Academic Researcher.\n","Answer the user's question using your internal knowledge base.\n","Maintain a formal, objective, and scientific tone.\"\"\"\n","\n","    # --- Qwen İçin Chat Formatı (ChatML) ---\n","    # Mistral'deki [INST] yerine messages listesi ve apply_chat_template kullanıyoruz\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_instruction},\n","        {\"role\": \"user\", \"content\": user_query}\n","    ]\n","\n","    # Promptu tokenize et\n","    text = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n","\n","    print(f\"Generating Qwen response ({'RAG' if use_rag else 'BASELINE'})...\")\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=4096, # Uzun akademik metin için\n","            temperature=0.2,\n","            top_p=0.9,\n","            do_sample=True,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","    # Sadece üretilen kısmı al (inputu kes)\n","    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n","    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)\n","\n","    return answer.strip(), contexts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTrhjYg2c70v","executionInfo":{"status":"ok","timestamp":1766063255604,"user_tz":-180,"elapsed":26,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"outputId":"fc05ede0-5bdf-411a-bf34-dbfa0b92951d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:71: SyntaxWarning: invalid escape sequence '\\%'\n","<>:194: SyntaxWarning: invalid escape sequence '\\%'\n","<>:71: SyntaxWarning: invalid escape sequence '\\%'\n","<>:194: SyntaxWarning: invalid escape sequence '\\%'\n","/tmp/ipython-input-3645678776.py:71: SyntaxWarning: invalid escape sequence '\\%'\n","  * **Strictly Avoid** LaTeX for simple numbers or units (e.g., write \"15%\" or \"200 km\", NOT $15\\%$).\n","/tmp/ipython-input-3645678776.py:194: SyntaxWarning: invalid escape sequence '\\%'\n","  \n"]}]},{"cell_type":"markdown","source":["#**MAIN EXECUTION**"],"metadata":{"id":"ML8kpwzE60yz"}},{"cell_type":"code","source":["def sanitize_table_cell(text):\n","    \"\"\"Tabloyu kıran karakterleri temizler.\"\"\"\n","    if not text: return \"N/A\"\n","    text = str(text).replace('\\n', ' ').replace('\\r', ' ').replace('|', '&#124;')\n","    return re.sub(r'\\s+', ' ', text).strip()"],"metadata":{"id":"8XuKN73cPhyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from collections import defaultdict\n","from IPython.display import Markdown, display\n","\n","def clean_and_display_report(query, top_k=5):\n","    generated_text, used_sources = generate_academic_paper(query, top_k=top_k)\n","\n","    split_pattern = r'(?i)\\n\\s*(References|Bibliography|Sources|Studies cited within).*$'\n","    parts = re.split(split_pattern, generated_text)\n","    clean_text = parts[0] if parts else generated_text\n","    clean_text = re.sub(r'^[ \\t]+', '', generated_text, flags=re.MULTILINE)\n","\n","    cited_indices = set()\n","    matches = re.findall(r'\\[(\\d+)]', clean_text)\n","    for m in matches:\n","        cited_indices.add(int(m))\n","\n","    markdown_report = \"## Generated Academic Text\\n\\n\"\n","    markdown_report += clean_text.strip() + \"\\n\\n\"\n","    markdown_report += \"---\\n\\n\"\n","\n","    grouped_sources = defaultdict(list)\n","    for i, src in enumerate(used_sources, 1):\n","        if i not in cited_indices:\n","            continue\n","\n","        title = src.get('title', 'Unknown Title')\n","        grouped_sources[title].append({\n","            \"id\": i,\n","            \"section\": src.get('section', 'General'),\n","            \"distance\": src.get('distance'),\n","            \"authors\": src.get('authors'),\n","            \"year\": src.get(\"year\"),\n","            \"url\": src.get(\"url\"),\n","            \"references\": src.get('references', [])\n","        })\n","\n","    if not grouped_sources:\n","        markdown_report += \"> *No sources were directly cited in the text.*\"\n","    else:\n","        markdown_report += \"## Bibliography & Source References\\n\\n\"\n","\n","        for title, chunks in grouped_sources.items():\n","            first_chunk = chunks[0]\n","            full_authors = get_full_author_list(first_chunk['authors'])\n","\n","            safe_title = sanitize_table_cell(title)\n","            safe_authors = sanitize_table_cell(full_authors)\n","\n","            year = first_chunk['year'] or \"n.d.\"\n","            url = first_chunk['url'] or \"n.d.\"\n","\n","            source_ids = \", \".join([str(c['id']) for c in chunks])\n","\n","            markdown_report += f\"### [Source {source_ids}] {safe_title}\\n\"\n","            markdown_report += f\"**Authors:** *{full_authors}* ({year})\\n\"\n","            markdown_report += f\"**Url:** *{url}*\\n\\n\"\n","\n","            markdown_report += \"| Ref ID | Section Used | Key Citations Inside | Score |\\n\"\n","            markdown_report += \"| :---: | :--- | :--- | :---: |\\n\"\n","\n","            for c in chunks:\n","                score = 1 - c['distance']\n","                safe_section = sanitize_table_cell(c['section'])\n","\n","                inner_refs_display = \"-\"\n","                if c['references']:\n","                    refs_list = []\n","                    for r in c['references']:\n","                        rid = r.get('id')\n","                        rtext = clean_ref_text(r.get('text', ''))\n","                        safe_rtext = sanitize_table_cell(rtext)\n","                        refs_list.append(f\"• [{rid}] {safe_rtext}\")\n","\n","                    inner_refs_display = \"<br>\".join(refs_list)\n","\n","                markdown_report += f\"| **[{c['id']}]** | {safe_section} | {inner_refs_display} | **{score:.2f}** |\\n\"\n","\n","            markdown_report += \"\\n<br>\\n\"\n","\n","    display(Markdown(markdown_report))"],"metadata":{"id":"8GZJRDttYZl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","query = \"What is the primary research function of the BacPrep platform and what model does it currently employ?\"\n","\n","clean_and_display_report(query, 10)\n","\n","\n","\n"],"metadata":{"id":"8IudWJBq62CT","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from collections import defaultdict\n","from IPython.display import Markdown, display\n","\n","def clean_and_display_report_qwen(query, use_rag=True, top_k=5):\n","    \"\"\"\n","    Raporu oluşturur ve görüntüler.\n","    - use_rag=True: Detaylı kaynak tablosuyla birlikte rapor basar.\n","    - use_rag=False: Sadece baseline cevabı basar.\n","    \"\"\"\n","\n","    # ARTIK QUERY_QWEN ÇAĞIRIYORUZ\n","    generated_text, used_sources = query_qwen(query, use_rag=use_rag, top_k=top_k)\n","\n","    # Metin temizleme (Referans başlıklarını uçurma)\n","    split_pattern = r'(?i)\\n\\s*(References|Bibliography|Sources|Studies cited within).*$'\n","    parts = re.split(split_pattern, generated_text)\n","    clean_text = parts[0] if parts else generated_text\n","    clean_text = re.sub(r'^[ \\t]+', '', clean_text, flags=re.MULTILINE)\n","\n","    # Başlık\n","    mode_title = \"RAG Augmented Response (Qwen)\" if use_rag else \"Baseline Response (Qwen)\"\n","    markdown_report = f\"## {mode_title}\\n\\n\"\n","    markdown_report += clean_text.strip() + \"\\n\\n\"\n","    markdown_report += \"---\\n\\n\"\n","\n","    # --- RAG KAPALIYSA BURADA BİTİR ---\n","    if not use_rag or not used_sources:\n","        display(Markdown(markdown_report))\n","        return\n","\n","    # --- RAG AÇIKSA TABLOYU OLUŞTUR ---\n","    cited_indices = set()\n","    matches = re.findall(r'\\[(\\d+)]', clean_text)\n","    for m in matches:\n","        cited_indices.add(int(m))\n","\n","    grouped_sources = defaultdict(list)\n","    for i, src in enumerate(used_sources, 1):\n","        # Sadece metinde atıf yapılanları listele\n","        if i not in cited_indices:\n","            continue\n","\n","        title = src.get('title', 'Unknown Title')\n","        grouped_sources[title].append({\n","            \"id\": i,\n","            \"section\": src.get('section', 'General'),\n","            \"distance\": src.get('distance'),\n","            \"authors\": src.get('authors'),\n","            \"year\": src.get(\"year\"),\n","            \"url\": src.get(\"url\"),\n","            \"references\": src.get('references', [])\n","        })\n","\n","    if not grouped_sources:\n","        markdown_report += \"> *No sources were directly cited in the text although RAG was active.*\"\n","    else:\n","        markdown_report += \"## References\\n\\n\"\n","\n","        for title, chunks in grouped_sources.items():\n","            first_chunk = chunks[0]\n","            full_authors = get_full_author_list(first_chunk['authors'])\n","\n","            safe_title = sanitize_table_cell(title)\n","            safe_authors = sanitize_table_cell(full_authors)\n","            year = first_chunk['year'] or \"n.d.\"\n","            url = first_chunk['url'] or \"n.d.\"\n","            source_ids = \", \".join([str(c['id']) for c in chunks])\n","\n","            markdown_report += f\"### [Source {source_ids}] {safe_title}\\n\"\n","            markdown_report += f\"**Authors:** *{full_authors}* ({year})\\n\"\n","            markdown_report += f\"**Url:** *{url}*\\n\\n\"\n","\n","            markdown_report += \"| Ref ID | Section Used | Key Citations Inside | Score |\\n\"\n","            markdown_report += \"| :---: | :--- | :--- | :---: |\\n\"\n","\n","            for c in chunks:\n","                score = 1 - c['distance']\n","                safe_section = sanitize_table_cell(c['section'])\n","\n","                inner_refs_display = \"-\"\n","                if c['references']:\n","                    refs_list = []\n","                    for r in c['references']:\n","                        rid = r.get('id')\n","                        rtext = clean_ref_text(r.get('text', ''))\n","                        safe_rtext = sanitize_table_cell(rtext)\n","                        refs_list.append(f\"• [{rid}] {safe_rtext}\")\n","                    inner_refs_display = \"<br>\".join(refs_list)\n","\n","                markdown_report += f\"| **[{c['id']}]** | {safe_section} | {inner_refs_display} | **{score:.2f}** |\\n\"\n","\n","            markdown_report += \"\\n<br>\\n\"\n","\n","    display(Markdown(markdown_report))"],"metadata":{"id":"EVsvy0b5gAyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# query = \"what is the advantages of transformers to RNN's\"\n","# query = \"Are LLM's better GNN's\"\n","# 1. BASELINE QWEN (Veritabanı yok, sadece Qwen'in bilgisi)\n","# print(\">>> RUNNING BASELINE MODE...\")\n","# clean_and_display_report_qwen(query, use_rag=False)\n","\n","# 2. RAG QWEN (Veritabanı var, kaynaklar ve tablo var)\n","# print(\"\\n>>> RUNNING RAG MODE...\")\n","clean_and_display_report_qwen(query, use_rag=True, top_k=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":839},"id":"tUchuMrzd_uC","executionInfo":{"status":"ok","timestamp":1766063334791,"user_tz":-180,"elapsed":27621,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"outputId":"a7d28e50-bccf-4fc5-aea4-9ba4f6461936","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Retrieving top 20 contexts for: 'Are LLM's better GNN's'...\n","Generating Qwen response (RAG)...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## RAG Augmented Response (Qwen)\n\n## LLMs vs. GNNs in Graph Learning\n\nRecent studies have explored the integration of Large Language Models (LLMs) with Graph Neural Networks (GNNs) to enhance robust graph learning under various conditions. The empirical analysis conducted in the work by Wang et al. (2025) reveals that LLM augmentations often fall behind simpler GNN counterparts under modest deficiencies. Specifically, both LLM-as-Encoder and LLM-as-Enhancer paradigms exhibit comparable or worse accuracy than traditional GNN-based methods (Sources [1], [2]). This observation underscores the limitations of LLMs in providing semantically diverse and coherent augmentations, which are crucial for improving the discriminative capacity of GNNs. The authors attribute these limitations to the inherent semantic homogeneity in LLM-generated content, leading to high intra-class variance and small inter-class margins in learned representations (Source [17]).\n\nIn contrast, GraphLM, as described in another study, integrates structural features with broader semantic understanding, thereby addressing some of the limitations of GNNs and GraphTransformers. GraphLM is noted for its ability to handle long-range dependencies and hierarchical structures more comprehensively, making it a promising alternative for tasks requiring deep graph comprehension (Source [15]).\n\nWhile the current evidence suggests that simpler GNN-based methods often outperform LLM-enhanced approaches under less severe deficiencies, the potential of LLMs in more complex scenarios remains an open area of research. Further investigation is needed to explore how LLMs can be optimized to better complement GNNs in specific applications.\n\n*Next Step*: Would you like to explore the specific limitations of LLMs in detail or delve into the potential synergies between LLMs and GNNs in more complex tasks?\n\n---\n\n## References\n\n### [Source 1, 2, 17] Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement\n**Authors:** *Zhaoyan Wang, Zheng Gao, Arogya Kharel, In-Young Ko* (2025)\n**Url:** *https://arxiv.org//pdf/2510.01910*\n\n| Ref ID | Section Used | Key Citations Inside | Score |\n| :---: | :--- | :--- | :---: |\n| **[1]** | Empirical Analysis | - | **0.63** |\n| **[2]** | Empirical Analysis | - | **0.63** |\n| **[17]** | Methodology 4.1 System Overview | - | **0.59** |\n\n<br>\n### [Source 15] Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs\n**Authors:** *Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong* (2025)\n**Url:** *https://arxiv.org//pdf/2509.22251*\n\n| Ref ID | Section Used | Key Citations Inside | Score |\n| :---: | :--- | :--- | :---: |\n| **[15]** | Why structural information of KGs is important and must not be overlooked? | - | **0.59** |\n\n<br>\n"},"metadata":{}}]},{"cell_type":"markdown","source":["#**METRICS**"],"metadata":{"id":"1op016Zimg-V"}},{"cell_type":"code","source":["!pip install ragas datasets langchain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GGGG_nRemggu","executionInfo":{"status":"ok","timestamp":1766043277706,"user_tz":-180,"elapsed":13531,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"outputId":"e2ae0f4b-bcfb-4838-810f-eca8d1064790"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ragas\n","  Downloading ragas-0.4.1-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.3)\n","Requirement already satisfied: numpy<3.0.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.0.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from ragas) (0.12.0)\n","Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.12.3)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ragas) (1.6.0)\n","Collecting appdirs (from ragas)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n","Collecting diskcache>=5.6.3 (from ragas)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from ragas) (0.20.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from ragas) (13.9.4)\n","Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ragas) (4.67.1)\n","Collecting instructor (from ragas)\n","  Downloading instructor-1.13.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (11.3.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from ragas) (3.6.1)\n","Collecting scikit-network (from ragas)\n","  Downloading scikit_network-0.33.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (from ragas) (1.1.3)\n","Collecting langchain-community (from ragas)\n","  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n","Collecting langchain_openai (from ragas)\n","  Downloading langchain_openai-1.1.5-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n","Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.4)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (1.33)\n","Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (0.4.58)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (9.1.2)\n","Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (0.12.0)\n","Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n","Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n","Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (4.12.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (0.12.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (2.41.4)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n","Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas) (0.17.0)\n","Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas) (3.1.6)\n","Collecting jiter<1,>=0.10.0 (from openai>=1.0.0->ragas)\n","  Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n","Collecting pre-commit>=4.3.0 (from instructor->ragas)\n","  Downloading pre_commit-4.5.1-py2.py3-none-any.whl.metadata (1.2 kB)\n","Collecting ty>=0.0.1a23 (from instructor->ragas)\n","  Downloading ty-0.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas) (2.19.2)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas) (8.3.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas) (1.5.4)\n","Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community->ragas)\n","  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (2.0.45)\n","Collecting requests>=2.32.2 (from datasets)\n","  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n","Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community->ragas)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (2.12.0)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (0.4.3)\n","Collecting langchain-core (from ragas)\n","  Downloading langchain_core-1.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->ragas) (2025.11.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from scikit-network->ragas) (1.16.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->ragas) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->ragas) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->ragas) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core->ragas) (3.0.0)\n","Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community->ragas)\n","  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n","Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (0.25.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->ragas) (0.1.2)\n","Collecting cfgv>=2.0.0 (from pre-commit>=4.3.0->instructor->ragas)\n","  Downloading cfgv-3.5.0-py2.py3-none-any.whl.metadata (8.9 kB)\n","Collecting identify>=1.0.0 (from pre-commit>=4.3.0->instructor->ragas)\n","  Downloading identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)\n","Collecting nodeenv>=0.11.1 (from pre-commit>=4.3.0->instructor->ragas)\n","  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n","Collecting virtualenv>=20.10.0 (from pre-commit>=4.3.0->instructor->ragas)\n","  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->ragas) (1.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community->ragas) (3.3.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas)\n","  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas) (4.5.1)\n","Downloading ragas-0.4.1-py3-none-any.whl (419 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.9/419.9 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Downloading instructor-1.13.0-py3-none-any.whl (160 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.9/160.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-1.1.5-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-1.2.2-py3-none-any.whl (476 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.1/476.1 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_network-0.33.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.8/358.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pre_commit-4.5.1-py2.py3-none-any.whl (226 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ty-0.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cfgv-3.5.0-py2.py3-none-any.whl (7.4 kB)\n","Downloading identify-2.6.15-py2.py3-none-any.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n","Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m140.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: distlib, appdirs, virtualenv, ty, requests, nodeenv, mypy-extensions, marshmallow, jiter, identify, diskcache, cfgv, typing-inspect, scikit-network, pre-commit, dataclasses-json, langchain-core, instructor, langchain-text-splitters, langchain_openai, langchain-classic, langchain-community, ragas\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.4\n","    Uninstalling requests-2.32.4:\n","      Successfully uninstalled requests-2.32.4\n","  Attempting uninstall: jiter\n","    Found existing installation: jiter 0.12.0\n","    Uninstalling jiter-0.12.0:\n","      Successfully uninstalled jiter-0.12.0\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 1.1.3\n","    Uninstalling langchain-core-1.1.3:\n","      Successfully uninstalled langchain-core-1.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed appdirs-1.4.4 cfgv-3.5.0 dataclasses-json-0.6.7 diskcache-5.6.3 distlib-0.4.0 identify-2.6.15 instructor-1.13.0 jiter-0.11.1 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.2.2 langchain-text-splitters-1.1.0 langchain_openai-1.1.5 marshmallow-3.26.1 mypy-extensions-1.1.0 nodeenv-1.9.1 pre-commit-4.5.1 ragas-0.4.1 requests-2.32.5 scikit-network-0.33.5 ty-0.0.3 typing-inspect-0.9.0 virtualenv-20.35.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["requests"]},"id":"5c698153be894d2288c743331a65ad1b"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install langchain-huggingface"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pLW_qPxxe_0","executionInfo":{"status":"ok","timestamp":1766043282344,"user_tz":-180,"elapsed":4626,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"outputId":"75b26d1b-f75c-408a-a699-dc130ec08a17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-huggingface\n","  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n","Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.2.2)\n","Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.5)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.33)\n","Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.4.58)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.12.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (9.1.2)\n","Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.12.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.11.5)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.41.4)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.11.12)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (4.12.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.16.0)\n","Downloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n","Installing collected packages: langchain-huggingface\n","Successfully installed langchain-huggingface-1.2.0\n"]}]},{"cell_type":"code","source":["import json\n","from tqdm import tqdm\n","import contextlib\n","import io\n","\n","# Test verisini yükle\n","try:\n","    test_data\n","except NameError:\n","    with open(\"/content/drive/MyDrive/NLP/codes/data/test_data.json\", \"r\", encoding=\"utf-8\") as f:\n","        test_data = json.load(f)\n","\n","# --- TOKEN VE FORMAT KISITLAMASI ---\n","# Modele kesin bir dille 3 paragraf sınırı koyuyoruz.\n","constraint_suffix = \"\\n\\nCRITICAL INSTRUCTION: Answer this question in EXACTLY 3 paragraphs. Ensure the answer is concise and directly addresses the prompt.\"\n","\n","rag_results = []\n","baseline_results = []\n","\n","print(\"🚀 Veri üretimi (Generator: Qwen, Kısıt: 3 Paragraf) başlıyor...\")\n","\n","# Print çıktılarını gizlemek için context manager\n","with contextlib.redirect_stdout(io.StringIO()):\n","    for item in tqdm(test_data, desc=\"Sorular İşleniyor\"):\n","        original_query = item[\"question\"]\n","\n","        # Sorguyu modifiye et (Constraint ekle)\n","        modified_query = f\"{original_query}{constraint_suffix}\"\n","\n","        # --- A) RAG Modu ---\n","        try:\n","            rag_answer, rag_contexts_dicts = query_qwen(modified_query, use_rag=True, top_k=5)\n","            rag_context_texts = [ctx['text'] for ctx in rag_contexts_dicts]\n","\n","            rag_results.append({\n","                \"question\": original_query,\n","                \"answer\": rag_answer,\n","                \"contexts\": rag_context_texts,\n","                \"ground_truth\": item.get(\"ground_truth\", \"\")\n","            })\n","        except Exception as e:\n","            pass\n","\n","        # --- B) Baseline Modu ---\n","        try:\n","            base_answer, _ = query_qwen(modified_query, use_rag=False)\n","\n","            baseline_results.append({\n","                \"question\": original_query,\n","                \"answer\": base_answer,\n","                \"contexts\": [],\n","                \"ground_truth\": item.get(\"ground_truth\", \"\")\n","            })\n","        except Exception as e:\n","            pass\n","\n","# Verileri JSON olarak kaydet\n","output_file = \"/content/drive/MyDrive/NLP/codes/qwen_constrained_results_for_mistral_eval.json\"\n","combined_data = {\n","    \"rag\": rag_results,\n","    \"baseline\": baseline_results\n","}\n","\n","with open(output_file, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(combined_data, f, ensure_ascii=False, indent=4)\n","\n","print(f\"\\n✅ Veri üretimi tamamlandı: {output_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"tKRoIoMvmkRd","executionInfo":{"status":"error","timestamp":1766044034576,"user_tz":-180,"elapsed":752224,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"}},"outputId":"4b3bc078-d6ad-4642-9fe2-0914052e2ee4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rSorular İşleniyor:   0%|          | 0/50 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["🚀 Veri üretimi (Generator: Qwen, Kısıt: 3 Paragraf) başlıyor...\n"]},{"output_type":"stream","name":"stderr","text":["Sorular İşleniyor:  56%|█████▌    | 28/50 [12:31<09:50, 26.83s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2506911195.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# --- B) Baseline Modu ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mbase_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_qwen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodified_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_rag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             baseline_results.append({\n","\u001b[0;32m/tmp/ipython-input-3645678776.py\u001b[0m in \u001b[0;36mquery_qwen\u001b[0;34m(user_query, use_rag, top_k)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Uzun akademik metin için\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n","from datasets import Dataset\n","from ragas import evaluate\n","from ragas.metrics import faithfulness, answer_relevancy\n","import pandas as pd\n","import json\n","import gc\n","\n","# --- 1. MISTRAL JUDGE YÜKLEME ---\n","print(\"⚖️ Jüri Modeli (Mistral 7B) Yükleniyor...\")\n","\n","# BURAYI DÜZELTTİK: Qwen yerine Mistral ID'si\n","JUDGE_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","\n","try:\n","    judge_tokenizer = AutoTokenizer.from_pretrained(JUDGE_MODEL_ID, trust_remote_code=True)\n","    judge_model = AutoModelForCausalLM.from_pretrained(\n","        JUDGE_MODEL_ID,\n","        dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        trust_remote_code=True\n","    )\n","except Exception as e:\n","    print(f\"Model yüklenirken hata oluştu: {e}\")\n","    # Eğer gated model hatası alırsan HF_TOKEN secret'ını kontrol etmelisin.\n","\n","# Ragas için Pipeline (Repetition Penalty Eklendi)\n","judge_pipe = pipeline(\n","    \"text-generation\",\n","    model=judge_model,\n","    tokenizer=judge_tokenizer,\n","    max_new_tokens=1024,          # 512 bazen JSON’u yarıda kesiyor\n","    do_sample=False,              # kritik: JSON formatı için\n","    temperature=0.0,\n","    top_p=1.0,\n","    repetition_penalty=1.0,\n","    eos_token_id=judge_tokenizer.eos_token_id,\n","    pad_token_id=judge_tokenizer.eos_token_id,\n","    return_full_text=False\n",")\n","\n","mistral_judge = HuggingFacePipeline(pipeline=judge_pipe)\n","print(\"✅ Mistral Judge Hazır.\")\n","\n","# --- 2. EMBEDDING MODELİ ---\n","embeddings_model = HuggingFaceEmbeddings(\n","    model_name=\"BAAI/bge-m3\",\n","    model_kwargs={'device': 'cuda'},\n","    encode_kwargs={'normalize_embeddings': True}\n",")\n","\n","# --- 3. VERİYİ YÜKLE ---\n","# Dosya yolunu kendi yoluna göre güncelle\n","INPUT_FILE = \"/content/drive/MyDrive/NLP/codes/data/qwen_constrained_results_for_mistral_eval.json\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/NLP/codes/data/final_metrics_judge_mistral.csv\"\n","\n","with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","rag_dataset = Dataset.from_list(data[\"rag\"])\n","baseline_dataset = Dataset.from_list(data[\"baseline\"])\n","\n","# --- 4. DEĞERLENDİRME FONKSİYONU ---\n","def evaluate_with_mistral(dataset, metric_list, name):\n","    print(f\"\\n--- {name} Değerlendirmesi Başlıyor (Jüri: Mistral) ---\")\n","    results = []\n","\n","    for i in range(len(dataset)):\n","        sample = dataset.select([i])\n","        try:\n","            score = evaluate(\n","                dataset=sample,\n","                metrics=metric_list,\n","                llm=mistral_judge,\n","                embeddings=embeddings_model,\n","                raise_exceptions=True # Hata olursa görelim (artık parametrelerle düzelttik)\n","            )\n","            results.append(score.to_pandas())\n","            print(\".\", end=\"\", flush=True)\n","        except Exception as e:\n","            print(\"x\", end=\"\", flush=True)\n","            # Hata mesajını kısaltarak bas, ekranı doldurmasın\n","            print(f\"\\n[HATA - {name} Index {i}]: {str(e)[:100]}...\")\n","\n","    if results:\n","        return pd.concat(results, ignore_index=True)\n","    return pd.DataFrame()\n","\n","# --- 5. ÇALIŞTIR ---\n","\n","# RAG Değerlendirmesi\n","rag_df = evaluate_with_mistral(rag_dataset, [faithfulness, answer_relevancy], \"RAG\")\n","if not rag_df.empty:\n","    rag_df[\"mode\"] = \"RAG\"\n","\n","# Baseline Değerlendirmesi\n","baseline_df = evaluate_with_mistral(baseline_dataset, [answer_relevancy], \"Baseline\")\n","if not baseline_df.empty:\n","    baseline_df[\"mode\"] = \"Baseline\"\n","\n","# --- 6. RAPORLAMA ---\n","if not rag_df.empty or not baseline_df.empty:\n","    final_df = pd.concat([rag_df, baseline_df], ignore_index=True)\n","    final_df.to_csv(OUTPUT_FILE, index=False)\n","\n","    print(\"\\n\\n=== SONUÇ ÖZETİ (Jüri: Mistral) ===\")\n","    if 'mode' in final_df.columns:\n","        summary = final_df.groupby(\"mode\")[[\"faithfulness\", \"answer_relevancy\"]].mean()\n","        display(summary)\n","    else:\n","        print(final_df.mean(numeric_only=True))\n","\n","    print(f\"\\n✅ Sonuçlar '{OUTPUT_FILE}' dosyasına kaydedildi.\")\n","else:\n","    print(\"❌ Hiçbir değerlendirme tamamlanamadı.\")"],"metadata":{"id":"ZEepVAhxnxhd"},"execution_count":null,"outputs":[]}]}