{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ux4vPNKMLIDf"},"outputs":[],"source":["import os\n","import json\n","import re\n","from pathlib import Path\n","from typing import Dict, Any, List, Iterable\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21197,"status":"ok","timestamp":1766937745255,"user":{"displayName":"Hakan Demir","userId":"13532718680527459382"},"user_tz":-180},"id":"e3gTVX7HPYfa","outputId":"dda6aeaf-34bd-4b93-f908-020e6557487d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#If you want to access datas through google drive run this code section.\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSMcnSNBLTEn"},"outputs":[],"source":["REF_PATTERN = re.compile(r\"\\[(\\d+(?:\\s*,\\s*\\d+)*)\\]\")\n","PAREN_CITATION_PATTERN = re.compile(r\"\\(([^()]*\\d{4}[^()]*)\\)\")\n","AUTHOR_YEAR_PATTERN = re.compile(r\"(.+?),\\s*(\\d{4}[a-z]?)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRFo95bOy-O6"},"outputs":[],"source":["def extract_arxiv_info(paper_id: str, link_map: Dict[str, str]):\n","    \"\"\"\n","    Retrieves the PDF URL and extracts the publication year from the arXiv ID/URL.\n","\n","    Args:\n","      paper_id: The identifier of the paper (filename without extension).\n","      link_map: A dictionary mapping filenames to PDF URLs.\n","\n","    Returns:\n","      The PDF URL and the extracted year if found.\n","    \"\"\"\n","    pdf_key = f\"{paper_id}.pdf\"\n","    url = link_map.get(pdf_key)\n","\n","    year = None\n","\n","    if url:\n","        #arXiv URLs usually contain the pattern /YYMM.nnnn\n","        match = re.search(r\"/(\\d{4})\\.\\d+\", url)\n","        if match:\n","            yymm = match.group(1)\n","            yy = yymm[:2]\n","\n","            #Convert 2-digit year to 4-digit year\n","            try:\n","                year = int(\"20\" + yy)\n","            except ValueError:\n","                year = None\n","\n","    return url, year"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIRJvnthLe5f"},"outputs":[],"source":["def extract_numeric_reference_ids(text: str) -> List[int]:\n","  \"\"\"\n","  Parses the text to find numeric citation IDs like [1], [2, 3].\n","\n","  Args:\n","    text: The input text paragraph.\n","\n","  Returns:\n","    A sorted list of unique reference IDs found in the text.\n","  \"\"\"\n","  ids: List[int] = []\n","  for match in REF_PATTERN.findall(text):\n","    #Handle multiple citations in one bracket, e.g., \"1, 2\"\n","    parts = match.split(\",\")\n","    for p in parts:\n","      p = p.strip()\n","      if p.isdigit():\n","        ids.append(int(p))\n","\n","  return sorted(set(ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUwRBY-USy0T"},"outputs":[],"source":["def extract_author_year_citations(text: str) -> List[Dict[str, Any]]:\n","  \"\"\"\n","    Parses the text to find author-year style citations like (Author, 2020).\n","\n","    Args:\n","        text: The input text paragraph.\n","\n","    Returns:\n","        A list of dictionaries containing raw text, authors, and year.\n","    \"\"\"\n","  citations: List[Dict[str, Any]] = []\n","\n","  for paren_content in PAREN_CITATION_PATTERN.findall(text):\n","    #Handle multiple citations seperated by semicolons\n","    parts = re.split(r\";\", paren_content)\n","    for part in parts:\n","      part = part.strip()\n","      if not part:\n","        continue\n","\n","      m = AUTHOR_YEAR_PATTERN.search(part)\n","      if m:\n","        authors = m.group(1).strip()\n","        year = m.group(2).strip()\n","        citations.append({\n","            \"raw\": part,\n","            \"authors\": authors,\n","            \"year\": year,\n","        })\n","\n","  return citations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqhDt9jOL5yF"},"outputs":[],"source":["def iter_sections(\n","    section: Dict[str, Any],\n","    section_path: List[str]\n",") -> Iterable[Dict[str, Any]]:\n","  \"\"\"\n","    Recursively iterates through nested sections and subsections to yield paragraphs.\n","\n","    Args:\n","        section: The current section dictionary.\n","        section_path: The hierarchical path of section titles leading to this section.\n","\n","    Yields:\n","        A dictionary containing path, index, and text of a paragraph.\n","    \"\"\"\n","  current_path = section_path + [section.get(\"title\", \"\").strip()]\n","\n","  #Yield paragraphs in the current section\n","  for i, para in enumerate(section.get(\"paragraphs\", [])):\n","    yield {\n","        \"section_path\": current_path,\n","        \"para_index\": i,\n","        \"text\": para\n","    }\n","\n","  #Recursively process subsections\n","  for subsection in section.get(\"subsections\", []):\n","    yield from iter_sections(subsection, current_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6y2KX3LiORK"},"outputs":[],"source":["def resolve_inline_reference(\n","        authors: str,\n","        year: str,\n","        reference_map: Dict[int, str]\n","):\n","    \"\"\"\n","      Attempts to match an inline citation (Author, Year) to a reference in the bibliography.\n","\n","      Args:\n","          authors: Author string from the text.\n","          year: Year string from the text.\n","          reference_map: The parsed bibliography map {id: full_reference_text}.\n","\n","      Returns:\n","          The matched reference dictionary or None if not found.\n","    \"\"\"\n","    authors_l = authors.lower()\n","    year_l = year.lower()\n","\n","    #Checks if author and year exist in the reference text\n","    for rid, full_text in reference_map.items():\n","        ft_l = full_text.lower()\n","        if authors_l in ft_l and year_l in ft_l:\n","            return {\"id\": rid, \"text\": full_text}\n","\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsz2kuB0Mnqk"},"outputs":[],"source":["def process_single_paper(json_path: Path, link_map: Dict[str, str]) -> List[Dict[str, Any]]:\n","  \"\"\"\n","    Processes a single JSON paper file, extracting chunks and resolving references.\n","\n","    Args:\n","        json_path: Path to the input JSON file.\n","        link_map: Mapping for PDF links.\n","\n","    Returns:\n","        A list of processed chunks (dictionaries) ready for export.\n","    \"\"\"\n","  with json_path.open(\"r\", encoding=\"utf-8\") as f:\n","    paper = json.load(f)\n","\n","  #Header information extraction\n","  title = paper.get(\"title\", \"\").strip()\n","  authors = paper.get(\"authors\", [])\n","  paper_id = json_path.stem\n","\n","  # Get URL and Year (fallback to arXiv ID extraction if metadata year is missing)\n","  pdf_url, extracted_year = extract_arxiv_info(paper_id, link_map)\n","  year = paper.get(\"year\")\n","  if year is None:\n","    year = extracted_year\n","\n","  venue = paper.get(\"venue\")\n","\n","  #Build reference map (ID -> Text)\n","  paper_references = paper.get(\"references\", [])\n","  reference_map: Dict[int, str] = {}\n","  for ref in paper_references:\n","    try:\n","        rid = ref.get(\"id\")\n","        rtext = ref.get(\"text\")\n","        if rid is not None and rtext is not None:\n","            reference_map[int(rid)] = rtext\n","    except Exception:\n","        continue\n","\n","  chunks: List[Dict[str, Any]] = []\n","\n","  #Process abstract\n","  abstract_raw = paper.get(\"abstract\") or \"\"\n","  abstract_text = abstract_raw.strip()\n","\n","  if abstract_text:\n","    #Extract citations\n","    numeric_ref_ids = extract_numeric_reference_ids(abstract_text)\n","    author_year_cits = extract_author_year_citations(abstract_text)\n","\n","    #Resolve citations to actual bibliography entries\n","    resolved_numeric = [{\n","        \"id\": rid,\n","        \"text\": reference_map.get(rid)\n","        }\n","        for rid in numeric_ref_ids\n","        if rid in reference_map\n","    ]\n","\n","    resolved_inline = []\n","    for cit in author_year_cits:\n","        authors_c = cit[\"authors\"]\n","        year_c = cit[\"year\"]\n","        ref_match = resolve_inline_reference(authors_c, year_c, reference_map)\n","        #Avoid duplicates if already found via numeric ID or previous inline match\n","        if ref_match and ref_match not in resolved_numeric and ref_match not in resolved_inline:\n","            resolved_inline.append(ref_match)\n","\n","    resolved_references = resolved_numeric + resolved_inline\n","\n","    #Create abstract chunk\n","    chunk_id = f\"{paper_id}_abstract_p0\"\n","    chunks.append({\n","        \"chunk_id\": chunk_id,\n","        \"paper_id\": paper_id,\n","        \"title\": title,\n","        \"section_title\": \"Abstract\",\n","        \"section_path\": [\"Abstract\"],\n","        \"para_index\": 0,\n","        \"text\": abstract_text,\n","        \"reference_ids\": numeric_ref_ids,\n","        \"inline_citations\": author_year_cits,\n","        \"year\": year,\n","        \"venue\": venue,\n","        \"url\": pdf_url,\n","        \"authors\": authors,\n","        \"references\": resolved_references,\n","    })\n","\n","  #Process sections and subsections\n","  for sec_idx, sec in enumerate(paper.get(\"sections\", [])):\n","    #Use generator to flatten nested subsections\n","    for para_info in iter_sections(sec, []):\n","      section_path = para_info[\"section_path\"]\n","      para_index = para_info[\"para_index\"]\n","      text_raw = para_info[\"text\"] or \"\"\n","      text = text_raw.strip()\n","      if not text:\n","        continue\n","\n","      #Extract and resolve citations for this paragraph\n","      numeric_ref_ids = extract_numeric_reference_ids(text)\n","      author_year_cits = extract_author_year_citations(text)\n","\n","      resolved_numeric = [\n","            {\"id\": rid, \"text\": reference_map.get(rid)}\n","            for rid in numeric_ref_ids\n","            if rid in reference_map\n","        ]\n","\n","      resolved_inline = []\n","      for cit in author_year_cits:\n","          authors_c = cit[\"authors\"]\n","          year_c = cit[\"year\"]\n","          ref_match = resolve_inline_reference(authors_c, year_c, reference_map)\n","          if ref_match and ref_match not in resolved_numeric and ref_match not in resolved_inline:\n","              resolved_inline.append(ref_match)\n","\n","      resolved_references = resolved_numeric + resolved_inline\n","\n","      section_title = section_path[-1] if section_path else \"\"\n","\n","      chunk_id = f\"{paper_id}__sec{sec_idx}_p{para_index}\"\n","\n","      chunks.append({\n","          \"chunk_id\": chunk_id,\n","          \"paper_id\": paper_id,\n","          \"title\": title,\n","          \"section_title\": section_title,\n","          \"section_path\": section_path,\n","          \"para_index\": para_index,\n","          \"text\": text,\n","          \"reference_ids\": numeric_ref_ids,\n","          \"inline_citations\": author_year_cits,\n","          \"year\": year,\n","          \"venue\": venue,\n","          \"url\": pdf_url,\n","          \"authors\": authors,\n","          \"references\": resolved_references,\n","      })\n","\n","  return chunks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3cvFe_6ODNN"},"outputs":[],"source":["def build_all_chunks(data_dir: str, links_path: str, output_path: str) -> None:\n","  \"\"\"\n","    Main driver function to iterate over all JSON files and write the output JSONL.\n","\n","    Args:\n","        data_dir: Directory containing source JSON files.\n","        links_path: Path to the PDF links JSON mapping file.\n","        output_path: Destination path for the .jsonl output file.\n","    \"\"\"\n","  data_dir = Path(data_dir)\n","  out_path = Path(output_path)\n","\n","  #Ensure output directory exists\n","  out_path.parent.mkdir(parents=True, exist_ok=True)\n","\n","  print(f\"Loading link mapping from: {links_path}\")\n","  with open(links_path, \"r\", encoding=\"utf-8\") as f:\n","    link_map = json.load(f)\n","\n","  json_files = sorted(data_dir.glob(\"*.json\"))\n","\n","  with out_path.open(\"w\", encoding=\"utf-8\") as out_f:\n","    #Use tqdm for a progress bar\n","    for json_path in tqdm(json_files, desc=\"Processing papers\"):\n","      try:\n","        chunks = process_single_paper(json_path, link_map)\n","        for ch in chunks:\n","          #Write each chunk as a sepearte line in jsonl format\n","          out_f.write(json.dumps(ch, ensure_ascii=False) + \"\\n\")\n","      except Exception as e:\n","        #Log errors but do not stop the entire process\n","        print(f\"Error in {json_path}: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"pgycs2wiOn7O"},"outputs":[],"source":["ROOT_DIR = Path(\"drive/MyDrive/NLP/codes\")  # AcademicTextGenerator/\n","\n","build_all_chunks(\n","    data_dir=str(ROOT_DIR / \"data\" / \"jsons\"),\n","    links_path=str(ROOT_DIR / \"data\" / \"pdf_links_matching.json\"),\n","    output_path=str(ROOT_DIR / \"data\" / \"chunks\" / \"chunks.jsonl\"),\n",")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}