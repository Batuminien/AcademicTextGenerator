{
  "title": "Efficient Identification of High Similarity Clusters in Polygon Datasets",
  "authors": [
    {
      "firstname": "John",
      "surname": "Daras",
      "email": "ind2109@columbia.edu"
    }
  ],
  "abstract": "Advancements in tools like Shapely 2.0 and Triton can significantly improve the efficiency of spatial similarity computations by enabling faster and more scalable geometric operations [1], [2]. However, for extremely large datasets, these optimizations may face challenges due to the sheer volume of computations required. To address this, we propose a framework that reduces the number of clusters requiring verification, thereby decreasing the computational load on these systems. The framework integrates dynamic similarity index thresholding, supervised scheduling [3], and recall-constrained optimization to efficiently identify clusters with the highest spatial similarity while meeting user-defined precision and recall requirements [4]. By leveraging Kernel Density Estimation (KDE) to dynamically determine similarity thresholds [5] and machine learning models to prioritize clusters, our approach achieves substantial reductions in computational cost without sacrificing accuracy. Experimental results demonstrate the scalability and effectiveness of the method, offering a practical solution for large-scale geospatial analysis.",
  "sections": [
    {
      "title": "I. INTRODUCTION",
      "paragraphs": [
        "Geospatial data constitutes the cornerstone of numerous applications across various domains, including urban planning, environmental monitoring, infrastructure development, and medicine. For example, OpenStreetMap contains global data amounting to over 1.5 terabytes [6], while GeoNames describes more than 12 million locations, providing extensive point geometries such as latitude and longitude [7]. Expanding these datasets, geospatial knowledge graphs like YAGO2geo integrate millions of lines, polygons, and multipolygons from OpenStreetMap and administrative divisions [8], while WorldKG represents around 113.4 million geographic entities [9]. KnowWhereGraph, a more recent initiative, comprises over 12 billion RDF triples, including data on polygons and multipolygons, and supports applications in disaster relief, agricultural land use, and food-related supply chains [10]. Even cross-domain knowledge graphs such as DBpedia and Wikidata incorporate a substantial amount of geospatial information, underscoring the critical role of spatial data on the Web.",
        "Beyond these well-known repositories, spatial datasets also play a transformative role in medicine, particularly in the analysis and modeling of organ structures. For instance, the Visible Human Project provides high-resolution spatial data for anatomical structures [11], while the Human Connectome Project captures detailed spatial relationships within the brain [12]. Other datasets, such as the ACDC dataset, focus on cardiac MRI images to segment the heart [13], while the 3D Liver Tumor Dataset provides high-resolution spatial data for liver segmentation and tumor localization [14]. These datasets are essential for advancing medical imaging, surgical planning, and personalized medicine.",
        "Despite the prominence and growing volume of spatial data, processing and analyzing these datasets efficiently remains a significant challenge. For instance, while OpenStreetMap geometries are widely used, only a fraction of them-0.52% as of April 2021-are linked to other sources such as Wikidata, highlighting the lack of comprehensive integration across datasets. Similarly, in medical datasets, the complexity and volume of spatial data often lead to computational bottlenecks, particularly when identifying spatial similarities or patterns across large datasets.",
        "Identifying clusters of geometric objects with strong spatial similarities is a fundamental task in geospatial analysis, with applications spanning urban planning, environmental monitoring, infrastructure analysis, medicine, and global datasets. In urban planning, such clusters reveal patterns in land use, infrastructure, or building layouts, aiding targeted development and disaster management. In environmental monitoring, they highlight regions with similar vegetation, soil, or climate characteristics, supporting conservation and agricultural optimization. Medical applications benefit from clustering spatially similar organ shapes or tissue structures to identify abnormalities, plan surgeries, and personalize treatments. Similarly, in datasets like OpenStreetMap, clustering geometrically similar objects improves data integration and interlinking for navigation, disaster response, and supply chain optimization. Across these domains, clustering high-similarity geometries enables meaningful insights, resource optimization, and predictive modeling.",
        "To quantify this, each cluster is assigned a similarity index, calculated as the average pairwise similarity score of its objects. This index serves as a benchmark for determining which clusters exhibit the highest spatial resemblance, providing the foundation for downstream analysis and decisionmaking. In this work the similarity index is derived from robust spatial similarity metrics, which are designed to capture various geometric and spatial properties of the objects in a cluster. These metrics are grouped into four categories: (1) Overlap Metrics (e.g., Jaccard similarity [15], area similarity); (2) Shape Complexity Metrics (e.g., curvature, Fourier descriptors) [16], [17]; (3) Proportional and Spatial Metrics (e.g., aspect ratio similarity, bounding box distance) [18]; and (4) Boundary Metrics (e.g., perimeter, circularity [18]). Together, these metrics provide a comprehensive and nuanced measure of spatial similarity, enabling the accurate calculation of similarity indexes for clusters. However, calculating these similarity metrics for every cluster in a large dataset can be computationally expensive, particularly when the goal is to identify only the top percentage of high-similarity clusters.",
        "Recent advancements in tools like Triton and Shapely 2.0 offer opportunities to improve the efficiency of these calculations. Shapely 2.0 introduces vectorized operations, enabling geometric computations to be performed across multiple shapes simultaneously. By leveraging these capabilities, tasks such as calculating bounding boxes, intersections, and unions for large numbers of clusters can be completed more efficiently, reducing reliance on iterative loops in Python and improving throughput. In addition, Triton provides a way to harness GPU acceleration directly within Python, which can significantly reduce the computational overhead of tasks such as similarity metric calculations once geometric operations have been completed. Despite these optimizations, evaluating similarity metrics across large datasets remains demanding, underscoring the importance of reducing the number of required verifications.",
        "Traditionally, identifying clusters in the top percentage of similarity indexes (e.g., top 10%, 30%, or 50%) would require calculating the similarity indexes for all clusters, sorting them, and selecting the desired proportion-an approach that is computationally prohibitive for large datasets. To overcome this limitation, the proposed framework introduces Kernel Density Estimation (KDE) to dynamically determine the similarity index threshold corresponding to the desired top percentage. By estimating the distribution of similarity indexes from a representative sample of clusters, this method identifies the threshold directly, eliminating the need for exhaustive similarity calculations across the entire dataset.",
        "After determining the threshold, the framework employs a supervised scheduling approach to prioritize clusters for verification. During training, a sample of clusters is selected and their similarity indexes are computed. Clusters with similarity indexes above the threshold are labeled as \"high similarity indexed\" (class 1), while those below are labeled as \"low similarity indexed\" (class 0). A machine learning model is then trained using computationally lightweight features paired with these labels, enabling the model to predict which clusters are most likely to exceed the threshold. During verification, the model's predictions focus computational resources on the most promising candidates, reducing overall cost while maintaining high recall.",
        "To further enhance reliability, a recall-constrained optimiza-tion technique is applied to ensure the system achieves a high recall rate. This ensures that the framework reliably identifies all important clusters with high similarity, even if it allows for some false positives. By dynamically adjusting the classification threshold, this technique prioritizes recall over precision so that no high-similarity clusters are overlooked-particularly critical in applications such as urban planning, environmental monitoring, and infrastructure analysis.",
        "Contributions. This study tackles key challenges in identifying top-percentage high-similarity clusters efficiently. Our main contributions are:",
        "• Integration of Shapely 2.0 vectorized operations and Triton to enhance the computational efficiency of similarity metric calculations. • A robust methodology for calculating similarity indexes using diverse spatial similarity metrics. • A dynamic thresholding mechanism via KDE for identifying user-specified top-percentage clusters. • A supervised scheduling framework that efficiently classifies and prioritizes clusters. • A recall-constrained optimization approach to achieve high recall while maintaining computational efficiency.",
        "Extensive experiments validate the framework's ability to accurately identify top clusters while significantly reducing computational costs compared to traditional methods. The subsequent sections detail the methodology, supervised scheduling, recall-constrained optimization, and experimental results, highlighting the framework's scalability and effectiveness."
      ],
      "subsections": []
    },
    {
      "title": "II. RELATED WORK",
      "paragraphs": [
        "Identifying high-similarity clusters within large datasets is a challenging task that has drawn insights from multiple research areas, including geospatial analysis, supervised scheduling, and recall-constrained optimization. These fields provide foundational techniques that this work builds upon while addressing unique gaps, such as dynamic similarity index thresholding, machine learning integration, and recall optimization.",
        "The measurement of spatial similarity often forms the cornerstone of cluster analysis. Foundational works rely on metrics such as Jaccard similarity, area overlap, and boundary alignment to quantify spatial resemblance between geometries. Filtering-Verification frameworks have long been a staple in geospatial processing. For instance, Efficient Filtering Algorithms for Location-Aware Publish/Subscribe propose a scalable framework for filtering and verifying spatial objects in location-aware systems. The method divides spatial data into Minimum Bounding Rectangles (MBRs) and uses these rectangles to filter candidate pairs. To further improve performance, the system employs a publish/subscribe mechanism that tracks updates to spatial objects, ensuring that only the relevant pairs are processed. However, while the filtering step reduces computational overhead, the exhaustive verification phase remains computationally expensive for large-scale datasets.",
        "To address scalability challenges, Efficient Privacy-Preserving Spatial Range Query Over Outsourced Encrypted Data introduces a novel method for conducting spatial queries over encrypted datasets. The approach focuses on securely performing range queries while maintaining data confidentiality. The authors propose an efficient spatial indexing mechanism that partitions data into encrypted cells, allowing range-based filtering without decrypting the entire dataset. Although this work primarily addresses privacy concerns, its range querybased filtering emphasizes the importance of efficient indexing for large-scale spatial data, a concept central to high-similarity clustering.",
        "Progressive methods enhance scalability by prioritizing the processing of candidate pairs, particularly in resourceconstrained environments. Supervised Progressive GIA.nt extends the Filtering-Verification paradigm by introducing a machine learning-based scheduling component. The system trains a lightweight model using features such as area overlap, boundary alignment, and aspect ratios to assign scores to candidate pairs. These scores determine the verification order, ensuring that pairs with higher probabilities of topological relationships are processed first. By leveraging a supervised learning approach, this method reduces runtime while maintaining high recall. However, its focus on pairwise interlinking tasks rather than clustering limits its applicability to the challenges addressed in this paper.",
        "Parallel processing frameworks also play a critical role in handling massive spatial datasets. Big Spatial Data Processing Frameworks: Feature and Performance Evaluation evaluates the performance of distributed systems, such as Apache Spark, in processing spatial queries. The study compares various data partitioning strategies, including spatial R-trees and gridbased indexing, to optimize query execution time. The results highlight the importance of selecting appropriate indexing mechanisms for different dataset characteristics. While this work underscores the utility of parallelism, its focus on spatial queries rather than similarity-based clustering leaves room for improvement in cluster-level analyses.",
        "Dynamic thresholding techniques have been applied in domains requiring adaptive decision-making under data variability. For instance, Dynamic Thresholding and Recall-Constrained Optimization for Environmental Monitoring employs Kernel Density Estimation (KDE) to determine dynamic thresholds for identifying critical environmental events. By analyzing a sample of data points, the system estimates the distribution of key features, allowing thresholds to be adjusted dynamically. This reduces computational overhead while ensuring that significant events are not overlooked. The KDE-based dynamic thresholding mechanism aligns closely with this study's methodology for determining similarity index thresholds.",
        "Recall-constrained optimization is essential in applications where missing critical regions or clusters can have severe consequences. Ecological Thresholds: The Key to Successful Environmental Management or an Important Concept with No Practical Application explores the trade-offs between precision and recall in ecological monitoring. The study demonstrates that fixed thresholds often fail to capture dataset variability, leading to missed ecological changes. By incorporating adaptive thresholding, this work highlights the need for recallfocused strategies in data-driven decision-making. Similarly, Iterated Dynamic Thresholding Search for Packing Equal Circles into a Square introduces iterative threshold adjustments to optimize spatial layouts, ensuring that critical relationships between objects are preserved. This iterative mechanism provides insights into balancing computational efficiency with spatial accuracy.",
        "Recent advancements in computational tools, such as Shapely 2.0 and Triton, present opportunities to improve the efficiency of spatial similarity calculations in large-scale datasets. Shapely 2.0 introduces vectorized geometric operations, enabling the parallel processing of tasks like intersection, union, and area calculations. Triton facilitates highperformance GPU-accelerated computations for numerically intensive tasks, such as feature aggregation and normalization, making it especially valuable for handling large datasets. Prior studies, such as those on GPU-accelerated geometry and scalable spatial data mining, have demonstrated the potential of these technologies to enhance performance. However, the computational burden remains significant, underscoring the importance of reducing the number of required verifications. By combining these tools with dynamic thresholding and supervised scheduling, this work ensures computational efficiency while maintaining scalability and precision.",
        "While prior work in clustering, supervised scheduling, and recall optimization has individually advanced these fields, no existing framework integrates these methods for the specific purpose of identifying high-similarity clusters efficiently. This paper addresses this gap by combining dynamic similarity index thresholding using KDE, supervised scheduling with machine learning, and recall-constrained optimization into an end-to-end solution. Unlike existing works, this approach dynamically determines a similarity index threshold tailored to user-specified top percentages, trains a machine learning model on computationally efficient features to prioritize clusters, and applies recall-constrained optimization to ensure all critical clusters are identified. This integrated methodology fills a key gap in both geospatial and clustering literature, offering a scalable and robust solution for applications in geospatial analysis, environmental monitoring, and medical imaging."
      ],
      "subsections": []
    },
    {
      "title": "III. PRELIMINARIES",
      "paragraphs": [],
      "subsections": []
    },
    {
      "title": "A. Mathematical Formulation of the Similarity Index",
      "paragraphs": [
        "The similarity index quantifies the spatial resemblance ofobjects within a cluster. It is defined as the average pairwise similarity score between all objects in a cluster C:",
        "• |C| is the number of objects in the cluster.",
        "• O i and O j are geometric objects within the cluster.",
        "• Similarity(O i , O j ) is the pairwise similarity score, computed using the spatial similarity metrics described in Section 4.2. This index provides a single value summarizing the spatial consistency of a cluster. Clusters with higher similarity indexes exhibit stronger spatial alignment, uniformity in shape, and proportional characteristics.",
        "B. Spatial Similarity Metrics a) Polygon Centering: Before computing pairwise similarities, all polygons are preprocessed to ensure that their positions in space do not influence similarity calculations. This involves translating each polygon such that its centroid aligns with the origin. The steps include:",
        "1. Centroid Calculation: For each polygon, its centroid is determined.",
        "2. Translation: The polygon is translated by shift ing its centroid to the origin using affine transformations.",
        "Purpose: This preprocessing ensures that spatial translations (e.g., the location of polygons in a global coordinate system) do not affect similarity computations. By focusing solely on intrinsic geometric and spatial characteristics, such as shape, boundary complexity, and area, this step ensures consistency and fairness in similarity calculations.",
        "Rationale: The absolute position of a polygon is irrelevant for many applications, such as clustering or pattern analysis. Centering allows the framework to prioritize geometric properties, making it particularly effective for applications in urban planning, environmental monitoring, and medical imaging The pairwise similarity score Similarity(O i , O j ) is computed using a combination of metrics that capture different geometric and spatial properties of polygons. These metrics include: :contentReference[oaicite:2]index=2 1) Jaccard Similarity",
        "Purpose: Measures the overlap between two polygons. Why Used: Highlights overlap-based relationships while penalizing disjoint or minimally overlapping shapes, making it critical for identifying clusters with shared spatial regions. Formula:",
        "2) Area Similarity Purpose: Emphasizes proportional overlap by accounting for the relative scales of the polygons. Why Used: Balances similarity evaluation for polygons of varying sizes, unlike Jaccard similarity, which can under emphasize smaller overlaps. Formula:",
        "3) Curvature Similarity Purpose: Quantifies boundary complexity, differentiating between simple and intricate shapes.",
        "Why Used: Ensures that clusters are sensitive to the level of detail in object boundaries, which is essential for complex geometric objects. Formula:",
        "4) Fourier Descriptor Similarity Purpose: Encodes global shape geometry into a frequency domain.",
        "Why Used: Provides robustness against minor distortions, making it ideal for comparing shapes with slight variations. Formula:"
      ],
      "subsections": []
    },
    {
      "title": "5) Aspect Ratio Similarity",
      "paragraphs": [
        "Purpose: Reflects differences in elongation or orientation of bounding boxes.",
        "Why Used: Adds a proportionality-based perspective, useful for distinguishing between elongated and compact shapes. Formula:"
      ],
      "subsections": []
    },
    {
      "title": "6) Perimeter Similarity",
      "paragraphs": [
        "Purpose: Focuses on the boundary lengths of polygons.",
        "Why Used: Captures size-related differences and complements area-based metrics. Formula:",
        "7) Bounding Box Distance Purpose: Measures the spatial proximity between bounding box centers. Why Used: Crucial for clustering applications where nearby objects are more likely to be related. Formula:"
      ],
      "subsections": []
    },
    {
      "title": "8) Polygon Circularity Similarity",
      "paragraphs": [
        "Purpose: Highlights differences in roundness between polygons.",
        "Why Used: Distinguishes between regular shapes (e.g., circles) and irregular ones, providing nuanced insights into shape characteristics. Formulas:"
      ],
      "subsections": []
    },
    {
      "title": ") 9) Combined Similarity",
      "paragraphs": [
        "Purpose: Aggregates the above metrics into a single similarity score using weighted contributions.",
        "Why Used: Balances individual metrics to deliver a robust and comprehensive similarity measure. Formula:"
      ],
      "subsections": []
    },
    {
      "title": "C. Supervised Scheduling",
      "paragraphs": [
        "Supervised scheduling is a key optimization technique employed in this framework to identify high-similarity clusters efficiently. Rather than exhaustively calculating similarity indexes for all clusters, supervised scheduling leverages machine learning to prioritize clusters likely to exceed the similarity index threshold.",
        "In this framework, a machine learning model is trained using computationally lightweight features extracted from clusters and binary labels indicating whether a cluster exceeds the similarity index threshold. During prediction, the model assigns a likelihood score to each cluster, allowing the system to focus on the most promising candidates.",
        "This approach significantly reduces computational costs while ensuring high recall, making it particularly effective for large datasets and applications where computational efficiency is paramount. A detailed description of the supervised scheduling algorithm is provided in Section 4."
      ],
      "subsections": []
    },
    {
      "title": "D. Kernel Density Estimation for Similarity Index Thresholding",
      "paragraphs": [
        "Kernel Density Estimation (KDE) is a non-parametric statistical method used to estimate the probability density function (PDF) of a dataset. In this framework, KDE plays a pivotal role in dynamically determining the similarity index threshold, which defines the boundary between high-similarity and lowsimilarity clusters.",
        "Purpose: The similarity index threshold is essential for identifying clusters within the top percentage of similarity indexes, as specified by the user. For example, to identify the top 10% of clusters, the threshold is set such that only 10% of similarity indexes in the dataset are above it.",
        "How KDE is Applied: A representative sample of clusters is selected from the dataset. Similarity indexes are calculated for this sample using robust spatial similarity metrics. KDE is applied to estimate the probability density function of the similarity indexes, providing a smooth approximation of their distribution. Finally, the threshold is determined by finding the similarity index value such that the area under the PDF above this value corresponds to the desired percentage (e.g., 10% or 30%).",
        "Advantages of KDE: KDE offers several advantages in this framework. First, it dynamically adapts to the underlying distribution of similarity indexes, allowing the framework to handle diverse datasets without relying on static thresholds. Second, it is computationally efficient because it estimates the threshold from a representative sample, eliminating the need to compute similarity indexes for the entire dataset. Finally, KDE provides flexibility, supporting varying userdefined percentages, making it applicable across different use cases.",
        "KDE ensures that the threshold dynamically adapts to the datasetˆas characteristics while maintaining computational efficiency. A detailed explanation of the KDE process and its integration into the overall framework is provided in Section 4."
      ],
      "subsections": []
    },
    {
      "title": "IV. APPROACH A. Features for Supervised Scheduling",
      "paragraphs": [
        "For each cluster, we define a representative geometry g, which serves as a reference for computing cluster-wide properties. Geometry g is either a polygon or a linestring, chosen such that its envelope (bounding box) intersects with the envelopes of all other geometries in the cluster. This ensures that geometry g encapsulates the spatial footprint of the cluster, providing a unified basis for feature computation.",
        "The representative geometry g plays a central role in simplifying the characterization of clusters. By relying on a single geometry that spatially relates to all cluster members, the framework can compute cluster-wide features efficiently while maintaining consistency across diverse datasets. The supervised scheduling framework relies on a comprehensive set of features to represent each cluster. These features are extracted from:",
        "1) Source Geometries. These are the geometries within the cluster. 2) Target Geometry. This is the representative geometry g, defined as the geometry whose envelope intersects with the envelopes of all geometries in the cluster. The extracted features are lightweight, focusing on computational efficiency while still capturing critical spatial and geometric properties of the clusters. Lightweight features are essential to reduce computational overhead, enabling the machine learning model to operate effectively on large datasets without compromising accuracy or scalability. The 15 features we extract are inspired from the paper Supervised Progressive GIA.nt [3]. We extract the following features for each pair of source geometry (cluster geometry) and target geometry (representative geometry g): Min-Max Values for Individual Geometries are the minimum and maximum values of each feature F 1 to F 15, computed across all individual geometries in the dataset. They define the range for normalizing individual features for each geometry. For example, the minimum value for the envelope area (F1) corresponds to the smallest envelope area across all geometries in the dataset, while the maximum value corresponds to the largest. The normalization formula for individual geometries is:",
        "• N F i,j is the normalized feature for geometry i and feature j. • F V i,j is the feature value for geometry i and feature j.",
        "• min V al j is the minimum value of feature j across all geometries. • max V al j is the maximum value of feature j across all geometries.",
        "Min-Max Values for Clusters (Cluster-Level Min-Max) are the minimum and maximum values of aggregated cluster features, computed across all clusters in the dataset. Aggregated features (mean values for each feature in a cluster) are normalized using these values to ensure comparability across clusters. The formula for cluster-level normalization is:",
        "• N CF j is the normalized cluster feature for feature j.",
        "• N F j is the mean of normalized feature N F i,j across all geometries in the cluster. • min Cf j is the minimum value of the aggregated feature j across all clusters. • max Cf j is the maximum value of the aggregated feature j across all clusters. After applying the above process, we derive a total of 16 final features for each cluster. These normalized features will later be used to train the machine learning model.",
        "Purpose of Normalization: The two-step formulas are designed to improve the accuracy of the machine learning model by ensuring that features are consistent across scales. This helps to reduce the impact of variations in raw feature magnitudes, which can arise from the diverse sizes, shapes, and scales of geometries in the dataset. By applying these formulas, the features can be compared in a more uniform way. At the individual geometry level, the formulas help features capture meaningful patterns while reducing the influence of extreme values. At the cluster level, the formulas ensure that aggregated features incorporate information not only from within the cluster but also relative to the overall range of values observed across all clusters in the dataset. This broader perspective helps the model identify clusters with high similarity indices more effectively."
      ],
      "subsections": []
    },
    {
      "title": "B. Algorithm",
      "paragraphs": [
        "Cluster Finding (lines 1-24): The Cluster Finding step organizes geometries into clusters and prepares data for subsequent phases of the algorithm. This step involves two main tasks: indexing the source geometries and identifying candidate clusters by associating target geometries with relevant source geometries.",
        "First, an index I is initialized to store the spatial positions of source geometries. The granularity of the index grid is determined by the function Def ineIndexGranularity(S), which computes the grid resolution based on the properties of the source dataset S. Each geometry s ∈ S is added to the index I using AddT oIndex(I, s). This indexing enables efficient spatial queries in subsequent steps.",
        "Next, several data structures are initialized. The total number of potential geometry pairs is computed as D = |S| × |T |, where |S| and |T | are the sizes of the source and target datasets, respectively. The sourceStats structure stores statistics for each source geometry, such as the number of distinct and real pairs. Two subsets, sample and verif ication sample, are created using the function RandomGenerator(m, D) to randomly select geometry pairs for KDE modeling and validation."
      ],
      "subsections": []
    },
    {
      "title": "Algorithm 1 Algorithm",
      "paragraphs": [
        "Input: the source dataset S, the target dataset T , the feature set F , the maximum sample size m, the class size N , the probabilistic classification algorithm A, desired recall r d , similarity range similarity range, Output: the links L R = {(c, s) ⊆ C × S : s(c)} CS ← CS ∪ GetTileContents(I, i, j) The algorithm then iterates over each target geometry t ∈ T to identify candidate clusters. For each target geometry t, a set CS is initialized to store potential source geometry candidates whose bounding boxes intersect with the bounding box of t. The diagonal corners of t's bounding box, (x 1 , y 1 ) and (x 2 , y 2 ), are computed using GetDiagCorners(t). The bounding box is then divided into grid cells, and for each grid cell, the contents of the corresponding tiles in the index I are added to CS.",
        "For each geometry s ∈ CS, the sourceStats structure is updated to track the total number of distinct pairs involving s using U pdateT otalDistinctP airs(sourceStats, s). If the bounding boxes of s and t intersect, the sourceStats structure is further updated with the count of real pairs using U pdateRealP airs(sourceStats, s), and the geometry s is added to a set of candidates for the cluster. If the pair ID is part of the randomly generated sample set, the pair (s, t) is added to both the sample and kde sample subsets for subsequent analysis. Finally, the set of candidate geometries for the current target geometry t is added to allClusters, which stores all identified clusters. This prepares the dataset for further processing, including the KDE modeling and training phases.",
        "The Cluster Finding step is crucial for efficiently associating source and target geometries. By leveraging spatial indexing and grid-based filtering, the algorithm focuses only on relevant candidate geometries, reducing computational overhead and facilitating the identification of meaningful clusters for later stages of the process. KDE Phase to Find Minimum Similarity Threshold (lines 25-29): The KDE phase estimates the minimum similarity threshold required to identify clusters with high similarity indices. This step begins by iterating through all clusters in the kde sample, which is a representative subset of the dataset. For each cluster, the similarity index is computed using the function SimilarityCalculator(Cluster). The computed similarity indices are stored in a list.",
        "Once all similarity indices are collected, a Kernel Density Estimation (KDE) model is applied to fit the distribution of similarity indices using the GetBestM odel(List) function. KDE provides a smooth estimation of the underlying probability density function, enabling precise identification of thresholds. Using this model, the EstimateT hreshold(KDE, similarity range) function determines the minimum similarity threshold, which corresponds to the desired similarity range specified by the user (e.g., top 10% or top 50% similarity indices). This threshold will be used as a benchmark in the subsequent training and verification phases.",
        "Training Phase (lines 30-38): The training phase aims to label clusters and train a machine learning model to classify clusters efficiently. Two sets, negClusters (negative clusters) and posClusters (positive clusters), are initialized to store clusters with similarity indices below and above the threshold, respectively. The sample and verif ication sample are shuffled to ensure a balanced and unbiased representation of the dataset.",
        "The algorithm iterates through the clusters in the sample. For each cluster, the similarity index is computed using SimilarityCalculator(Cluster). If the similarity index is greater than or equal to the minimum similarity threshold, the cluster is added to posClusters; otherwise, it is added to negClusters. This process continues until both sets contain at least N clusters, ensuring a sufficient number of labeled samples for training.",
        "Once the clusters are labeled, features are extracted from the union of posClusters and negClusters using the GetF eatures(posClusters ∪ negClusters, F, sourceStats, I) function. This step generates a comprehensive feature set for training the machine learning model. The labeled data is then used to train the probabilistic classifier M using the T rain(L) function, where L represents the labeled dataset with extracted features. After training, key data structures are initialized for the subsequent verification phase: L R , which will store the final results; minw, which tracks the minimum weight; T C, which stores total clusters; and P Q, a priority queue The KDE and training phases are critical to efficiently identifying high-similarity clusters. KDE reduces computational overhead by estimating a data-driven threshold, while the training phase equips the framework with a model capable of classifying clusters based on lightweight, extracted features.",
        "Estimate Recall and Simulate Verification Process in a Small Sample (lines 39-58): This phase evaluates the system's ability to identify highsimilarity clusters and estimates the recall under simulated verification. The process begins by iterating through all clusters in the kde sample, a representative subset of the dataset. For each cluster, the feature vector u is generated using the GetF eatureV ector(Cluster, F ) function, which extracts the lightweight features defined earlier. The trained classifier M is then used to compute the classification probability w s,t for the cluster. This probability represents the likelihood that the cluster belongs to the high-similarity category. Each cluster, along with its classification probability, is added to a priority queue P Q, which ranks clusters by their weights w s,t . The size of P Q is limited to N , the desired number of top clusters to process. The number of clusters with similarity indices above the minimum similarity threshold is calculated using the N umberAboveSimilarityT hreshold(P Q) function, which identifies the count of clusters classified as highly similar.",
        "Next, the algorithm simulates the verification process using the priority queue. Clusters are processed in descending order of their classification probabilities by repeatedly removing the highest-ranked cluster from P Q using P Q.popLast(). For each cluster, the similarity index is recalculated using the SimilarityCalculator(c) function. If the recalculated similarity index meets or exceeds the minimum similarity threshold, the counter for high-similarity predictions is incremented. The simulation stops when the counter reaches a predefined value equal to similarity range • N . This ensures that the simulation is bounded by the specified similarity range. The approximate recall, recall approx, is then computed as the ratio of correctly predicted high-similarity clusters (highSimilarityP rediction) to the total number of actual high-similarity clusters (HighSimIndices). Finally, the max size parameter, which determines the maximum number of clusters to verify during the full verification phase, is estimated using the formula:",
        ") where r d is the desired recall rate. This formula adjusts the maximum verification size based on the estimated recall, the total number of high-similarity indices, and the desired recall proportion.",
        "Verification Phase (lines 59-78): The verification phase validates clusters by identifying those with high similarity indices. The process begins by initializing an empty priority queue P Q. For each cluster in the dataset, the feature vector u is computed using the predefined feature set F . The trained machine learning model M is then used to calculate the probability w s,t of the cluster being a high-similarity cluster. Each cluster, along with its probability, is added to P Q, prioritizing clusters with higher probabilities.",
        "The algorithm iteratively processes clusters in descending order of their classification probabilities. The cluster c with the highest probability is removed from P Q using P Q.popLast(), and its actual similarity index I M is calculated. If I M exceeds or equals the minimum similarity threshold, the cluster is added to the result set L R . The process continues until the counter reaches max size or P Q is empty. Once the priority queue is exhausted or the size constraint is met, the result set L R , containing the verified high-similarity clusters, is returned. This phase efficiently identifies the most promising clusters while adhering to recall constraints and computational limits."
      ],
      "subsections": []
    },
    {
      "title": "V. EXPERIMENTAL ANALYSIS",
      "paragraphs": [
        "The experimental analysis was conducted using a v28 TPU machine with 334.56 GB of RAM, utilizing Python 3.9 as the programming language. The experiments were implemented on Google Colab. This setup allowed to evaluate the proposed framework's performance on diverse datasets with varying sizes and complexities effectively. Additionally, the algorithm was executed without utilizing Triton for its computations, and further performance comparisons were conducted independently.",
        "The experimental analysis evaluates the performance of the proposed framework on three datasets with varying sizes, complexities, and average cluster sizes. These differences allow us to explore the framework's scalability and its ability to handle diverse conditions effectively.",
        "Dataset D1 includes 229,276 source and 583,833 target geometries, forming a total of 295,481 clusters with an average size of approximately six polygons per cluster. Dataset D2 contains 210,483 source geometries and 2,898,899 target geometries, resulting in 654,196 clusters with an average cluster size of around 13 polygons. Finally, Dataset D3 comprises 200,294 source geometries and 7,392,699 target geometries, yielding 1,324,980 clusters with an average size of 34 polygons.",
        "Regarding the distribution of similarity ranges, in Dataset D1, most clusters fall within the 40-70% similarity range, while clusters with similarity indices above 90% represent a very small fraction-only 65 out of nearly 300,000 clusters. Dataset D2 follows a similar pattern, with the majority concentrated in the 50-70% range and less than 0.5% of clusters in the 90-100% similarity range. Dataset D3 demonstrates a more skewed distribution, with the majority in the 50-70% range and an even smaller proportion of high-similarity clusters. These results suggest that highly similar clusters are rare, which underscores the importance of using targeted methods to identify them efficiently.",
        "We also examined the dataset coverage required to identify clusters within specific top percentages, such as the top 10%, 30%, and 50%. In Dataset D1, identifying the top 10% of clusters required processing ∼32% of the dataset, while ∼45% and ∼60% were processed to identify the top 30% and 50%, respectively. Dataset D2 required ∼41%, ∼55%, and ∼70%, and Dataset D3 required ∼68%, ∼85%, and ∼94% for the same ranges. These findings demonstrate the adaptability of the framework to prioritize computational resources, though larger and more complex datasets naturally require greater processing effort.",
        "Further insights emerge from analyzing the analogy between the percentage of clusters checked and the percentage of clusters targeted. For Dataset D1, the ratio of checked to targeted clusters was approximately 3.25 for the top 10% and decreased as the target percentage increased, reflecting improved efficiency for broader ranges. Similarly, Dataset D2 exhibited a ratio of ∼4 for the top 10%, decreasing to ∼1.8 for the top 50%. Dataset D3 began at ∼6.8 for the top 10% but also demonstrated decreasing ratios, reaching ∼1.9 for the top 50%. These trends indicate that while larger and more complex datasets pose challenges, the framework effectively allocates effort based on the desired coverage. The relationship between average cluster size and dataset coverage reveals a direct impact on computational requirements. Larger clusters, such as those in Dataset D3, demand the processing of a much larger percentage of the dataset to achieve the same recall levels as smaller clusters. For example, identifying the top 50% of clusters in Dataset D3 required processing ∼94% of the dataset, compared to ∼60% in Dataset D1. This highlights the challenges of handling larger cluster sizes and underscores the necessity of employing optimization techniques."
      ],
      "subsections": []
    },
    {
      "title": "Insights on Shapely 2.0 and Triton",
      "paragraphs": [
        "To further support these analyses, Shapely 2.0 offers notable advancements in geometric operations. By enabling vectorized processing, Shapely 2.0 processes arrays of geometries in a single operation, avoiding the overhead of handling each geometry sequentially.",
        "Performance Improvements: Vectorized operations in Shapely 2.0 are significantly faster, offering up to threefold improvements in tasks such as calculating Minimum Bounding Rectangles, intersections, and unions. These enhancements help reduce the computational time for critical pre-processing steps in this study.",
        "Enhanced GeoPandas Integration: The improved integration with GeoPandas enables faster spatial joins and overlays, particularly useful when handling large datasets like those analyzed here. This compatibility facilitates efficient preparation of geometries for similarity computations.",
        "Scalability for Large Datasets: Shapely 2.0's optimizations make it feasible to process large-scale spatial datasets efficiently. While these improvements enhance computational performance, they must be combined with targeted methods to manage the overall computational workload effectively.",
        "By integrating these tools into the proposed framework, the computational overhead associated with geometric operations is reduced, making it feasible to address scalability challenges. However, while these advancements are promising, their application requires careful evaluation to ensure alignment with the framework's objectives.",
        "In addition to the above, Triton was evaluated separately for computing similarity metrics independently of the algorithm. This evaluation involved directly computing similarity metrics using averages to gauge potential speedups. Triton demon-strated speed improvements compared to standard Python code, especially when handling large-scale parallelizable operations. While these results are promising, they also highlight the need to reduce the number of required computations within the algorithm to ensure that Triton's parallel processing capabilities are utilized efficiently.",
        "However, it is important to acknowledge that incorporating Triton into the broader framework requires careful optimization. The complexity of similarity computations, particularly in large datasets, can still pose challenges if left unoptimized. By leveraging Triton's strengths in highly parallel operations while minimizing redundant calculations, future work aims to fully harness its capabilities in conjunction with tools like Shapely 2.0. These combined efforts hold promise for achieving a scalable and efficient solution for identifying highsimilarity clusters in large datasets."
      ],
      "subsections": []
    }
  ],
  "body_paragraphs": [
    "Geospatial data constitutes the cornerstone of numerous applications across various domains, including urban planning, environmental monitoring, infrastructure development, and medicine. For example, OpenStreetMap contains global data amounting to over 1.5 terabytes [6], while GeoNames describes more than 12 million locations, providing extensive point geometries such as latitude and longitude [7]. Expanding these datasets, geospatial knowledge graphs like YAGO2geo integrate millions of lines, polygons, and multipolygons from OpenStreetMap and administrative divisions [8], while WorldKG represents around 113.4 million geographic entities [9]. KnowWhereGraph, a more recent initiative, comprises over 12 billion RDF triples, including data on polygons and multipolygons, and supports applications in disaster relief, agricultural land use, and food-related supply chains [10]. Even cross-domain knowledge graphs such as DBpedia and Wikidata incorporate a substantial amount of geospatial information, underscoring the critical role of spatial data on the Web.",
    "Beyond these well-known repositories, spatial datasets also play a transformative role in medicine, particularly in the analysis and modeling of organ structures. For instance, the Visible Human Project provides high-resolution spatial data for anatomical structures [11], while the Human Connectome Project captures detailed spatial relationships within the brain [12]. Other datasets, such as the ACDC dataset, focus on cardiac MRI images to segment the heart [13], while the 3D Liver Tumor Dataset provides high-resolution spatial data for liver segmentation and tumor localization [14]. These datasets are essential for advancing medical imaging, surgical planning, and personalized medicine.",
    "Despite the prominence and growing volume of spatial data, processing and analyzing these datasets efficiently remains a significant challenge. For instance, while OpenStreetMap geometries are widely used, only a fraction of them-0.52% as of April 2021-are linked to other sources such as Wikidata, highlighting the lack of comprehensive integration across datasets. Similarly, in medical datasets, the complexity and volume of spatial data often lead to computational bottlenecks, particularly when identifying spatial similarities or patterns across large datasets.",
    "Identifying clusters of geometric objects with strong spatial similarities is a fundamental task in geospatial analysis, with applications spanning urban planning, environmental monitoring, infrastructure analysis, medicine, and global datasets. In urban planning, such clusters reveal patterns in land use, infrastructure, or building layouts, aiding targeted development and disaster management. In environmental monitoring, they highlight regions with similar vegetation, soil, or climate characteristics, supporting conservation and agricultural optimization. Medical applications benefit from clustering spatially similar organ shapes or tissue structures to identify abnormalities, plan surgeries, and personalize treatments. Similarly, in datasets like OpenStreetMap, clustering geometrically similar objects improves data integration and interlinking for navigation, disaster response, and supply chain optimization. Across these domains, clustering high-similarity geometries enables meaningful insights, resource optimization, and predictive modeling.",
    "To quantify this, each cluster is assigned a similarity index, calculated as the average pairwise similarity score of its objects. This index serves as a benchmark for determining which clusters exhibit the highest spatial resemblance, providing the foundation for downstream analysis and decisionmaking. In this work the similarity index is derived from robust spatial similarity metrics, which are designed to capture various geometric and spatial properties of the objects in a cluster. These metrics are grouped into four categories: (1) Overlap Metrics (e.g., Jaccard similarity [15], area similarity); (2) Shape Complexity Metrics (e.g., curvature, Fourier descriptors) [16], [17]; (3) Proportional and Spatial Metrics (e.g., aspect ratio similarity, bounding box distance) [18]; and (4) Boundary Metrics (e.g., perimeter, circularity [18]). Together, these metrics provide a comprehensive and nuanced measure of spatial similarity, enabling the accurate calculation of similarity indexes for clusters. However, calculating these similarity metrics for every cluster in a large dataset can be computationally expensive, particularly when the goal is to identify only the top percentage of high-similarity clusters.",
    "Recent advancements in tools like Triton and Shapely 2.0 offer opportunities to improve the efficiency of these calculations. Shapely 2.0 introduces vectorized operations, enabling geometric computations to be performed across multiple shapes simultaneously. By leveraging these capabilities, tasks such as calculating bounding boxes, intersections, and unions for large numbers of clusters can be completed more efficiently, reducing reliance on iterative loops in Python and improving throughput. In addition, Triton provides a way to harness GPU acceleration directly within Python, which can significantly reduce the computational overhead of tasks such as similarity metric calculations once geometric operations have been completed. Despite these optimizations, evaluating similarity metrics across large datasets remains demanding, underscoring the importance of reducing the number of required verifications.",
    "Traditionally, identifying clusters in the top percentage of similarity indexes (e.g., top 10%, 30%, or 50%) would require calculating the similarity indexes for all clusters, sorting them, and selecting the desired proportion-an approach that is computationally prohibitive for large datasets. To overcome this limitation, the proposed framework introduces Kernel Density Estimation (KDE) to dynamically determine the similarity index threshold corresponding to the desired top percentage. By estimating the distribution of similarity indexes from a representative sample of clusters, this method identifies the threshold directly, eliminating the need for exhaustive similarity calculations across the entire dataset.",
    "After determining the threshold, the framework employs a supervised scheduling approach to prioritize clusters for verification. During training, a sample of clusters is selected and their similarity indexes are computed. Clusters with similarity indexes above the threshold are labeled as \"high similarity indexed\" (class 1), while those below are labeled as \"low similarity indexed\" (class 0). A machine learning model is then trained using computationally lightweight features paired with these labels, enabling the model to predict which clusters are most likely to exceed the threshold. During verification, the model's predictions focus computational resources on the most promising candidates, reducing overall cost while maintaining high recall.",
    "To further enhance reliability, a recall-constrained optimiza-tion technique is applied to ensure the system achieves a high recall rate. This ensures that the framework reliably identifies all important clusters with high similarity, even if it allows for some false positives. By dynamically adjusting the classification threshold, this technique prioritizes recall over precision so that no high-similarity clusters are overlooked-particularly critical in applications such as urban planning, environmental monitoring, and infrastructure analysis.",
    "Contributions. This study tackles key challenges in identifying top-percentage high-similarity clusters efficiently. Our main contributions are:",
    "• Integration of Shapely 2.0 vectorized operations and Triton to enhance the computational efficiency of similarity metric calculations. • A robust methodology for calculating similarity indexes using diverse spatial similarity metrics. • A dynamic thresholding mechanism via KDE for identifying user-specified top-percentage clusters. • A supervised scheduling framework that efficiently classifies and prioritizes clusters. • A recall-constrained optimization approach to achieve high recall while maintaining computational efficiency.",
    "Extensive experiments validate the framework's ability to accurately identify top clusters while significantly reducing computational costs compared to traditional methods. The subsequent sections detail the methodology, supervised scheduling, recall-constrained optimization, and experimental results, highlighting the framework's scalability and effectiveness.",
    "Identifying high-similarity clusters within large datasets is a challenging task that has drawn insights from multiple research areas, including geospatial analysis, supervised scheduling, and recall-constrained optimization. These fields provide foundational techniques that this work builds upon while addressing unique gaps, such as dynamic similarity index thresholding, machine learning integration, and recall optimization.",
    "The measurement of spatial similarity often forms the cornerstone of cluster analysis. Foundational works rely on metrics such as Jaccard similarity, area overlap, and boundary alignment to quantify spatial resemblance between geometries. Filtering-Verification frameworks have long been a staple in geospatial processing. For instance, Efficient Filtering Algorithms for Location-Aware Publish/Subscribe propose a scalable framework for filtering and verifying spatial objects in location-aware systems. The method divides spatial data into Minimum Bounding Rectangles (MBRs) and uses these rectangles to filter candidate pairs. To further improve performance, the system employs a publish/subscribe mechanism that tracks updates to spatial objects, ensuring that only the relevant pairs are processed. However, while the filtering step reduces computational overhead, the exhaustive verification phase remains computationally expensive for large-scale datasets.",
    "To address scalability challenges, Efficient Privacy-Preserving Spatial Range Query Over Outsourced Encrypted Data introduces a novel method for conducting spatial queries over encrypted datasets. The approach focuses on securely performing range queries while maintaining data confidentiality. The authors propose an efficient spatial indexing mechanism that partitions data into encrypted cells, allowing range-based filtering without decrypting the entire dataset. Although this work primarily addresses privacy concerns, its range querybased filtering emphasizes the importance of efficient indexing for large-scale spatial data, a concept central to high-similarity clustering.",
    "Progressive methods enhance scalability by prioritizing the processing of candidate pairs, particularly in resourceconstrained environments. Supervised Progressive GIA.nt extends the Filtering-Verification paradigm by introducing a machine learning-based scheduling component. The system trains a lightweight model using features such as area overlap, boundary alignment, and aspect ratios to assign scores to candidate pairs. These scores determine the verification order, ensuring that pairs with higher probabilities of topological relationships are processed first. By leveraging a supervised learning approach, this method reduces runtime while maintaining high recall. However, its focus on pairwise interlinking tasks rather than clustering limits its applicability to the challenges addressed in this paper.",
    "Parallel processing frameworks also play a critical role in handling massive spatial datasets. Big Spatial Data Processing Frameworks: Feature and Performance Evaluation evaluates the performance of distributed systems, such as Apache Spark, in processing spatial queries. The study compares various data partitioning strategies, including spatial R-trees and gridbased indexing, to optimize query execution time. The results highlight the importance of selecting appropriate indexing mechanisms for different dataset characteristics. While this work underscores the utility of parallelism, its focus on spatial queries rather than similarity-based clustering leaves room for improvement in cluster-level analyses.",
    "Dynamic thresholding techniques have been applied in domains requiring adaptive decision-making under data variability. For instance, Dynamic Thresholding and Recall-Constrained Optimization for Environmental Monitoring employs Kernel Density Estimation (KDE) to determine dynamic thresholds for identifying critical environmental events. By analyzing a sample of data points, the system estimates the distribution of key features, allowing thresholds to be adjusted dynamically. This reduces computational overhead while ensuring that significant events are not overlooked. The KDE-based dynamic thresholding mechanism aligns closely with this study's methodology for determining similarity index thresholds.",
    "Recall-constrained optimization is essential in applications where missing critical regions or clusters can have severe consequences. Ecological Thresholds: The Key to Successful Environmental Management or an Important Concept with No Practical Application explores the trade-offs between precision and recall in ecological monitoring. The study demonstrates that fixed thresholds often fail to capture dataset variability, leading to missed ecological changes. By incorporating adaptive thresholding, this work highlights the need for recallfocused strategies in data-driven decision-making. Similarly, Iterated Dynamic Thresholding Search for Packing Equal Circles into a Square introduces iterative threshold adjustments to optimize spatial layouts, ensuring that critical relationships between objects are preserved. This iterative mechanism provides insights into balancing computational efficiency with spatial accuracy.",
    "Recent advancements in computational tools, such as Shapely 2.0 and Triton, present opportunities to improve the efficiency of spatial similarity calculations in large-scale datasets. Shapely 2.0 introduces vectorized geometric operations, enabling the parallel processing of tasks like intersection, union, and area calculations. Triton facilitates highperformance GPU-accelerated computations for numerically intensive tasks, such as feature aggregation and normalization, making it especially valuable for handling large datasets. Prior studies, such as those on GPU-accelerated geometry and scalable spatial data mining, have demonstrated the potential of these technologies to enhance performance. However, the computational burden remains significant, underscoring the importance of reducing the number of required verifications. By combining these tools with dynamic thresholding and supervised scheduling, this work ensures computational efficiency while maintaining scalability and precision.",
    "While prior work in clustering, supervised scheduling, and recall optimization has individually advanced these fields, no existing framework integrates these methods for the specific purpose of identifying high-similarity clusters efficiently. This paper addresses this gap by combining dynamic similarity index thresholding using KDE, supervised scheduling with machine learning, and recall-constrained optimization into an end-to-end solution. Unlike existing works, this approach dynamically determines a similarity index threshold tailored to user-specified top percentages, trains a machine learning model on computationally efficient features to prioritize clusters, and applies recall-constrained optimization to ensure all critical clusters are identified. This integrated methodology fills a key gap in both geospatial and clustering literature, offering a scalable and robust solution for applications in geospatial analysis, environmental monitoring, and medical imaging.",
    "The similarity index quantifies the spatial resemblance ofobjects within a cluster. It is defined as the average pairwise similarity score between all objects in a cluster C:",
    "• |C| is the number of objects in the cluster.",
    "• O i and O j are geometric objects within the cluster.",
    "• Similarity(O i , O j ) is the pairwise similarity score, computed using the spatial similarity metrics described in Section 4.2. This index provides a single value summarizing the spatial consistency of a cluster. Clusters with higher similarity indexes exhibit stronger spatial alignment, uniformity in shape, and proportional characteristics.",
    "B. Spatial Similarity Metrics a) Polygon Centering: Before computing pairwise similarities, all polygons are preprocessed to ensure that their positions in space do not influence similarity calculations. This involves translating each polygon such that its centroid aligns with the origin. The steps include:",
    "1. Centroid Calculation: For each polygon, its centroid is determined.",
    "2. Translation: The polygon is translated by shift ing its centroid to the origin using affine transformations.",
    "Purpose: This preprocessing ensures that spatial translations (e.g., the location of polygons in a global coordinate system) do not affect similarity computations. By focusing solely on intrinsic geometric and spatial characteristics, such as shape, boundary complexity, and area, this step ensures consistency and fairness in similarity calculations.",
    "Rationale: The absolute position of a polygon is irrelevant for many applications, such as clustering or pattern analysis. Centering allows the framework to prioritize geometric properties, making it particularly effective for applications in urban planning, environmental monitoring, and medical imaging The pairwise similarity score Similarity(O i , O j ) is computed using a combination of metrics that capture different geometric and spatial properties of polygons. These metrics include: :contentReference[oaicite:2]index=2 1) Jaccard Similarity",
    "Purpose: Measures the overlap between two polygons. Why Used: Highlights overlap-based relationships while penalizing disjoint or minimally overlapping shapes, making it critical for identifying clusters with shared spatial regions. Formula:",
    "2) Area Similarity Purpose: Emphasizes proportional overlap by accounting for the relative scales of the polygons. Why Used: Balances similarity evaluation for polygons of varying sizes, unlike Jaccard similarity, which can under emphasize smaller overlaps. Formula:",
    "3) Curvature Similarity Purpose: Quantifies boundary complexity, differentiating between simple and intricate shapes.",
    "Why Used: Ensures that clusters are sensitive to the level of detail in object boundaries, which is essential for complex geometric objects. Formula:",
    "4) Fourier Descriptor Similarity Purpose: Encodes global shape geometry into a frequency domain.",
    "Why Used: Provides robustness against minor distortions, making it ideal for comparing shapes with slight variations. Formula:",
    "Purpose: Reflects differences in elongation or orientation of bounding boxes.",
    "Why Used: Adds a proportionality-based perspective, useful for distinguishing between elongated and compact shapes. Formula:",
    "Purpose: Focuses on the boundary lengths of polygons.",
    "Why Used: Captures size-related differences and complements area-based metrics. Formula:",
    "7) Bounding Box Distance Purpose: Measures the spatial proximity between bounding box centers. Why Used: Crucial for clustering applications where nearby objects are more likely to be related. Formula:",
    "Purpose: Highlights differences in roundness between polygons.",
    "Why Used: Distinguishes between regular shapes (e.g., circles) and irregular ones, providing nuanced insights into shape characteristics. Formulas:",
    "Purpose: Aggregates the above metrics into a single similarity score using weighted contributions.",
    "Why Used: Balances individual metrics to deliver a robust and comprehensive similarity measure. Formula:",
    "Supervised scheduling is a key optimization technique employed in this framework to identify high-similarity clusters efficiently. Rather than exhaustively calculating similarity indexes for all clusters, supervised scheduling leverages machine learning to prioritize clusters likely to exceed the similarity index threshold.",
    "In this framework, a machine learning model is trained using computationally lightweight features extracted from clusters and binary labels indicating whether a cluster exceeds the similarity index threshold. During prediction, the model assigns a likelihood score to each cluster, allowing the system to focus on the most promising candidates.",
    "This approach significantly reduces computational costs while ensuring high recall, making it particularly effective for large datasets and applications where computational efficiency is paramount. A detailed description of the supervised scheduling algorithm is provided in Section 4.",
    "Kernel Density Estimation (KDE) is a non-parametric statistical method used to estimate the probability density function (PDF) of a dataset. In this framework, KDE plays a pivotal role in dynamically determining the similarity index threshold, which defines the boundary between high-similarity and lowsimilarity clusters.",
    "Purpose: The similarity index threshold is essential for identifying clusters within the top percentage of similarity indexes, as specified by the user. For example, to identify the top 10% of clusters, the threshold is set such that only 10% of similarity indexes in the dataset are above it.",
    "How KDE is Applied: A representative sample of clusters is selected from the dataset. Similarity indexes are calculated for this sample using robust spatial similarity metrics. KDE is applied to estimate the probability density function of the similarity indexes, providing a smooth approximation of their distribution. Finally, the threshold is determined by finding the similarity index value such that the area under the PDF above this value corresponds to the desired percentage (e.g., 10% or 30%).",
    "Advantages of KDE: KDE offers several advantages in this framework. First, it dynamically adapts to the underlying distribution of similarity indexes, allowing the framework to handle diverse datasets without relying on static thresholds. Second, it is computationally efficient because it estimates the threshold from a representative sample, eliminating the need to compute similarity indexes for the entire dataset. Finally, KDE provides flexibility, supporting varying userdefined percentages, making it applicable across different use cases.",
    "KDE ensures that the threshold dynamically adapts to the datasetˆas characteristics while maintaining computational efficiency. A detailed explanation of the KDE process and its integration into the overall framework is provided in Section 4.",
    "For each cluster, we define a representative geometry g, which serves as a reference for computing cluster-wide properties. Geometry g is either a polygon or a linestring, chosen such that its envelope (bounding box) intersects with the envelopes of all other geometries in the cluster. This ensures that geometry g encapsulates the spatial footprint of the cluster, providing a unified basis for feature computation.",
    "The representative geometry g plays a central role in simplifying the characterization of clusters. By relying on a single geometry that spatially relates to all cluster members, the framework can compute cluster-wide features efficiently while maintaining consistency across diverse datasets. The supervised scheduling framework relies on a comprehensive set of features to represent each cluster. These features are extracted from:",
    "1) Source Geometries. These are the geometries within the cluster. 2) Target Geometry. This is the representative geometry g, defined as the geometry whose envelope intersects with the envelopes of all geometries in the cluster. The extracted features are lightweight, focusing on computational efficiency while still capturing critical spatial and geometric properties of the clusters. Lightweight features are essential to reduce computational overhead, enabling the machine learning model to operate effectively on large datasets without compromising accuracy or scalability. The 15 features we extract are inspired from the paper Supervised Progressive GIA.nt [3]. We extract the following features for each pair of source geometry (cluster geometry) and target geometry (representative geometry g): Min-Max Values for Individual Geometries are the minimum and maximum values of each feature F 1 to F 15, computed across all individual geometries in the dataset. They define the range for normalizing individual features for each geometry. For example, the minimum value for the envelope area (F1) corresponds to the smallest envelope area across all geometries in the dataset, while the maximum value corresponds to the largest. The normalization formula for individual geometries is:",
    "• N F i,j is the normalized feature for geometry i and feature j. • F V i,j is the feature value for geometry i and feature j.",
    "• min V al j is the minimum value of feature j across all geometries. • max V al j is the maximum value of feature j across all geometries.",
    "Min-Max Values for Clusters (Cluster-Level Min-Max) are the minimum and maximum values of aggregated cluster features, computed across all clusters in the dataset. Aggregated features (mean values for each feature in a cluster) are normalized using these values to ensure comparability across clusters. The formula for cluster-level normalization is:",
    "• N CF j is the normalized cluster feature for feature j.",
    "• N F j is the mean of normalized feature N F i,j across all geometries in the cluster. • min Cf j is the minimum value of the aggregated feature j across all clusters. • max Cf j is the maximum value of the aggregated feature j across all clusters. After applying the above process, we derive a total of 16 final features for each cluster. These normalized features will later be used to train the machine learning model.",
    "Purpose of Normalization: The two-step formulas are designed to improve the accuracy of the machine learning model by ensuring that features are consistent across scales. This helps to reduce the impact of variations in raw feature magnitudes, which can arise from the diverse sizes, shapes, and scales of geometries in the dataset. By applying these formulas, the features can be compared in a more uniform way. At the individual geometry level, the formulas help features capture meaningful patterns while reducing the influence of extreme values. At the cluster level, the formulas ensure that aggregated features incorporate information not only from within the cluster but also relative to the overall range of values observed across all clusters in the dataset. This broader perspective helps the model identify clusters with high similarity indices more effectively.",
    "Cluster Finding (lines 1-24): The Cluster Finding step organizes geometries into clusters and prepares data for subsequent phases of the algorithm. This step involves two main tasks: indexing the source geometries and identifying candidate clusters by associating target geometries with relevant source geometries.",
    "First, an index I is initialized to store the spatial positions of source geometries. The granularity of the index grid is determined by the function Def ineIndexGranularity(S), which computes the grid resolution based on the properties of the source dataset S. Each geometry s ∈ S is added to the index I using AddT oIndex(I, s). This indexing enables efficient spatial queries in subsequent steps.",
    "Next, several data structures are initialized. The total number of potential geometry pairs is computed as D = |S| × |T |, where |S| and |T | are the sizes of the source and target datasets, respectively. The sourceStats structure stores statistics for each source geometry, such as the number of distinct and real pairs. Two subsets, sample and verif ication sample, are created using the function RandomGenerator(m, D) to randomly select geometry pairs for KDE modeling and validation.",
    "Input: the source dataset S, the target dataset T , the feature set F , the maximum sample size m, the class size N , the probabilistic classification algorithm A, desired recall r d , similarity range similarity range, Output: the links L R = {(c, s) ⊆ C × S : s(c)} CS ← CS ∪ GetTileContents(I, i, j) The algorithm then iterates over each target geometry t ∈ T to identify candidate clusters. For each target geometry t, a set CS is initialized to store potential source geometry candidates whose bounding boxes intersect with the bounding box of t. The diagonal corners of t's bounding box, (x 1 , y 1 ) and (x 2 , y 2 ), are computed using GetDiagCorners(t). The bounding box is then divided into grid cells, and for each grid cell, the contents of the corresponding tiles in the index I are added to CS.",
    "For each geometry s ∈ CS, the sourceStats structure is updated to track the total number of distinct pairs involving s using U pdateT otalDistinctP airs(sourceStats, s). If the bounding boxes of s and t intersect, the sourceStats structure is further updated with the count of real pairs using U pdateRealP airs(sourceStats, s), and the geometry s is added to a set of candidates for the cluster. If the pair ID is part of the randomly generated sample set, the pair (s, t) is added to both the sample and kde sample subsets for subsequent analysis. Finally, the set of candidate geometries for the current target geometry t is added to allClusters, which stores all identified clusters. This prepares the dataset for further processing, including the KDE modeling and training phases.",
    "The Cluster Finding step is crucial for efficiently associating source and target geometries. By leveraging spatial indexing and grid-based filtering, the algorithm focuses only on relevant candidate geometries, reducing computational overhead and facilitating the identification of meaningful clusters for later stages of the process. KDE Phase to Find Minimum Similarity Threshold (lines 25-29): The KDE phase estimates the minimum similarity threshold required to identify clusters with high similarity indices. This step begins by iterating through all clusters in the kde sample, which is a representative subset of the dataset. For each cluster, the similarity index is computed using the function SimilarityCalculator(Cluster). The computed similarity indices are stored in a list.",
    "Once all similarity indices are collected, a Kernel Density Estimation (KDE) model is applied to fit the distribution of similarity indices using the GetBestM odel(List) function. KDE provides a smooth estimation of the underlying probability density function, enabling precise identification of thresholds. Using this model, the EstimateT hreshold(KDE, similarity range) function determines the minimum similarity threshold, which corresponds to the desired similarity range specified by the user (e.g., top 10% or top 50% similarity indices). This threshold will be used as a benchmark in the subsequent training and verification phases.",
    "Training Phase (lines 30-38): The training phase aims to label clusters and train a machine learning model to classify clusters efficiently. Two sets, negClusters (negative clusters) and posClusters (positive clusters), are initialized to store clusters with similarity indices below and above the threshold, respectively. The sample and verif ication sample are shuffled to ensure a balanced and unbiased representation of the dataset.",
    "The algorithm iterates through the clusters in the sample. For each cluster, the similarity index is computed using SimilarityCalculator(Cluster). If the similarity index is greater than or equal to the minimum similarity threshold, the cluster is added to posClusters; otherwise, it is added to negClusters. This process continues until both sets contain at least N clusters, ensuring a sufficient number of labeled samples for training.",
    "Once the clusters are labeled, features are extracted from the union of posClusters and negClusters using the GetF eatures(posClusters ∪ negClusters, F, sourceStats, I) function. This step generates a comprehensive feature set for training the machine learning model. The labeled data is then used to train the probabilistic classifier M using the T rain(L) function, where L represents the labeled dataset with extracted features. After training, key data structures are initialized for the subsequent verification phase: L R , which will store the final results; minw, which tracks the minimum weight; T C, which stores total clusters; and P Q, a priority queue The KDE and training phases are critical to efficiently identifying high-similarity clusters. KDE reduces computational overhead by estimating a data-driven threshold, while the training phase equips the framework with a model capable of classifying clusters based on lightweight, extracted features.",
    "Estimate Recall and Simulate Verification Process in a Small Sample (lines 39-58): This phase evaluates the system's ability to identify highsimilarity clusters and estimates the recall under simulated verification. The process begins by iterating through all clusters in the kde sample, a representative subset of the dataset. For each cluster, the feature vector u is generated using the GetF eatureV ector(Cluster, F ) function, which extracts the lightweight features defined earlier. The trained classifier M is then used to compute the classification probability w s,t for the cluster. This probability represents the likelihood that the cluster belongs to the high-similarity category. Each cluster, along with its classification probability, is added to a priority queue P Q, which ranks clusters by their weights w s,t . The size of P Q is limited to N , the desired number of top clusters to process. The number of clusters with similarity indices above the minimum similarity threshold is calculated using the N umberAboveSimilarityT hreshold(P Q) function, which identifies the count of clusters classified as highly similar.",
    "Next, the algorithm simulates the verification process using the priority queue. Clusters are processed in descending order of their classification probabilities by repeatedly removing the highest-ranked cluster from P Q using P Q.popLast(). For each cluster, the similarity index is recalculated using the SimilarityCalculator(c) function. If the recalculated similarity index meets or exceeds the minimum similarity threshold, the counter for high-similarity predictions is incremented. The simulation stops when the counter reaches a predefined value equal to similarity range • N . This ensures that the simulation is bounded by the specified similarity range. The approximate recall, recall approx, is then computed as the ratio of correctly predicted high-similarity clusters (highSimilarityP rediction) to the total number of actual high-similarity clusters (HighSimIndices). Finally, the max size parameter, which determines the maximum number of clusters to verify during the full verification phase, is estimated using the formula:",
    ") where r d is the desired recall rate. This formula adjusts the maximum verification size based on the estimated recall, the total number of high-similarity indices, and the desired recall proportion.",
    "Verification Phase (lines 59-78): The verification phase validates clusters by identifying those with high similarity indices. The process begins by initializing an empty priority queue P Q. For each cluster in the dataset, the feature vector u is computed using the predefined feature set F . The trained machine learning model M is then used to calculate the probability w s,t of the cluster being a high-similarity cluster. Each cluster, along with its probability, is added to P Q, prioritizing clusters with higher probabilities.",
    "The algorithm iteratively processes clusters in descending order of their classification probabilities. The cluster c with the highest probability is removed from P Q using P Q.popLast(), and its actual similarity index I M is calculated. If I M exceeds or equals the minimum similarity threshold, the cluster is added to the result set L R . The process continues until the counter reaches max size or P Q is empty. Once the priority queue is exhausted or the size constraint is met, the result set L R , containing the verified high-similarity clusters, is returned. This phase efficiently identifies the most promising clusters while adhering to recall constraints and computational limits.",
    "The experimental analysis was conducted using a v28 TPU machine with 334.56 GB of RAM, utilizing Python 3.9 as the programming language. The experiments were implemented on Google Colab. This setup allowed to evaluate the proposed framework's performance on diverse datasets with varying sizes and complexities effectively. Additionally, the algorithm was executed without utilizing Triton for its computations, and further performance comparisons were conducted independently.",
    "The experimental analysis evaluates the performance of the proposed framework on three datasets with varying sizes, complexities, and average cluster sizes. These differences allow us to explore the framework's scalability and its ability to handle diverse conditions effectively.",
    "Dataset D1 includes 229,276 source and 583,833 target geometries, forming a total of 295,481 clusters with an average size of approximately six polygons per cluster. Dataset D2 contains 210,483 source geometries and 2,898,899 target geometries, resulting in 654,196 clusters with an average cluster size of around 13 polygons. Finally, Dataset D3 comprises 200,294 source geometries and 7,392,699 target geometries, yielding 1,324,980 clusters with an average size of 34 polygons.",
    "Regarding the distribution of similarity ranges, in Dataset D1, most clusters fall within the 40-70% similarity range, while clusters with similarity indices above 90% represent a very small fraction-only 65 out of nearly 300,000 clusters. Dataset D2 follows a similar pattern, with the majority concentrated in the 50-70% range and less than 0.5% of clusters in the 90-100% similarity range. Dataset D3 demonstrates a more skewed distribution, with the majority in the 50-70% range and an even smaller proportion of high-similarity clusters. These results suggest that highly similar clusters are rare, which underscores the importance of using targeted methods to identify them efficiently.",
    "We also examined the dataset coverage required to identify clusters within specific top percentages, such as the top 10%, 30%, and 50%. In Dataset D1, identifying the top 10% of clusters required processing ∼32% of the dataset, while ∼45% and ∼60% were processed to identify the top 30% and 50%, respectively. Dataset D2 required ∼41%, ∼55%, and ∼70%, and Dataset D3 required ∼68%, ∼85%, and ∼94% for the same ranges. These findings demonstrate the adaptability of the framework to prioritize computational resources, though larger and more complex datasets naturally require greater processing effort.",
    "Further insights emerge from analyzing the analogy between the percentage of clusters checked and the percentage of clusters targeted. For Dataset D1, the ratio of checked to targeted clusters was approximately 3.25 for the top 10% and decreased as the target percentage increased, reflecting improved efficiency for broader ranges. Similarly, Dataset D2 exhibited a ratio of ∼4 for the top 10%, decreasing to ∼1.8 for the top 50%. Dataset D3 began at ∼6.8 for the top 10% but also demonstrated decreasing ratios, reaching ∼1.9 for the top 50%. These trends indicate that while larger and more complex datasets pose challenges, the framework effectively allocates effort based on the desired coverage. The relationship between average cluster size and dataset coverage reveals a direct impact on computational requirements. Larger clusters, such as those in Dataset D3, demand the processing of a much larger percentage of the dataset to achieve the same recall levels as smaller clusters. For example, identifying the top 50% of clusters in Dataset D3 required processing ∼94% of the dataset, compared to ∼60% in Dataset D1. This highlights the challenges of handling larger cluster sizes and underscores the necessity of employing optimization techniques.",
    "To further support these analyses, Shapely 2.0 offers notable advancements in geometric operations. By enabling vectorized processing, Shapely 2.0 processes arrays of geometries in a single operation, avoiding the overhead of handling each geometry sequentially.",
    "Performance Improvements: Vectorized operations in Shapely 2.0 are significantly faster, offering up to threefold improvements in tasks such as calculating Minimum Bounding Rectangles, intersections, and unions. These enhancements help reduce the computational time for critical pre-processing steps in this study.",
    "Enhanced GeoPandas Integration: The improved integration with GeoPandas enables faster spatial joins and overlays, particularly useful when handling large datasets like those analyzed here. This compatibility facilitates efficient preparation of geometries for similarity computations.",
    "Scalability for Large Datasets: Shapely 2.0's optimizations make it feasible to process large-scale spatial datasets efficiently. While these improvements enhance computational performance, they must be combined with targeted methods to manage the overall computational workload effectively.",
    "By integrating these tools into the proposed framework, the computational overhead associated with geometric operations is reduced, making it feasible to address scalability challenges. However, while these advancements are promising, their application requires careful evaluation to ensure alignment with the framework's objectives.",
    "In addition to the above, Triton was evaluated separately for computing similarity metrics independently of the algorithm. This evaluation involved directly computing similarity metrics using averages to gauge potential speedups. Triton demon-strated speed improvements compared to standard Python code, especially when handling large-scale parallelizable operations. While these results are promising, they also highlight the need to reduce the number of required computations within the algorithm to ensure that Triton's parallel processing capabilities are utilized efficiently.",
    "However, it is important to acknowledge that incorporating Triton into the broader framework requires careful optimization. The complexity of similarity computations, particularly in large datasets, can still pose challenges if left unoptimized. By leveraging Triton's strengths in highly parallel operations while minimizing redundant calculations, future work aims to fully harness its capabilities in conjunction with tools like Shapely 2.0. These combined efforts hold promise for achieving a scalable and efficient solution for identifying highsimilarity clusters in large datasets."
  ],
  "references": [
    {
      "id": 1,
      "text": "ship-shapely, adv.\n\t\t10.1093/oed/8280976727\n\t\t\n\t\n\t\n\t\tOxford English Dictionary\n\t\t\n\t\t\tOxford University Press\n\t\t\t2023. Jul. 20, 2025"
    },
    {
      "id": 2,
      "text": "Introducing triton: Open-source gpu programming for neural networks\n\t\t\n\t\t\tOpenai\n\t\t\n\t\t\n\t\t\n\t\t\t2021. Jan. 10, 2025"
    },
    {
      "id": 3,
      "text": "Supervised Scheduling for Geospatial Interlinking\n\t\t\n\t\t\tMariaDespoinaSiampou\n\t\t\t0009-0006-1646-3618\n\t\t\n\t\t\n\t\t\tGeorgePapadakis\n\t\t\t0000-0002-7298-9431\n\t\t\n\t\t\n\t\t\tNikosMamoulis\n\t\t\t0000-0003-3423-4895\n\t\t\n\t\t\n\t\t\tManolisKoubarakis\n\t\t\t0000-0002-1954-8338\n\t\t\n\t\t10.1145/3589132.3625585\n\t\n\t\n\t\tProceedings of the 31st ACM International Conference on Advances in Geographic Information Systems\n\t\tthe 31st ACM International Conference on Advances in Geographic Information Systems\n\t\t\n\t\t\tACM\n\t\t\t2023"
    },
    {
      "id": 4,
      "text": "Esthetic rehabilitation with zirconia-based shell crowns: a case report.\n\t\t\n\t\t\tStefanosKourtis\n\t\t\n\t\t\n\t\t\tEvangeliaBachlava\n\t\t\n\t\t\n\t\t\tVasilikiRoussou\n\t\t\n\t\t10.25141/2471-657x-2016-4.0102\n\t\n\t\n\t\tInternational Journal of Dentistry and Oral Health\n\t\tIJDOH\n\t\t2471-657X\n\t\t\n\t\t\t2\n\t\t\t4\n\t\t\t\n\t\t\t2023\n\t\t\tBiocore Group\n\t\t\n\t\t\n\t\t\tNational and Kapodistrian Univ. of Athens\n\t\t\n\t\n\tDept. Informatics & Telecommunications\n\tMaster's thesis"
    },
    {
      "id": 5,
      "text": "On Estimation of a Probability Density Function and Mode\n\t\t\n\t\t\tEmanuelParzen\n\t\t\n\t\t10.1214/aoms/1177704472\n\t\n\t\n\t\tThe Annals of Mathematical Statistics\n\t\tAnn. Math. Statist.\n\t\t0003-4851\n\t\t\n\t\t\t33\n\t\t\t3\n\t\t\t\n\t\t\t1962\n\t\t\tInstitute of Mathematical Statistics"
    },
    {
      "id": 6,
      "text": "Figure 1: (A) Sampling locations on the world map (from OpenStreetMap; https://www.openstreetmap.org/#map=10/-4.4820/55.5833) and (B) giant tortoise on turf (from Wikipedia; https://en.wikipedia.org/wiki/Aldabra_giant_tortoise#/media/File:Giant_Tortoise.JPG).\n\t\t\n\t\t\tOpenstreetmapWiki\n\t\t\n\t\t10.7717/peerj.19566/fig-1\n\t\t\n\t\t\n\t\t\t2021. Jan. 10, 2025\n\t\t\tPeerJ"
    },
    {
      "id": 7,
      "text": "Publications available from GeoNames\n\t\t\n\t\t\tGeonames\n\t\t\n\t\t10.4095/298608\n\t\t\n\t\t\n\t\t\t2021. Jan. 10, 2025\n\t\t\tNatural Resources Canada/CMSS/Information Management"
    },
    {
      "id": 8,
      "text": "Extending the YAGO2 Knowledge Graph with Precise Geospatial Knowledge\n\t\t\n\t\t\tNikolaosKaralis\n\t\t\n\t\t\n\t\t\tGeorgiosMandilaras\n\t\t\n\t\t\n\t\t\tManolisKoubarakis\n\t\t\n\t\t10.1007/978-3-030-30796-7_12\n\t\n\t\n\t\tLecture Notes in Computer Science\n\t\t\n\t\t\tSpringer International Publishing\n\t\t\t2019\n\t\t\t11779"
    },
    {
      "id": 9,
      "text": "WorldKG: A World-Scale Geographic Knowledge Graph\n\t\t\n\t\t\tAlishibaDsouza\n\t\t\n\t\t\n\t\t\tNicolasTempelmeier\n\t\t\n\t\t\n\t\t\tRanYu\n\t\t\n\t\t\n\t\t\tSimonGottschalk\n\t\t\n\t\t\n\t\t\tElenaDemidova\n\t\t\n\t\t10.1145/3459637.3482023\n\t\n\t\n\t\tProceedings of the 30th ACM International Conference on Information & Knowledge Management\n\t\tthe 30th ACM International Conference on Information & Knowledge Management\n\t\t\n\t\t\tACM\n\t\t\t2021"
    },
    {
      "id": 10,
      "text": "Know, Know Where, KnowWhereGraph: A densely connected, cross‐domain knowledge graph and geo‐enrichment service stack for applications in environmental intelligence\n\t\t\n\t\t\tKrzysztofJanowicz\n\t\t\n\t\t\n\t\t\tPascalHitzler\n\t\t\n\t\t\n\t\t\tWenwenLi\n\t\t\n\t\t\n\t\t\tDeanRehberger\n\t\t\n\t\t\n\t\t\tMarkSchildhauer\n\t\t\n\t\t\n\t\t\tRuiZhu\n\t\t\n\t\t\n\t\t\tCoganShimizu\n\t\t\n\t\t\n\t\t\tColbyKFisher\n\t\t\n\t\t\n\t\t\tLingCai\n\t\t\n\t\t\n\t\t\tGengchenMai\n\t\t\n\t\t\n\t\t\tJosephZalewski\n\t\t\n\t\t\n\t\t\tLuZhou\n\t\t\n\t\t\n\t\t\tShirlyStephen\n\t\t\n\t\t\n\t\t\tSeilaGonzalez\n\t\t\n\t\t\n\t\t\tBryceMecum\n\t\t\n\t\t\n\t\t\tAnnaLopez‐carr\n\t\t\n\t\t\n\t\t\tAndrewSchroeder\n\t\t\n\t\t\n\t\t\tDavidSmith\n\t\t\n\t\t\n\t\t\tDawnWright\n\t\t\n\t\t\n\t\t\tSizheWang\n\t\t\n\t\t\n\t\t\tYuanyuanTian\n\t\t\n\t\t\n\t\t\tZilongLiu\n\t\t\n\t\t\n\t\t\tMeilinShi\n\t\t\n\t\t\n\t\t\tAnthonyD'onofrio\n\t\t\n\t\t\n\t\t\tZhiningGu\n\t\t\n\t\t\n\t\t\tKittyCurrier\n\t\t\n\t\t10.1002/aaai.12043\n\t\n\t\n\t\tAI Magazine\n\t\tAI Magazine\n\t\t0738-4602\n\t\t2371-9621\n\t\t\n\t\t\t43\n\t\t\t1\n\t\t\t\n\t\t\t2022\n\t\t\tWiley"
    },
    {
      "id": 11,
      "text": "The Visible Human Male: A Technical Report\n\t\t\n\t\t\tVMSpitzer\n\t\t\n\t\t\n\t\t\tMJAckerman\n\t\t\n\t\t\n\t\t\tALScherzinger\n\t\t\n\t\t\n\t\t\tDGWhitlock\n\t\t\n\t\t10.1136/jamia.1996.96236280\n\t\n\t\n\t\tJournal of the American Medical Informatics Association\n\t\tJournal of the American Medical Informatics Association\n\t\t1067-5027\n\t\t1527-974X\n\t\t\n\t\t\t3\n\t\t\t2\n\t\t\t\n\t\t\t1996\n\t\t\tOxford University Press (OUP)"
    },
    {
      "id": 12,
      "text": "The WU-Minn Human Connectome Project: An overview\n\t\t\n\t\t\tDavidCVan Essen\n\t\t\n\t\t\n\t\t\tStephenMSmith\n\t\t\n\t\t\n\t\t\tDeannaMBarch\n\t\t\n\t\t\n\t\t\tTimothyE JBehrens\n\t\t\n\t\t\n\t\t\tEssaYacoub\n\t\t\n\t\t\n\t\t\tKamilUgurbil\n\t\t\n\t\t10.1016/j.neuroimage.2013.05.041\n\t\n\t\n\t\tNeuroImage\n\t\tNeuroImage\n\t\t1053-8119\n\t\t\n\t\t\t80\n\t\t\t\n\t\t\t2013\n\t\t\tElsevier BV"
    },
    {
      "id": 13,
      "text": "Deep Learning Techniques for Automatic MRI Cardiac Multi-Structures Segmentation and Diagnosis: Is the Problem Solved?\n\t\t\n\t\t\tOlivierBernard\n\t\t\t0000-0003-0752-9946\n\t\t\n\t\t\n\t\t\tAlainLalande\n\t\t\t0000-0002-7970-366X\n\t\t\n\t\t\n\t\t\tClementZotti\n\t\t\t0000-0002-0713-9924\n\t\t\n\t\t\n\t\t\tFrederickCervenansky\n\t\t\n\t\t\n\t\t\tXinYang\n\t\t\n\t\t\n\t\t\tPheng-AnnHeng\n\t\t\n\t\t\n\t\t\tIremCetin\n\t\t\n\t\t\n\t\t\tKarimLekadir\n\t\t\n\t\t\n\t\t\tOscarCamara\n\t\t\n\t\t\n\t\t\tMiguelAngelGonzalez Ballester\n\t\t\n\t\t\n\t\t\tGerardSanroma\n\t\t\n\t\t\n\t\t\tSandyNapel\n\t\t\n\t\t\n\t\t\tSteffenPetersen\n\t\t\n\t\t\n\t\t\tGeorgiosTziritas\n\t\t\t0000-0002-1802-1825\n\t\t\n\t\t\n\t\t\tEliasGrinias\n\t\t\n\t\t\n\t\t\tMahendraKhened\n\t\t\n\t\t\n\t\t\tVargheseAlexKollerathu\n\t\t\n\t\t\n\t\t\tGanapathyKrishnamurthi\n\t\t\n\t\t\n\t\t\tMarc-MichelRohé\n\t\t\n\t\t\n\t\t\tXavierPennec\n\t\t\n\t\t\n\t\t\tMaximeSermesant\n\t\t\t0000-0002-6256-8350\n\t\t\n\t\t\n\t\t\tFabianIsensee\n\t\t\n\t\t\n\t\t\tPaulJäger\n\t\t\n\t\t\n\t\t\tKlausHMaier-Hein\n\t\t\n\t\t\n\t\t\tPeterMFull\n\t\t\n\t\t\n\t\t\tIvoWolf\n\t\t\n\t\t\n\t\t\tSandyEngelhardt\n\t\t\n\t\t\n\t\t\tChristianFBaumgartner\n\t\t\t0000-0002-3629-4384\n\t\t\n\t\t\n\t\t\tLisaMKoch\n\t\t\n\t\t\n\t\t\tJelmerMWolterink\n\t\t\t0000-0001-5505-475X\n\t\t\n\t\t\n\t\t\tIvanaIšgum\n\t\t\n\t\t\n\t\t\tYeonggulJang\n\t\t\n\t\t\n\t\t\tYoonmiHong\n\t\t\t0000-0003-2416-8249\n\t\t\n\t\t\n\t\t\tJayPatravali\n\t\t\n\t\t\n\t\t\tShubhamJain\n\t\t\n\t\t\n\t\t\tOlivierHumbert\n\t\t\n\t\t\n\t\t\tPierre-MarcJodoin\n\t\t\t0000-0002-6038-5753\n\t\t\n\t\t10.1109/tmi.2018.2837502\n\t\n\t\n\t\tIEEE Transactions on Medical Imaging\n\t\tIEEE Trans. Med. Imaging\n\t\t0278-0062\n\t\t1558-254X\n\t\t\n\t\t\t37\n\t\t\t11\n\t\t\t\n\t\t\t2018\n\t\t\tInstitute of Electrical and Electronics Engineers (IEEE)"
    },
    {
      "id": 14,
      "text": "The liver tumor segmentation benchmark (lits)\n\t\t\n\t\t\tPBilic\n\t\t\n\t\n\t\n\t\tMedical Image Analysis\n\t\t\n\t\t\t84\n\t\t\t102680\n\t\t\t2023"
    },
    {
      "id": 15,
      "text": "Étude comparative de la distribution florale dans une portion des alpes et des jura\n\t\t\n\t\t\tPJaccard\n\t\t\n\t\t10.69777/272806\n\t\n\t\n\t\tBull. Soc. Vaudoise des Sci. Nat\n\t\t\n\t\t\t37\n\t\t\t\n\t\t\t1901\n\t\t\tFonds de recherche du Québec"
    },
    {
      "id": 16,
      "text": "An efficiently computable metric for comparing polygonal shapes\n\t\t\n\t\t\tEMArkin\n\t\t\n\t\t\n\t\t\tLPChew\n\t\t\n\t\t\n\t\t\tDPHuttenlocher\n\t\t\n\t\t\n\t\t\tKKedem\n\t\t\n\t\t\n\t\t\tJS BMitchell\n\t\t\n\t\t10.1109/34.75509\n\t\n\t\n\t\tIEEE Transactions on Pattern Analysis and Machine Intelligence\n\t\tIEEE Trans. Pattern Anal. Machine Intell.\n\t\t0162-8828\n\t\t\n\t\t\t13\n\t\t\t3\n\t\t\t\n\t\t\t1991\n\t\t\tInstitute of Electrical and Electronics Engineers (IEEE)"
    },
    {
      "id": 17,
      "text": "Fourier Descriptors for Plane Closed Curves\n\t\t\n\t\t\tCharlesTZahn\n\t\t\n\t\t\n\t\t\tRalphZRoskies\n\t\t\n\t\t10.1109/tc.1972.5008949\n\t\n\t\n\t\tIEEE Transactions on Computers\n\t\tIEEE Trans. Comput.\n\t\t0018-9340\n\t\t\n\t\t\tC-21\n\t\t\t3\n\t\t\t\n\t\t\t1972\n\t\t\tInstitute of Electrical and Electronics Engineers (IEEE)"
    },
    {
      "id": 18,
      "text": "Review of shape representation and description techniques\n\t\t\n\t\t\tDengshengZhang\n\t\t\n\t\t\n\t\t\tGuojunLu\n\t\t\n\t\t10.1016/j.patcog.2003.07.008\n\t\n\t\n\t\tPattern Recognition\n\t\tPattern Recognition\n\t\t0031-3203\n\t\t\n\t\t\t37\n\t\t\t1\n\t\t\t\n\t\t\t2004\n\t\t\tElsevier BV"
    }
  ],
  "formulas": [
    {
      "id": "FORMULA_1",
      "raw": "SI(C) = 2 |C|(|C| -1) 1≤i<j≤|C| Similarity(O i , O j ). (1)"
    },
    {
      "id": "FORMULA_2",
      "raw": "J(A, B) = |A ∩ B| |A ∪ B| .(2)"
    },
    {
      "id": "FORMULA_3",
      "raw": "S area (A, B) = 2 |A ∩ B| |A| + |B| .(3)"
    },
    {
      "id": "FORMULA_4",
      "raw": "S curv (A, B) = exp - |n A -n B | max(n A , n B ) . (4"
    },
    {
      "id": "FORMULA_5",
      "raw": ")"
    },
    {
      "id": "FORMULA_6",
      "raw": "S FD (A, B) = 1 1 + ∥F(A) -F(B)∥ .(5)"
    },
    {
      "id": "FORMULA_7",
      "raw": "S ar (A, B) = 1 1 + |r(A) -r(B)| .(6)"
    },
    {
      "id": "FORMULA_8",
      "raw": "S perim (A, B) = 1 1 + |P (A) -P (B)| . (7"
    },
    {
      "id": "FORMULA_9",
      "raw": ")"
    },
    {
      "id": "FORMULA_10",
      "raw": "S bb (A, B) = 1 1 + ∥c(A) -c(B)∥ 2 .(8)"
    },
    {
      "id": "FORMULA_11",
      "raw": "ϕ(A) = 4π |A| P (A) 2 , S circ (A, B) = 1 1 + |ϕ(A) -ϕ(B)| . (9"
    },
    {
      "id": "FORMULA_12",
      "raw": "s(A, B) = i w i m i (A, B).(10)"
    },
    {
      "id": "FORMULA_13",
      "raw": "F1) Source"
    },
    {
      "id": "FORMULA_14",
      "raw": "N F i,j = F V i,j -min V al j max V al j •10000"
    },
    {
      "id": "FORMULA_15",
      "raw": "N CF j = N F j -min Cf j max Cf j •10000"
    },
    {
      "id": "FORMULA_16",
      "raw": "maxSize = r d • 1 recallApprox • HighSimIndices N •totalClusters(11"
    }
  ]
}