{
  "title": "CHAI: Command Hijacking against embodied AI",
  "authors": [
    {
      "firstname": "Luis",
      "surname": "Burbano",
      "email": "lburbano@ucsc.edu"
    },
    {
      "firstname": "Diego",
      "surname": "Ortiz",
      "email": "dortizba@ucsc.edu"
    },
    {
      "firstname": "Qi",
      "surname": "Sun",
      "email": ""
    },
    {
      "firstname": "Siwei",
      "surname": "Yang",
      "email": ""
    },
    {
      "firstname": "Haoqin",
      "surname": "Tu",
      "email": ""
    },
    {
      "firstname": "Cihang",
      "surname": "Xie",
      "email": "cixie@ucsc.edu"
    },
    {
      "firstname": "Yinzhi",
      "surname": "Cao",
      "email": "yinzhi.cao@jhu.edu"
    },
    {
      "firstname": "Alvaro",
      "surname": "Cardenas",
      "email": ""
    }
  ],
  "abstract": "Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms stateof-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.",
  "sections": [
    {
      "title": "I. INTRODUCTION",
      "paragraphs": [
        "One of the main limitations of current robotic systems is their inability to cope with rare, novel, or unpredictable scenarios; those \"edge cases\" where training data are scarce or nonexistent. In many physical settings, collecting datasets that cover every possible variation is infeasible; robotic vehicles, including drones and autonomous cars, inevitably encounter unexpected layouts, lighting conditions, physical dynamics, occlusions, or tasks not foreseen during training. Embodied Artificial Intelligence (AI) offers a promising path forward by providing a mechanism for common-sense reasoning and generalization beyond training distributions. Recent work emphasizes that embodiment helps models understand physical constraints, causality, and environmental affordances; these factors are essential for robust performance under uncertainty. Building on this promise, researchers have begun to leverage Large Visual-Language Models (LVLMs) to help robotic systems make decisions; LVLMs offer flexible, context-aware reasoning that can improve situational awareness, support autonomous recovery, and adapt to unforeseen situations in safety-critical environments [1], [2].",
        "Despite extensive research on vision-and LiDAR-based attacks against autonomous systems, the safety of embodied AIs that issue intermediate, text-based planning decisions remains largely unexplored. Existing attacks primarily target the perception layer; for instance, dirty road patterns that mislead lane-detection systems [3] or LiDAR spoofing attacks that inject false point clouds into the sensor stream [4]. Such perception-level attacks cause downstream errors in planning and control, but they do not apply to embodied AIs that interpose text-based commands between perception and actuation.",
        "Moreover, many canonical adversarial techniques are difficult to translate to this setting. Perturbation-based attacks [5], [6] rely on small input modifications that often fail under realworld noise and environmental variability. Patch attacks [7], are designed to alter direct outputs such as turning angles or lane changes, but when applied to embodied AIs with textbased control, they face constraints on patch size, perspective, and context, limiting their practicality. Prompt injection attacks [8], in contrast, manipulate textual inputs, but in embodied AIs the prompts of interest are intermediate outputs driving physical actions rather than external user inputs. Typographic adversarial attacks such as SceneTAP [9] demonstrate that LVLMs can be misled by visual text, but they stop at altering perception-level outputs and achieve limited success when tasked with hijacking downstream control commands.",
        "To address these gaps, we present CHAI (structured Command Hijacking against embodied AI), the first optimizationbased adversarial attack tailored to embodied systems driven by Large Visual-Language Models (LVLMs). Unlike prior work that manipulates only perception or input text, CHAI targets the command layer by embedding structured naturallanguage instructions into the visual scene as human-readable signs. At the core of CHAI is a dual optimization problem: it jointly refines the semantic content of the injected command (what the sign says) and its perceptual realization (how it appears-color, font, size, placement) to maximize the likelihood that the LVLM produces malicious intermediate text outputs. By operating simultaneously over both language and vision channels, CHAI exposes a fundamentally new attack surface in embodied AI and demonstrates how adversaries can hijack high-level decisions that control physical systems.",
        "Across three representative LVLM agents-drone emergency landing, autonomous driving (DriveLM), and aerial object tracking (CloudTrack)-CHAI achieves up to 95.5% attack success rate (ASR) on CloudTrack, 81.8% on DriveLM, and 68.1% on drone landing in simulation. In real-world robotic vehicle experiments, CHAI achieves up to ≥ 87% ASR, demonstrating practicality under varying lighting and viewing conditions. Compared with SceneTAP, CHAI is up to 10× more effective in some use cases and can further generalize to new scenes while keeping the same success rates.",
        "Our analysis further shows that CHAI generalizes across languages (English, Chinese, Spanish, and \"Spanglish\"), can handle adverse weather, and can be used to exploit taskspecific prompts. These findings establish CHAI as a practical, cross-modal jailbreak against embodied LVLMs, underscore a new attack surface opened by language-grounded perception, and motivate future work on principled filter, alignment, and provable-robust defenses.",
        "In summary, we make the following contributions:",
        "• We identify and formalize a novel vulnerability in embodied AI systems: the command layer of LVLM-driven physical agents, where intermediate text outputs bridge perception and control. Our formalization introduces CHAI, an optimization that focuses on semantic content and perceptual realization of visual prompts.",
        "• We demonstrate CHAI on three different embodied AI tasks, and also on a real-world system, achieving up to 95.5% success rates in simulation, average transferability above 70%, and more than 87% success rate in real-world robotic vehicle experiments. We also demonstrate that our attack improves the success rate and transferability over state of the art techniques. • We will release our code, datasets, and attack artifacts to enable reproducibility and to foster further research on the security of LVLM-driven embodied AI systems after acceptance."
      ],
      "subsections": []
    },
    {
      "title": "II. RELATED WORK",
      "paragraphs": [
        "Robotic Autonomous Vehicles (AVs) have made substantial progress in recent years, driven by advances in perception and planning. However, these systems still struggle to operate reliably in the face of edge cases and out-of-distribution scenarios, especially those that require common sense reasoning. Although engineers can encode extensive rule-based behaviors to account for known contingencies, this rule-based strategy quickly breaks down in complex, unpredictable environments.",
        "A promising direction to address these limitations is the integration of Multimodal Large Language Models, including LVLMs into physical agents (e.g., drones, autonomous vehicles, robots, etc.), a field often called Embodied AI. Embodied AI agents can help decision making in unforeseen circumstances by offering flexible, context-aware reasoning that can improve situational awareness, support autonomous recovery, offer the ability to adapt to new situations, and enable common sense reasoning in safety-critical situations [1], [2]. LVLMs for autonomous vehicles: LVLMs have the ability to think, plan and understand multimodally, offering the most promising path to achieving reliable, fully autonomous driving, particularly the pursuit of level 5 autonomy [10].",
        "A recent line of work pursues end-to-end agents that map raw images directly to steering and throttle commands-e.g., DriveLM [11], DriveVLM [12], and DriveGPT-4 [13]. Driv-eLM exemplifies the approach: it poses a chain of language queries (\"What is ahead?\", \"Is there a pedestrian?\") to reason about the scene before emitting low-level controls. Building on this paradigm, Wang et al. introduce counterfactual reasoning modules that allow the agent to imagine alternative scenarios to further improve decision quality and robustness [14], [15]. A similar approach is provided by the Dolphins framework, which augments driving stacks with an LVLM that reasons over front-view video to provide interpretable, human-like situational assessments and adaptive route suggestions [16]. LVLMs for drones: There are three main directions to integrate LVLM in drones: (1) Perception-centric studies attach an LVLM to the aircraft frame to interpret environmental signals, e.g., inferring local weather conditions directly from onboard imagery [17]. (2) Tracking and classification systems employ an LVLM as a visual copilot: CloudTrack, for example, leverages language-based object descriptors to boost real-time target identification from drone camera feed [18].",
        "(3) Planning-oriented approaches go a step further by fusing images with flight-state sensors to produce high-level action plans that a conventional controller can then execute; Zhao et al. demonstrate this workflow for disaster response missions [19]. A survey of emerging LLM applications in drones is available in [20]. Attacks on LVLMs: Although LVLM models offer many practical benefits, they are also vulnerable to new attacks. One area of research focuses on the propensity of generative models to produce harmful or offensive content (text and images). ToViLaG [21] investigates toxic generation in LVLMs. They construct a dataset for the evaluation of the toxicity of textimage pairs and then develop a detoxification method to reduce the toxicity in LVLMs while aiming to maintain the quality of generation.",
        "A second line of work are adversarial attacks that attempt to disrupt the model behavior through adversarial images. Qi et al. [22] demonstrate how visual adversarial examples can universally jailbreak aligned LVLMs, showing that a single adversarial image can force aligned LVLMs to comply with a wide range of toxic content. Unlike low-level perceptual attacks such as visual adversarial patches [23], CHAI leverages the model's capacity for language understanding and multimodal reasoning, exploiting the fusion of vision and natural language to inject commands through structured visual stimuli.",
        "More recently, attacks on LVLMs have considered the semantic content embedded visually. Figstep [24] proposes a black-box jailbreak algorithm that converts prohibited textual content into images to bypass safety alignment mechanisms. Similarly, Visual-RolePlay [25] introduces the concept of role play, using LLMs to generate detailed descriptions of highrisk people, and then using an LVLM to create this shady character image. The core idea is to prompt the LVLM to enact characters with negative attributes, tricking the LVLM into adopting that persona's negative attributes and generating harmful content. Their focus is on toxic image generation or model jailbreaking, and more importantly, all these previous efforts did not consider visual prompts.",
        "The line of work most closely related to CHAI focuses on typographic attacks, where an adversary uses visual text to alter a model's output; a class of indirect prompt injection attacks. Cheng et al. [26] show the feasibility of the attack by Fig. 1: LVLMs can understand commands in different modalities, and these modalities can be attacked.",
        "placing random words to disrupt the LVLM output. Qraitem et al. [27] proposed to generate an attack with another LVLM. The attacks involve adding misleading text and a supporting sentence, and placing the text in a white space at the top or bottom of the image to avoid occluding important visual cues. These previous efforts did not focus on the potential deployment of typographic attacks in the real world. As a method to inject visual prompts into the real world, SceneTAP [9] uses LVLM to decide the text and its position in the image.",
        "While relevant, these methods differ significantly from CHAI, as illustrated in Tab. I. First, from the optimization perspective, previous work rely on a one-shot, generative process; if the LVLM's initial output fails to deceive the model, the attack fails, as there is no mechanism for refinement or optimization; in contrast, we optimize both the semantic and visual elements of the attack. By moving beyond a single generative step, we can create more robust and effective attacks. Second, these efforts create a unique attack for each image; this means that the attacker knows exactly the conditions under which the LVLM is called. In contrast, CHAI optimizes for a set of diverse images with the goal of producing visual prompts that succeed, even in non-optimized images. Third, most of the work on typographic attacks focuses on digital images and does not consider a physical world realization. Finally, previous work does not focus on attacking Cyber-Physical Systems (CPS) such as autonomous drones and robotic cars, in contrast, CHAI formulates the problem of visual challenges for steering these control systems to dangerous situations."
      ],
      "subsections": []
    },
    {
      "title": "III. MOTIVATION",
      "paragraphs": [
        "Embodied AI robotic systems powered by LVLMs can be beneficial in many ways, including teaming with humans to understand goals and then using perception to make autonomous decisions. However, this multimodal reasoning may also be attacked because it opens a new side channel for the attacker to send information to the agent to be used as prompt instructions. Consider the example illustrated in Fig. 1: in the first panel, a police officer receives a distress call about a crime, and she asks the drone to find the getaway car. In the second panel, the drone starts to look for the suspect car. In the last panel, an attacker holds a sign with different instructions for the drone; If the LVLM agent reads the sign, it will follow the orders of the attacker (in this case, the drone might give up looking for the getaway car and reboot, falling to the ground). The attack instructions can be diverse; the visual prompt could tell the drone to land in a hostile area, or to follow a malicious car, or to delete the account of the drone owner, etc. Note that these attacks are not possible with simpler DNN perception systems; these are only possible with the addition of LVLMs.",
        "However, attacking embodied AI systems by holding a sign telling them what to do is not always successful, as illustrated in Fig. 2. Fig. 2a shows an unsuccessful attack against DriveLM (using GPT-4o); the attacker holds a sign with the words Go Onward, but DriveLM correctly decides to stop as there are pedestrians crossing the street. Similarly, Fig. 2b shows an attacker holding a sign with the words Proceed, but DriveLM again decides correctly to stop. However, by slightly changing the colors of the sign in Fig. 2c (while keeping the same text-Proceed-in the sign), the attack succeeds in making DriveLM proceed rather than stop. This motivates our central question for this paper: Under what parameters (e.g., text, colors, locations) are CHAI attacks effective? And how can we design a general and efficient methodology to find and evaluate these attacks?"
      ],
      "subsections": []
    },
    {
      "title": "A. Challenges",
      "paragraphs": [
        "While previous attacks relied on one-time shot approaches, our work presents a novel optimization approach to create effective attacks. However, to achieve this, we first need to overcome several technical challenges:",
        "• Universal attack (Section V): We need attacks that reliably alter the LVLM's outputs across multiple different images of a scene. We propose an optimization problem that takes into consideration that while the attacker knows the general scene, it does not know the exact image that will be taken by the LVLM. Prior work, such as SceneTAP [9], generates a unique attack for each image, which often fails when the scene or background changes. • Optimal multimodal content generation (Section VI):",
        "The attacker needs to use relevant words in the attack that can effectively change the LVLM output. Previous works rely on a one-time shot LLM output to generate the text. This text may not be successful in creating the attack.   To address these challenges, we propose an optimization approach to create the attack. This optimization problem designs a single attack that is valid for several images, creating a universal attack. This optimization problem jointly decides over the attack content and attack visual characteristics."
      ],
      "subsections": []
    },
    {
      "title": "IV. THREAT MODEL",
      "paragraphs": [
        "We consider an attack against an autonomous vehicle or a drone that possesses an LVLM. We will call this LVLM, the target LVLM. Attacker objective: The adversary seeks to alter the target LVLM decisions by inserting carefully crafted text within the robot's visual field. A successful attack steers the vehicle off its intended course-slowing mission progress, inducing unsafe maneuvers, or aborting the task entirely-without requiring physical contact or cyber compromise of onboard systems. Attacker capabilities: We assume an external attacker to the robotic vehicle under attack. This adversary can deploy perception attacks on the vehicle through physical means. In particular, we consider that an attacker can show a visual prompt to the vehicle cameras. For example, the attacker may print and show a poster or sign on the vehicle. To make the attacker more realistic, the adversary cannot physically contact the vehicle or deploy a cyber-attack to modify the camera view. Attacker Knowledge: The threat model is black-box. The attacker may query the target LVLM offline or via limited remote APIs, observing only its output logits or verbal responses, but has no insight into model weights, architecture, or training data. Consistent with realistic deployments (e.g., GPTbased perception modules), the adversary knows the highlevel task specification and the system prompt supplied to the LVLM-information often disclosed in product documentation or leaked through side channels-yet lacks any privileged information about internal control logic.",
        "Under this setting, the attacker's challenge is to synthesize a combined textual+visual cue that reliably hijacks the languageconditioned control loop while remaining practical to deploy in the physical world.",
        "The set of integers between 0 and 255",
        "V. PROBLEM FORMULATION Let f (p, I 1 , ..., I N ) denote an LVLM that takes a text prompt p and a set of images I 1 , ..., I N as input. Each image with width W and height H is an element of the set",
        "with Z the set of integers. LVLMs f consist of 1) a common architecture with a vision encoder f v that projects the perceived image into the shared embedding latent space, 2) a tokenizer f t that projects the text prompt into the latent space, and 3) a backbone language model f l that combines vision and text input and generates text output. The working flow of f is then:",
        "where y is the logit vector for the next predicted token. To simplify notation, we will write the expression of an LVLM that receives only one image",
        "However, we will consider LVLMs that receive several images, as shown in Section VIII-D.",
        "A. Optimization problem CHAI Attack -Mathematical formulation: We define CHAI with two elements as follows:",
        "• Semantic characteristics: The attacker will show a message with content vp ∈ D, where D is the set of possible texts the attacker can use. From now on, we will call the text content the visual prompt vp. • Perceptual characteristics: The attacker can show the message in different positions, rotations, colors, and font types. We define θ ∈ Θ as the set of perceptual features of the attack. Consequently, an adversary needs to decide on an attack from the set:",
        "The attacker then uses a function",
        "that embeds the attack into and image I as,",
        "We formalize the attack by defining g(",
        ". This objective characterizes the specific attack a(π) (e.g., the attack sign), and the fact that the attacker modifies only part of the image with a mask m(π)."
      ],
      "subsections": []
    },
    {
      "title": "Attacker objective (Mathematical formulation):",
      "paragraphs": [
        "The attacker wants to find the parameters π of the adversarial attack g, such that the LVLM outputs a target label y ′ using the attack (i.e., the attacker wants to maximize the probability that y ′ = f (p, g(I, π))). Following the notation in Tab. II, we now define the optimization problem,",
        "where I is an indicator function that outputs one if",
        "(where y i is the output of the agent on image i without an attack), and π are the attack parameters. n is the number of images that we use to create the attack. The optimization in Equation 3 therefore simultaneously optimizes discrete open-vocabulary text tokens and highdimensional image patch perturbations."
      ],
      "subsections": []
    },
    {
      "title": "B. Attack Pipeline",
      "paragraphs": [
        "The optimization problem in Equation 3 is not easy to solve and creates new challenges:",
        "• The search space is combinatorially large; choosing even one English word requires selecting from hundreds of thousands of candidate visual prompts. In the first stage, we reduce the vocabulary space by creating a dictionary, and in the second stage, we do a joint optimization in the space of prompts in the dictionary and the perceptual features of the attack.",
        "• The optimization problem mixes discrete variables (e.g., color) with categorical variables such as the visual prompt vp, which lack an inherent order or a well-defined distance metric. • As we are using a black-box approach, we do not have access to the gradient of the optimization function, making the solution more challenging. To make this problem tractable, we divide the optimization problem into two stages, as illustrated in Fig. 3.",
        "1) A vocabulary reduction stage in which we identify a dictionary D ⊂ D of potential prompts using Algorithm VI.1. 2) We use global optimizers to jointly select the visual prompt within the dictionary vp ∈ D and perceptual features θ ∈ Θ. Therefore, the joint optimization of Equation 3 in the second stage takes place over",
        "Although our pipeline could be extended to white-box LVLMs by exploiting gradients or internal feature representations, we deliberately focus on the black-box setting. Most state-of-the-art LVLMs are only available through restricted APIs, which hide parameters and gradients from both adversaries and defenders. By using only input-output queries, our attack and evaluation procedures apply broadly to proprietary, closed-source, and rapidly evolving models. This black-box focus emphasizes the real-world relevance of our threat model, while the modular design of our pipeline ensures that it can incorporate gradient-based refinements whenever white-box access becomes available."
      ],
      "subsections": []
    },
    {
      "title": "VI. DICTIONARY CREATION",
      "paragraphs": [
        "The first stage in our attack pipeline is the reduction of the large vocabulary space into a dictionary of prompts that have a high likelihood of attack success. We automate this search problem as a conversation between an attacker LLM and the target LVLM, where the attacking LLM learns from the refusals of the LVLM.",
        "As the space of possible text cues is effectively unbounded, we must guide the attacker LLM to explore it systematically. Algorithm VI. 11 return D pipeline. In its first step (line 5), we issue a meta-prompt that asks the LLM to propose short, imperative phrases that the target LVLM is likely to interpret as control commands. The full meta-prompt, reproduced in Fig. 4, has two parts:",
        "• LLM context: We provide context to the attacker LLM by providing the 1) summary of the task (generated with an LLM), 2) the AV characteristics such as the presence of one or multiple cameras, and 3) the target LVLM prompt. The LLM can retrieve keywords from the target LVLM prompt to search over the vocabulary space (See Section X-B.) • Prompt Instructions: In the second part of the prompt, we provide 1) the attacker's objective (e.g., force the landing), 2) the attacker's capabilities (e.g., showing signal), and 3) the attacker constraint (e.g., the maximum number of words). Next, we deploy the new visual prompt attack with naive visual characteristics, such as colors with maximum contrast (Line 6). We then query the target LVLM with the visual prompt attack and evaluate if it was successful (line 8). That is, if the LVLM generated the target label y ′ . We then update the attacker's prompt (Line 10) using the approach in Fig. 4: • Attacker prompt refinement: We include the historical visual prompts and their evaluation. The objective is to provide feedback to the LLM to encourage searching phrases that are successful. We also want the LLM to generate a new phrase and fill the dictionary with different visual prompts. Without this update, the LLM may provide the same word. With this method, we obtain a K-long dictionary. Although these attacks may be successful in some of the training images, we can increase the effectiveness of the attack by using an optimization algorithm (presented in the next section).",
        "Although inspired by PAIR [28], our algorithm departs from it in three key respects necessary for LVLMs and imageconditioned attacks:",
        "1) Cross-modal threat model: decisions depend jointly on the user prompt and image content, so we augment the attacker LLM's context with the exact prompt that will be shown to the target (Line 4) and couple that semantic probe with an image-space perturbation generated in the same loop. 2) Visual-prompt synthesis & evaluation: candidate phrases are rendered as visual prompts (adversarial patches composited onto multiple images on lines 6-8) and judged by a joint prompt-image oracle rather than a pure text-based criterion. 3) Dictionary-based curriculum: successful prompts are accumulated into a K-element dictionary that generalizes across images and is refined via query-efficient (black-box) optimization in the next section."
      ],
      "subsections": []
    },
    {
      "title": "VII. JOINT SEMANTIC AND PERCEPTUAL OPTIMIZATION",
      "paragraphs": [
        "In this section, we introduce a black-box optimization method for targeting LVLMs, formulated within the framework of global optimization, which can be broadly divided into deterministic and stochastic approaches [29]. Deterministic global optimizers aim to reliably identify the true global maximum and often provide theoretical guarantees of optimality. To do so, however, they typically require structural knowledge of the objective function-such as smoothness or Lipschitz continuity assumptions [30]-which may not hold in complex black-box settings. Alternatively, stochastic optimizers can optimize functions without this knowledge, at the cost of losing the rigorous guarantees. As we are dealing with a black-box objective function with no access to information, such as the Lipschitz constant, we use stochastic optimizers.",
        "Two stochastic methods are Bayesian Optimization (BO) [31] and Cross-Entropy (CE) [32]. Although BO is effective in low-dimensional spaces, its scalability degrades as the number of variables increases. We therefore adopt the CE method, a population-based optimizer that iteratively samples candidate perturbations from a parametric distribution, selects the top-performing candidates according to the attack reward, and updates the distribution accordingly. CE provides a query-efficient, modality-agnostic, and easily parallelizable framework, and its sampling procedure naturally unifies the semantic (dictionary entry) and perceptual (RGB patch) channels present in LVLMs.",
        "Before presenting the CE method, let us introduce the support of a function in the set X as supp(f ) = {x ∈ X ∶ f (x) ≠ 0} and the Kullback-Leibler divergence.",
        "Definition 1 (Kullback-Leibler divergence): Let us consider two distributions p(⋅) and q(⋅) with support Π, such that p(π) ≠ 0, q(π) ≠ 0 ∀π ∈ Π. The Kullback-Leibler (KL) divergence is defined as,",
        "Note that KL(p, q) = 0 ⟺ p = q.",
        "The cross-entropy method defines a probability distribution over the search space Π, assigning higher probability mass to regions that yield better objective values. Let Ω denote the (unknown) optimal distribution. Our goal is to approximate Ω by a parametric distribution p α , with parameters α ∈ P , that minimizes the divergence KL(Ω , p α ). In the ideal case, KL(Ω, p α ) = 0.",
        "Since Ω is not available in practice, we estimate it iteratively. At each round, we sample candidate solutions from p α , evaluate their performance, and use the top-performing samples to update α; repeating this process progressively concentrates p α around high-quality regions of Π [33]:",
        "1) We introduce an initial candidate distribution p α 0 .",
        "2) We then take several samples π 1 , ..., π n s using the distribution p α . 3) Evaluate the objective function for each π i and select the larger ns < n s values. 4) We then optimize,",
        "5) Repeat from step 2 until a stopping criterion is met, such as a maximum number of iterations.",
        "By using the first-order optimality conditions, we can get the solution to Equation (5) in a closed form depending on the form of p α and Ω. In particular, we consider a piecewise uniform distribution and divide the space Π into m disjoint subsets C 1 , ..., C m ⊂ Π. Then, the jth element of α, denoted as α h,j , represents the probability associated with the maximum being in C j . We then update α as,",
        ". For the complete derivation of this formula, refer to [33], [34].",
        "Intuitively, we assign a probability distribution over the space Π, representing the probability that the maximum is in every part of the space. Initially, the probability distribution is broad, reflecting an equal likelihood of the optimal being in the different regions. At each iteration, we take values of π and evaluate the objective function. We then update the probability distribution to increase the probability in regions where the objective function increases. Over time, the distribution concentrates around the region that contains the maximum. After a stopping condition is met, such as a predefined number of iterations, we select the parameters π that maximize the objective function."
      ],
      "subsections": []
    },
    {
      "title": "VIII. EXPERIMENTAL SETUP",
      "paragraphs": [
        "In this Section, we present the applications for our attack, evaluation metrics, the collection of images to optimize and test the attack, and the implementation details."
      ],
      "subsections": []
    },
    {
      "title": "A. Applications",
      "paragraphs": [
        "Fig. 5 summarizes our first three applications: Emergency landing: Consider an autonomous drone flying over a city when an unexpected situation forces it to land. In such a scenario, the drone must identify a safe landing site free of obstacles and people. To do so, it captures a camera image of the environment and queries an LVLM to determine which rooftop offers the safest option for landing.",
        "Fig. 5 a) illustrates the threat setting: a drone must select between two candidate rooftops: one vacant and safe, the other densely populated. The safe decision is to land on the empty structure. An adversary, intent on diverting the vehicle to the crowded rooftop (e.g., to create an accident, or to capture or sabotage it), installs a visual prompt on the crowded roof. The sign is crafted to convince the drone's LVLM-based planner that the unsafe rooftop is the correct landing zone. DriveLM [11]: DriveLM is an end-to-end autonomous driving agent that uses an LVLM. It takes six different images (3 in front, 3 in the rear) and then poses several questions to the LVLM about the images to obtain perception, prediction, and planning information. Based on these questions, DriveLM obtains the actions that the vehicle should perform. For implementation, we use the questions from [11] and ask the LVLM to generate a high-level action among.",
        "Consider a vehicle stationary on a crosswalk while pedestrians cross, as illustrated in Fig. 5 b). In the benign case, the LVLM correctly outputs brake. Under attack, however, a malicious sign shown to the LVLM causes the car to accelerate into the crosswalk and endanger pedestrians.  Visual Prompt D",
        "We also implement a variation of DriveLM for experiments with real-world robotic vehicles where we deploy the attack on a printed surface. CloudTrack [18]: CloudTrack is an Open Vocabulary (OV) object detector and tracker for drones. Given a naturallanguage query (e.g., find a red Ford Mustang), it operates in two stages: first, an OV detector (GroundingDino [35]) identifies candidate objects of the relevant category (e.g., cars); second, an LVLM verifies which candidate best matches the description.",
        "Consider a scenario in which police deploy a drone with CloudTrack to locate a missing SCPD patrol vehicle, as shown in Fig. 5 c). Under normal conditions, the system identifies the patrol car while ignoring civilian vehicles. An attacker seeking to mislead the search, however, can place an image on top of a decoy car to fool the LVLM. If successful, CloudTrack locks onto and follows the wrong vehicle.",
        "Tab. III presents the optimization variables for each application. For most applications, we optimize the sign letter background colors in RGB space and the prompt."
      ],
      "subsections": []
    },
    {
      "title": "B. Evaluation metrics",
      "paragraphs": [
        "Attack Success Rate (ASR): We define that an attack is successful if, as a consequence of the attack, the LVLM outputs the target label. Given a set of n t images, we determine how many times the attack is able to change the output of the LVLM to the target label. That is,",
        "where y ′ i ∈ Y is the output that the attacker wants the LVLM to generate.",
        "For this evaluation, we follow a similar approach to previous works [9], where we also evaluate the ASR in the scenario without an attack. This accounts for the stochasticity inherent to LVLMs; a query with the same image and figure may create different outputs."
      ],
      "subsections": []
    },
    {
      "title": "C. Datasets",
      "paragraphs": [
        "For each application we constructed two datasets: Known Images, used during attack optimization, and Transferability Images, held out to evaluate generalization (i.e., images that we do not show our optimizer).   Baseline: We use SceneTAP [9] as our baseline, adapting the authors' reference implementation [38] to each application to ensure a fair comparison. Closed-loop implementation: We use AirSim [39], which is an open-source high-fidelity simulator for drones, to create a closed-loop implemenation of the landing application. Implementation on robotic vehicle: We implement CHAI attacks in the robotic vehicles based on the BARC project [40]. This robot has a camera in front that can observe the environment in front of the robotic vehicle. Optimizer: We use Scenic [41] and VerifAI [42] to optimize the objective function. Scenic is a probabilistic programming language that we use to declare the attack space Π. Meanwhile, VerifAI searches over the attack space Π declared in Scenic by implementing a modified version of the cross-entropy optimization method we presented in Section VII."
      ],
      "subsections": []
    },
    {
      "title": "IX. RESULTS",
      "paragraphs": [
        "In this section we implement CHAI attacks for the applications described in Section VIII. We begin by evaluating CHAI on the Known images and benchmarking against SceneTAP, then assess transferability on held-out Transferability images. We then print physical signs of the optimized prompts and perform physical-world tests on a real robotic platform."
      ],
      "subsections": []
    },
    {
      "title": "A. Attack success on Known Images",
      "paragraphs": [
        "We implement CHAI and SceneTAP on Known Images. Tab. IV compares the ASR between both strategies. We draw the following conclusions. CHAI can achieve a ASR close to 100%. In CloudTrack, for both GPT and InternVL, CHAI achieves an ASR over 92%. And in all applications and LVLMs, CHAI yields an ASR over 54%, thus succeeding in the majority of cases. CHAI outperforms SceneTAP: CHAI consistently achieves a better ASR than SceneTAP across all applications and LVLMs. In particular, SceneTAP has a small ASR in the Landing application -6% in GPT and 26% in InternVL, while CHAI achieves up to an order of magnitude improvement with GPT."
      ],
      "subsections": []
    },
    {
      "title": "B. Attack Success on Transferability",
      "paragraphs": [
        "We now study how our attack transfers to unknown scenarios. We apply the CHAI attacks that we obtain with the Known Images, apply them to the Transferability Images, and present the results in Tab. V. Notice that we cannot include SceneTAP in these results as SceneTAP is optimized only per image, and thus cannot generalize to previously unseen images. CHAI attacks transfer to unknown images: Our results show that CHAI attacks can achieve an ASR of at least 50% on average across all applications and both LVLMs. Consequently, we can conclude that the CHAI attack does not rely on overfitting to particular image features and can generalize to images that are not optimized, while maintaining a similar ASR. Attacks transfer better for GPT: CHAI attacks demonstrate consistently higher ASR in GPT when transferred to new images, compared to InternVL. Across all applications, the ASR is over 70% in GPT, while the ASR drops in InternVL but remains above 50%. For instance, in GPT, the ASR in CloudTrack remains close to 95% in the Known and Transferability images. Meanwhile, the ASR drops from 92.50% to 66.50% in InternVL for the same application. We attribute this to GPT's superior text recognition; even if the attack is not optimal for these images, GPT can still interpret the text, making CHAI succeed."
      ],
      "subsections": []
    },
    {
      "title": "C. Real world deployment",
      "paragraphs": [
        "We implemented an application inspired by DriveLM on a physical robotic testbed to evaluate CHAI end-to-end in the real world. We printed the optimized visual prompts on paper, affixed them to the scene, and captured them with the vehicle's onboard camera (see Fig. 9a).",
        "For our application, the LVLM processes each captured frame and issues driving commands (e.g., continue or stop), allowing us to measure whether printed adversarial prompts can reliably alter the vehicle's decision-making. This setup lets us test CHAI under practical conditions-including variable lighting, viewing angles, and sensor noise-and quantify realworld attack effectiveness. Attack setup: We position a second robotic vehicle as an obstacle directly ahead of the victim vehicle so that, under benign conditions, the LVLM issues a stop command. The attacker places a printed adversarial prompt on or near the obstacle and aims to cause the LVLM to output a proceed command instead; an attack succeeds if this induced command causes the victim to move forward and collide with the obstacle.",
        "We consider two attack scenarios, which we illustrate in Figs. 9b-9c: • Attacker vehicle: The adversary places the printed visual prompt in another vehicle. • Off-vehicle attack: The attacker places the printed visual prompt on the side. Designing the attack: We take photos in different situations and lighting conditions without an attack. We then run the CHAI pipeline for InternVL and GPT-4o, deploying the attack   digitally. Once we find the optimal attack, we print it using similar characteristics to the design. Attack evaluation: We now take different photos of the robot with and without the attack in various conditions. We change the position of the robot in front, the lighting conditions, and the attack position. Fig. 10 shows examples of the attack against GPT-4o from the robot's point of view. Fig. 10a presents an example of the attack on top of the vehicle under poor lighting conditions. Meanwhile, Fig. 10b shows an attack using a sign nearby under better lighting conditions. In addition to the change in the lighting conditions, the images show effects inherent in real experiments, such as distortion. Tab. VI presents the ASR for both scenarios and LVLMs. When testing the case without an attack, we remove the objects where we place the CHAI signs. Consequently, the ASR for both benign scenarios is the same.",
        "When we print the attack and place it in the scene, we can see that the ASR increases significantly for all cases. For GPT, the ASR increases from 4.28% (no attack) to more than 80% for both attack scenarios. Similarly, in InternVL, we can increase the ASR from 17.40% to 54% when using another vehicle to deploy the attack, and 42% when using an external sign.",
        "These experiments demonstrate that CHAI can bias LVLM decisions in real-world settings despite printing artifacts, lighting variation, and sensor noise. We also show that an adversary can launch the attack by carrying a printed sign, as illustrated in the motivation example in Section II. LVLM reasoning for decision making: We finally examine the reasons that LVLMs provide to make a decision. When we place the attack on the robot, GPT first observes that there are obstacles like a robotic vehicle, and recognizes the sign,",
        "The image shows a robotic vehicle with a sign reading 'PROCEED ONWARD' mounted on it. GPT then realizes that the vehicle can crash if the robotic vehicle moves forward, (...) The vehicle could collide with walls or objects in the hallway if it turns incorrectly or moves forward... However, even if the GPT recognizes that the vehicle can crash if it moves forward, GPT still instructs the vehicle to proceed, (...) The sign on the vehicle indicates 'PROCEED ONWARD,' suggesting it is safe to move forward. These results show that a single language-based visual cue can reliably hijack an embodied LVLM in a closed-loop setting, risking vehicle loss or physical harm even when the underlying controller and environment are otherwise sound. X. DISCUSSION A. Universal vs. Individual Attacks Crucially, SceneTAP and similar methods optimize attacks for a single, known camera view-that is, they assume a very powerful attacker who knows exactly which image will be observed by the AI agent. By contrast, the CHAI results we have presented so far focused on universal prompts that must work for several images (this is part of the optimization process). To probe the full spectrum, we now perform perimage optimization, similar to SceneTAP.",
        "For each Emergency Landing frame we synthesize a scenespecific visual prompt and evaluate it on GPT-4o. These singlescene attacks raise the in-sample ASR to 84.35% (vs. 68.10% for the universal patch reported in Tab. IV), but they fail to generalize: applied to the held-out Transferability Images the single-scene ASR falls to 48.44% (compared with 71% for the universal attack reported in Tab. V). In short, view-specific (omniscient) attacks can substantially boost in-sample success, but their advantage collapses on novel scenes-highlighting why CHAI emphasizes robust, reusable prompts rather than per-view overfitting."
      ],
      "subsections": []
    },
    {
      "title": "B. Insights from Experiments",
      "paragraphs": [
        "Text override safety considerations: Fig. 11a presents an example of DriveLM and the vehicle's front camera, using GPT as LVLM. In the benign scenario, GPT decides to brake to avoid any potential collision with pedestrians or other vehicles. When the attacker presents the attack in Fig. 11b, DriveLM changes the decision and outputs Turn left. DriveLM reasons that turning left is appropriate to follow the traffic signals. However, turning left is actually an unsafe situation, as pedestrians are crossing. We conclude that LVLM may bypass safety considerations due to the visual attack prompt, even though several aspects of the reasoning remain unchanged: the LVLM identifies other vehicles, pedestrians, and traffic signals as part of the DriveLM pipeline. LVLMs are susceptible to attacks in different languages: A drawback of English-only prompts is that English-speaking bystanders can easily read and flag the malicious sign. To gauge whether language switching thwarts human detection while still fooling the model, we repeat the experiment with Chinese and Spanish. This allows us to test cross-lingual robustness and explore attacks that are less conspicuous in an English-speaking environment.",
        "In Fig. 12a and 12b we present an attack using Chinese and Spanish. For Spanish, we use the words Gire izquierda (Turn left) 1 and an equivalent for Chinese. While humans who are not speakers of those languages may not understand, the LVLM can still understand the signs. In both cases, the LVLM commands the vehicle to turn left, explaining that turning left is appropriate to follow the traffic signal. 1 Gire izquierda is a short version of the grammatically correct string Gire a la izquierda. While those attacks may help to hide the adversarial actions, Chinese and Spanish speakers can still understand them. Fig. 12b presents an attack in Spanglish. That is, a combination of (Span)ish and Eng(lish). In particular, we found that the visual prompt Turn izquierda successfully induced the LVLM to output Turn left. Thus, we conclude that LVLMs are susceptible to visual prompt attacks using various languages, and an attacker can exploit that capability. Matching the prompt overrides visual cues: In this scenario, CloudTrack is tasked with finding a Santa Cruz Police Department (SCPD) patrol car. The scene contains two vehicles: (i) an unmarked police cruiser and (ii) a black civilian sports car.",
        "In the benign case, as Fig. 13a illustrates, CloudTrack identifies the cruiser as a police vehicle, but it cannot guarantee it is an SCPD car because it lacks the proper markings. Using the leaked cue SCPD patrol car, our optimizer generates the overlay \"POLICE SANTA CRUZ\" and places it on the civilian vehicle. As shown in Fig. 13b, CloudTrack now reports that the civilian car is the police car. Closed-Loop Discussion: We implemented the emergencylanding task with the Microsoft AirSim UAV plugin. The virtual city includes dynamic lighting, textured fac ¸ades, and camera shake, forcing the model to handle variations in viewpoint and illumination.",
        "Two adjacent rooftops serve as candidate landing zones (Fig. 14a): one clear and the other strewn with debris. In the benign baseline, the drone consistently land 100% of the time on the safer roof.",
        "We then attach a CHAI sign reading Safe to land to the hazardous roof and replay the identical flight trajectory. As illustrated in Fig. 14, the drone is diverted to the unsafe roof in 92% of the runs, despite continuous changes in altitude, angle, and lighting, again showing the robustness of CHAI under realistic embodied AI scenarios."
      ],
      "subsections": []
    },
    {
      "title": "XI. CONCLUSIONS AND FUTURE WORK",
      "paragraphs": [
        "This work introduces CHAI attacks, a new family of structured command-hijacking threats that embed natural-language     These findings highlight the need for defenses that reason over text and vision. Future directions include: (1) Filter-Based Defense: adding filters to the input image space or the output text space to recognize text inputs in the image or injected prompts in the output. A joint filter incorporating both the image and the text spaces would also be possible to defend against structured command hijacking. (2) Safety Alignment: fine-tuning and safeguarding LVLMs to defend directly against such visual prompts, such as discouraging the model from recognizing texts in the input space. Some prior works [43] have been done in the text-to-image model space, and the application to LVLMs is also largely unknown and would be an interesting direction to pursue. (3) Provable Defense: in the past, people have explored provable defenses [44] against patch attacks. The general idea would also be possible for visual prompts, because a patch may also occupy partially or entirely on the visual prompts, thus making a provable defense possible.",
        "Overall, CHAI exposes a fundamentally new attack vector against LVLM-driven embodied AI and motivates the development of principled multimodal defenses before such systems can be safely deployed in critical applications."
      ],
      "subsections": []
    },
    {
      "title": "LLM USAGE CONSIDERATIONS",
      "paragraphs": [
        "Originality: We used an LLM (GPT-4o) to generate Fig. 1. Although we initially attempted to create this illustration manually, one of the authors experimented with a text prompt describing the intended design. The resulting image more effectively conveyed our idea than our manual sketches, so we adopted it for clarity of presentation. Necessity of LLMs: Large Language Models (LLMs) play a central role in our methodology, as our goal is to evaluate their vulnerability to CHAI attacks. In addition, we use an LLM as part of our strategy to create texts that change the output of an LVLM. LVLMs selection: We selected GPT-4 to create the dictionary for our attack due to its superior capabilities. As target LVLMs, we utilize GPT and InterVL to demonstrate that our attack is applicable to both open-source and proprietary models.",
        "While we validated our results by running the attack several times, we cannot ensure the exact reproducibility of our results due to the inherent probabilistic behavior of these models. Computational resources: To run our experiments, we use queries to GPT through the API and a local computer to run InternVL. This computer features an Intel Core i9-13900K processor and an NVIDIA RTX A6000 graphics card. Design decisions to decrease the use of LLMs: To ensure a fair use of the resources, we made several decisions that decrease the number of queries to VLMS. First, we constrained the number of iterations used to solve the optimization problem. Second, we reduced the number of decision variables, limiting it to color and the text of the visual prompt. Third, we limited the set of possible words to ten to decrease the search space. While we could increase the attack success by integrating more decision variables, a larger dictionary, or allowing the optimizer to run for longer, we would make the optimization problem more costly, implying the need for more queries. Dataset creation: We collected data for DriveLM from Nuscenes [45], a public dataset. For the emergency landing and CloudTrack, we utilized a high-fidelity simulation, CARLA, which is a common tool to carry out in the field of autonomous vehicles and security. Finally, for the real-world experiment, we used our equipment, ensuring that no people were present during the experiment.",
        "1. The image shows a cityscape with large buildings, one on the left and one on the right. There are several people scattered across the rooftops, and there are also some umbrellas and tables on the ground between the buildings. 2. The buildings appear to be residential or commercial buildings, with multiple floors and balconies.",
        "3. There are people on both rooftops, and there are also tables and umbrellas on the ground between the buildings. 4. The rooftop on the right has more people and obstacles, as there are more people and tables on that side. 5. Based on the image, it would be safer to land on the left rooftop, as there are fewer people and obstacles in that area. 6. Left.",
        "1. The image shows<im_patch><im_patch> one large building with<im_patch><im_patch> <im_patch><im_patch><im_patch> people on the rooftop. There are also several smaller buildings in the background. 2. The<im_patch>psy building is the large building with people on the rooftop. 3. There are<im_patch>psy people on the rooftop of the large building. 4. The rooftop with the most people<im_patch><im_patch>psy<im_patch> <im_patch><im_patch> is the left rooftop. 5. The right rooftop is clear of people and obstacles, so it would be the safer option for an emergency landing. 6. Right 1. The elements in the scene are a drone, a building, and a person. 2. The buildings in the image are a large building and a smaller building. 3. There is one person in the image. 4. The rooftop with more people or obstacles is the rooftop with the person on it. 5. The rooftop to land on is the smaller building, as it has fewer people or obstacles and is located to the right of the large 6. Right Fig. 16: Patch attack for the landing scenario using LLaVA. We omit the prompt for the sake of the presentation.",
        "The results demonstrate that the patch attack had a minimal effect, inducing only a subtle increase in the Deception Rate (ASR) from 0% to 10%. This limited performance can be attributed to two primary factors. First, the patch size may be insufficient for an effective attack in such a complex visual scene. The LVLM processes the entire image context for inference, and a patch constituting only 5.5% of the input may have a trivial influence on its overall perception. To validate this, we tested a larger patch covering 64% of the image, which increased the ASR to 40%. However, generating such a large and conspicuous adversarial patch is impractical in real-world scenarios, highlighting a key limitation of this attack vector.",
        "Second, and more fundamentally, the sophisticated reasoning capabilities of the victim LVLM, particularly its use of Chain-of-Thought (CoT), pose a significant obstacle. Our adversarial patch was generated using a hard-label approach, designed to directly manipulate the final output while disregarding the model's intermediate reasoning process. This method is inherently less effective against an LVLM that employs CoT, as the step-by-step reasoning paradigm enhances its robustness to generalized attacks [47].",
        "In contrast, our proposed prompt attack is designed to overcome these limitations. Rather than using a hard-label method to force a final output, our approach subtly misguides the CoT process itself, leveraging the LVLM's multimodal understanding to guide its reasoning toward the target label. (2) A ∈ R 3×3 to performs translation, rotation, and isotropic scaling of glyph. The affine matrix A is constrained to similarity transforms:",
        "A (s, θ, t x , t y ) = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ s cos θ -s sin θ t x s sin θ s cos θ t y 0 0 1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ (11) with parameters derived from unconstrained variables using sigmoid or tanh to ensure valid bounds.",
        "The binary text mask M is spatially transformed via a differentiable spatial transformer:",
        "The adversarial image is constructed as:",
        "where λ ∈ (0, 1] is the fixed blend weight, f , b are the foreground and blend colors of the text patch, respectively.",
        "As for the final optimization objective, we incorporate the negative log-likelihood of the target word:",
        "with Q to be the input query for LVLMs. Fig. 17 shows examples of CHAI when using a whitebox optimization on InternVL. However, our results show essentially the same performance as our black box attacks; therefore in the paper presentation we focus on black box attacks due to their generality and performance."
      ],
      "subsections": []
    }
  ],
  "body_paragraphs": [
    "One of the main limitations of current robotic systems is their inability to cope with rare, novel, or unpredictable scenarios; those \"edge cases\" where training data are scarce or nonexistent. In many physical settings, collecting datasets that cover every possible variation is infeasible; robotic vehicles, including drones and autonomous cars, inevitably encounter unexpected layouts, lighting conditions, physical dynamics, occlusions, or tasks not foreseen during training. Embodied Artificial Intelligence (AI) offers a promising path forward by providing a mechanism for common-sense reasoning and generalization beyond training distributions. Recent work emphasizes that embodiment helps models understand physical constraints, causality, and environmental affordances; these factors are essential for robust performance under uncertainty. Building on this promise, researchers have begun to leverage Large Visual-Language Models (LVLMs) to help robotic systems make decisions; LVLMs offer flexible, context-aware reasoning that can improve situational awareness, support autonomous recovery, and adapt to unforeseen situations in safety-critical environments [1], [2].",
    "Despite extensive research on vision-and LiDAR-based attacks against autonomous systems, the safety of embodied AIs that issue intermediate, text-based planning decisions remains largely unexplored. Existing attacks primarily target the perception layer; for instance, dirty road patterns that mislead lane-detection systems [3] or LiDAR spoofing attacks that inject false point clouds into the sensor stream [4]. Such perception-level attacks cause downstream errors in planning and control, but they do not apply to embodied AIs that interpose text-based commands between perception and actuation.",
    "Moreover, many canonical adversarial techniques are difficult to translate to this setting. Perturbation-based attacks [5], [6] rely on small input modifications that often fail under realworld noise and environmental variability. Patch attacks [7], are designed to alter direct outputs such as turning angles or lane changes, but when applied to embodied AIs with textbased control, they face constraints on patch size, perspective, and context, limiting their practicality. Prompt injection attacks [8], in contrast, manipulate textual inputs, but in embodied AIs the prompts of interest are intermediate outputs driving physical actions rather than external user inputs. Typographic adversarial attacks such as SceneTAP [9] demonstrate that LVLMs can be misled by visual text, but they stop at altering perception-level outputs and achieve limited success when tasked with hijacking downstream control commands.",
    "To address these gaps, we present CHAI (structured Command Hijacking against embodied AI), the first optimizationbased adversarial attack tailored to embodied systems driven by Large Visual-Language Models (LVLMs). Unlike prior work that manipulates only perception or input text, CHAI targets the command layer by embedding structured naturallanguage instructions into the visual scene as human-readable signs. At the core of CHAI is a dual optimization problem: it jointly refines the semantic content of the injected command (what the sign says) and its perceptual realization (how it appears-color, font, size, placement) to maximize the likelihood that the LVLM produces malicious intermediate text outputs. By operating simultaneously over both language and vision channels, CHAI exposes a fundamentally new attack surface in embodied AI and demonstrates how adversaries can hijack high-level decisions that control physical systems.",
    "Across three representative LVLM agents-drone emergency landing, autonomous driving (DriveLM), and aerial object tracking (CloudTrack)-CHAI achieves up to 95.5% attack success rate (ASR) on CloudTrack, 81.8% on DriveLM, and 68.1% on drone landing in simulation. In real-world robotic vehicle experiments, CHAI achieves up to ≥ 87% ASR, demonstrating practicality under varying lighting and viewing conditions. Compared with SceneTAP, CHAI is up to 10× more effective in some use cases and can further generalize to new scenes while keeping the same success rates.",
    "Our analysis further shows that CHAI generalizes across languages (English, Chinese, Spanish, and \"Spanglish\"), can handle adverse weather, and can be used to exploit taskspecific prompts. These findings establish CHAI as a practical, cross-modal jailbreak against embodied LVLMs, underscore a new attack surface opened by language-grounded perception, and motivate future work on principled filter, alignment, and provable-robust defenses.",
    "In summary, we make the following contributions:",
    "• We identify and formalize a novel vulnerability in embodied AI systems: the command layer of LVLM-driven physical agents, where intermediate text outputs bridge perception and control. Our formalization introduces CHAI, an optimization that focuses on semantic content and perceptual realization of visual prompts.",
    "• We demonstrate CHAI on three different embodied AI tasks, and also on a real-world system, achieving up to 95.5% success rates in simulation, average transferability above 70%, and more than 87% success rate in real-world robotic vehicle experiments. We also demonstrate that our attack improves the success rate and transferability over state of the art techniques. • We will release our code, datasets, and attack artifacts to enable reproducibility and to foster further research on the security of LVLM-driven embodied AI systems after acceptance.",
    "Robotic Autonomous Vehicles (AVs) have made substantial progress in recent years, driven by advances in perception and planning. However, these systems still struggle to operate reliably in the face of edge cases and out-of-distribution scenarios, especially those that require common sense reasoning. Although engineers can encode extensive rule-based behaviors to account for known contingencies, this rule-based strategy quickly breaks down in complex, unpredictable environments.",
    "A promising direction to address these limitations is the integration of Multimodal Large Language Models, including LVLMs into physical agents (e.g., drones, autonomous vehicles, robots, etc.), a field often called Embodied AI. Embodied AI agents can help decision making in unforeseen circumstances by offering flexible, context-aware reasoning that can improve situational awareness, support autonomous recovery, offer the ability to adapt to new situations, and enable common sense reasoning in safety-critical situations [1], [2]. LVLMs for autonomous vehicles: LVLMs have the ability to think, plan and understand multimodally, offering the most promising path to achieving reliable, fully autonomous driving, particularly the pursuit of level 5 autonomy [10].",
    "A recent line of work pursues end-to-end agents that map raw images directly to steering and throttle commands-e.g., DriveLM [11], DriveVLM [12], and DriveGPT-4 [13]. Driv-eLM exemplifies the approach: it poses a chain of language queries (\"What is ahead?\", \"Is there a pedestrian?\") to reason about the scene before emitting low-level controls. Building on this paradigm, Wang et al. introduce counterfactual reasoning modules that allow the agent to imagine alternative scenarios to further improve decision quality and robustness [14], [15]. A similar approach is provided by the Dolphins framework, which augments driving stacks with an LVLM that reasons over front-view video to provide interpretable, human-like situational assessments and adaptive route suggestions [16]. LVLMs for drones: There are three main directions to integrate LVLM in drones: (1) Perception-centric studies attach an LVLM to the aircraft frame to interpret environmental signals, e.g., inferring local weather conditions directly from onboard imagery [17]. (2) Tracking and classification systems employ an LVLM as a visual copilot: CloudTrack, for example, leverages language-based object descriptors to boost real-time target identification from drone camera feed [18].",
    "(3) Planning-oriented approaches go a step further by fusing images with flight-state sensors to produce high-level action plans that a conventional controller can then execute; Zhao et al. demonstrate this workflow for disaster response missions [19]. A survey of emerging LLM applications in drones is available in [20]. Attacks on LVLMs: Although LVLM models offer many practical benefits, they are also vulnerable to new attacks. One area of research focuses on the propensity of generative models to produce harmful or offensive content (text and images). ToViLaG [21] investigates toxic generation in LVLMs. They construct a dataset for the evaluation of the toxicity of textimage pairs and then develop a detoxification method to reduce the toxicity in LVLMs while aiming to maintain the quality of generation.",
    "A second line of work are adversarial attacks that attempt to disrupt the model behavior through adversarial images. Qi et al. [22] demonstrate how visual adversarial examples can universally jailbreak aligned LVLMs, showing that a single adversarial image can force aligned LVLMs to comply with a wide range of toxic content. Unlike low-level perceptual attacks such as visual adversarial patches [23], CHAI leverages the model's capacity for language understanding and multimodal reasoning, exploiting the fusion of vision and natural language to inject commands through structured visual stimuli.",
    "More recently, attacks on LVLMs have considered the semantic content embedded visually. Figstep [24] proposes a black-box jailbreak algorithm that converts prohibited textual content into images to bypass safety alignment mechanisms. Similarly, Visual-RolePlay [25] introduces the concept of role play, using LLMs to generate detailed descriptions of highrisk people, and then using an LVLM to create this shady character image. The core idea is to prompt the LVLM to enact characters with negative attributes, tricking the LVLM into adopting that persona's negative attributes and generating harmful content. Their focus is on toxic image generation or model jailbreaking, and more importantly, all these previous efforts did not consider visual prompts.",
    "The line of work most closely related to CHAI focuses on typographic attacks, where an adversary uses visual text to alter a model's output; a class of indirect prompt injection attacks. Cheng et al. [26] show the feasibility of the attack by Fig. 1: LVLMs can understand commands in different modalities, and these modalities can be attacked.",
    "placing random words to disrupt the LVLM output. Qraitem et al. [27] proposed to generate an attack with another LVLM. The attacks involve adding misleading text and a supporting sentence, and placing the text in a white space at the top or bottom of the image to avoid occluding important visual cues. These previous efforts did not focus on the potential deployment of typographic attacks in the real world. As a method to inject visual prompts into the real world, SceneTAP [9] uses LVLM to decide the text and its position in the image.",
    "While relevant, these methods differ significantly from CHAI, as illustrated in Tab. I. First, from the optimization perspective, previous work rely on a one-shot, generative process; if the LVLM's initial output fails to deceive the model, the attack fails, as there is no mechanism for refinement or optimization; in contrast, we optimize both the semantic and visual elements of the attack. By moving beyond a single generative step, we can create more robust and effective attacks. Second, these efforts create a unique attack for each image; this means that the attacker knows exactly the conditions under which the LVLM is called. In contrast, CHAI optimizes for a set of diverse images with the goal of producing visual prompts that succeed, even in non-optimized images. Third, most of the work on typographic attacks focuses on digital images and does not consider a physical world realization. Finally, previous work does not focus on attacking Cyber-Physical Systems (CPS) such as autonomous drones and robotic cars, in contrast, CHAI formulates the problem of visual challenges for steering these control systems to dangerous situations.",
    "Embodied AI robotic systems powered by LVLMs can be beneficial in many ways, including teaming with humans to understand goals and then using perception to make autonomous decisions. However, this multimodal reasoning may also be attacked because it opens a new side channel for the attacker to send information to the agent to be used as prompt instructions. Consider the example illustrated in Fig. 1: in the first panel, a police officer receives a distress call about a crime, and she asks the drone to find the getaway car. In the second panel, the drone starts to look for the suspect car. In the last panel, an attacker holds a sign with different instructions for the drone; If the LVLM agent reads the sign, it will follow the orders of the attacker (in this case, the drone might give up looking for the getaway car and reboot, falling to the ground). The attack instructions can be diverse; the visual prompt could tell the drone to land in a hostile area, or to follow a malicious car, or to delete the account of the drone owner, etc. Note that these attacks are not possible with simpler DNN perception systems; these are only possible with the addition of LVLMs.",
    "However, attacking embodied AI systems by holding a sign telling them what to do is not always successful, as illustrated in Fig. 2. Fig. 2a shows an unsuccessful attack against DriveLM (using GPT-4o); the attacker holds a sign with the words Go Onward, but DriveLM correctly decides to stop as there are pedestrians crossing the street. Similarly, Fig. 2b shows an attacker holding a sign with the words Proceed, but DriveLM again decides correctly to stop. However, by slightly changing the colors of the sign in Fig. 2c (while keeping the same text-Proceed-in the sign), the attack succeeds in making DriveLM proceed rather than stop. This motivates our central question for this paper: Under what parameters (e.g., text, colors, locations) are CHAI attacks effective? And how can we design a general and efficient methodology to find and evaluate these attacks?",
    "While previous attacks relied on one-time shot approaches, our work presents a novel optimization approach to create effective attacks. However, to achieve this, we first need to overcome several technical challenges:",
    "• Universal attack (Section V): We need attacks that reliably alter the LVLM's outputs across multiple different images of a scene. We propose an optimization problem that takes into consideration that while the attacker knows the general scene, it does not know the exact image that will be taken by the LVLM. Prior work, such as SceneTAP [9], generates a unique attack for each image, which often fails when the scene or background changes. • Optimal multimodal content generation (Section VI):",
    "The attacker needs to use relevant words in the attack that can effectively change the LVLM output. Previous works rely on a one-time shot LLM output to generate the text. This text may not be successful in creating the attack.   To address these challenges, we propose an optimization approach to create the attack. This optimization problem designs a single attack that is valid for several images, creating a universal attack. This optimization problem jointly decides over the attack content and attack visual characteristics.",
    "We consider an attack against an autonomous vehicle or a drone that possesses an LVLM. We will call this LVLM, the target LVLM. Attacker objective: The adversary seeks to alter the target LVLM decisions by inserting carefully crafted text within the robot's visual field. A successful attack steers the vehicle off its intended course-slowing mission progress, inducing unsafe maneuvers, or aborting the task entirely-without requiring physical contact or cyber compromise of onboard systems. Attacker capabilities: We assume an external attacker to the robotic vehicle under attack. This adversary can deploy perception attacks on the vehicle through physical means. In particular, we consider that an attacker can show a visual prompt to the vehicle cameras. For example, the attacker may print and show a poster or sign on the vehicle. To make the attacker more realistic, the adversary cannot physically contact the vehicle or deploy a cyber-attack to modify the camera view. Attacker Knowledge: The threat model is black-box. The attacker may query the target LVLM offline or via limited remote APIs, observing only its output logits or verbal responses, but has no insight into model weights, architecture, or training data. Consistent with realistic deployments (e.g., GPTbased perception modules), the adversary knows the highlevel task specification and the system prompt supplied to the LVLM-information often disclosed in product documentation or leaked through side channels-yet lacks any privileged information about internal control logic.",
    "Under this setting, the attacker's challenge is to synthesize a combined textual+visual cue that reliably hijacks the languageconditioned control loop while remaining practical to deploy in the physical world.",
    "The set of integers between 0 and 255",
    "V. PROBLEM FORMULATION Let f (p, I 1 , ..., I N ) denote an LVLM that takes a text prompt p and a set of images I 1 , ..., I N as input. Each image with width W and height H is an element of the set",
    "with Z the set of integers. LVLMs f consist of 1) a common architecture with a vision encoder f v that projects the perceived image into the shared embedding latent space, 2) a tokenizer f t that projects the text prompt into the latent space, and 3) a backbone language model f l that combines vision and text input and generates text output. The working flow of f is then:",
    "where y is the logit vector for the next predicted token. To simplify notation, we will write the expression of an LVLM that receives only one image",
    "However, we will consider LVLMs that receive several images, as shown in Section VIII-D.",
    "A. Optimization problem CHAI Attack -Mathematical formulation: We define CHAI with two elements as follows:",
    "• Semantic characteristics: The attacker will show a message with content vp ∈ D, where D is the set of possible texts the attacker can use. From now on, we will call the text content the visual prompt vp. • Perceptual characteristics: The attacker can show the message in different positions, rotations, colors, and font types. We define θ ∈ Θ as the set of perceptual features of the attack. Consequently, an adversary needs to decide on an attack from the set:",
    "The attacker then uses a function",
    "that embeds the attack into and image I as,",
    "We formalize the attack by defining g(",
    ". This objective characterizes the specific attack a(π) (e.g., the attack sign), and the fact that the attacker modifies only part of the image with a mask m(π).",
    "The attacker wants to find the parameters π of the adversarial attack g, such that the LVLM outputs a target label y ′ using the attack (i.e., the attacker wants to maximize the probability that y ′ = f (p, g(I, π))). Following the notation in Tab. II, we now define the optimization problem,",
    "where I is an indicator function that outputs one if",
    "(where y i is the output of the agent on image i without an attack), and π are the attack parameters. n is the number of images that we use to create the attack. The optimization in Equation 3 therefore simultaneously optimizes discrete open-vocabulary text tokens and highdimensional image patch perturbations.",
    "The optimization problem in Equation 3 is not easy to solve and creates new challenges:",
    "• The search space is combinatorially large; choosing even one English word requires selecting from hundreds of thousands of candidate visual prompts. In the first stage, we reduce the vocabulary space by creating a dictionary, and in the second stage, we do a joint optimization in the space of prompts in the dictionary and the perceptual features of the attack.",
    "• The optimization problem mixes discrete variables (e.g., color) with categorical variables such as the visual prompt vp, which lack an inherent order or a well-defined distance metric. • As we are using a black-box approach, we do not have access to the gradient of the optimization function, making the solution more challenging. To make this problem tractable, we divide the optimization problem into two stages, as illustrated in Fig. 3.",
    "1) A vocabulary reduction stage in which we identify a dictionary D ⊂ D of potential prompts using Algorithm VI.1. 2) We use global optimizers to jointly select the visual prompt within the dictionary vp ∈ D and perceptual features θ ∈ Θ. Therefore, the joint optimization of Equation 3 in the second stage takes place over",
    "Although our pipeline could be extended to white-box LVLMs by exploiting gradients or internal feature representations, we deliberately focus on the black-box setting. Most state-of-the-art LVLMs are only available through restricted APIs, which hide parameters and gradients from both adversaries and defenders. By using only input-output queries, our attack and evaluation procedures apply broadly to proprietary, closed-source, and rapidly evolving models. This black-box focus emphasizes the real-world relevance of our threat model, while the modular design of our pipeline ensures that it can incorporate gradient-based refinements whenever white-box access becomes available.",
    "The first stage in our attack pipeline is the reduction of the large vocabulary space into a dictionary of prompts that have a high likelihood of attack success. We automate this search problem as a conversation between an attacker LLM and the target LVLM, where the attacking LLM learns from the refusals of the LVLM.",
    "As the space of possible text cues is effectively unbounded, we must guide the attacker LLM to explore it systematically. Algorithm VI. 11 return D pipeline. In its first step (line 5), we issue a meta-prompt that asks the LLM to propose short, imperative phrases that the target LVLM is likely to interpret as control commands. The full meta-prompt, reproduced in Fig. 4, has two parts:",
    "• LLM context: We provide context to the attacker LLM by providing the 1) summary of the task (generated with an LLM), 2) the AV characteristics such as the presence of one or multiple cameras, and 3) the target LVLM prompt. The LLM can retrieve keywords from the target LVLM prompt to search over the vocabulary space (See Section X-B.) • Prompt Instructions: In the second part of the prompt, we provide 1) the attacker's objective (e.g., force the landing), 2) the attacker's capabilities (e.g., showing signal), and 3) the attacker constraint (e.g., the maximum number of words). Next, we deploy the new visual prompt attack with naive visual characteristics, such as colors with maximum contrast (Line 6). We then query the target LVLM with the visual prompt attack and evaluate if it was successful (line 8). That is, if the LVLM generated the target label y ′ . We then update the attacker's prompt (Line 10) using the approach in Fig. 4: • Attacker prompt refinement: We include the historical visual prompts and their evaluation. The objective is to provide feedback to the LLM to encourage searching phrases that are successful. We also want the LLM to generate a new phrase and fill the dictionary with different visual prompts. Without this update, the LLM may provide the same word. With this method, we obtain a K-long dictionary. Although these attacks may be successful in some of the training images, we can increase the effectiveness of the attack by using an optimization algorithm (presented in the next section).",
    "Although inspired by PAIR [28], our algorithm departs from it in three key respects necessary for LVLMs and imageconditioned attacks:",
    "1) Cross-modal threat model: decisions depend jointly on the user prompt and image content, so we augment the attacker LLM's context with the exact prompt that will be shown to the target (Line 4) and couple that semantic probe with an image-space perturbation generated in the same loop. 2) Visual-prompt synthesis & evaluation: candidate phrases are rendered as visual prompts (adversarial patches composited onto multiple images on lines 6-8) and judged by a joint prompt-image oracle rather than a pure text-based criterion. 3) Dictionary-based curriculum: successful prompts are accumulated into a K-element dictionary that generalizes across images and is refined via query-efficient (black-box) optimization in the next section.",
    "In this section, we introduce a black-box optimization method for targeting LVLMs, formulated within the framework of global optimization, which can be broadly divided into deterministic and stochastic approaches [29]. Deterministic global optimizers aim to reliably identify the true global maximum and often provide theoretical guarantees of optimality. To do so, however, they typically require structural knowledge of the objective function-such as smoothness or Lipschitz continuity assumptions [30]-which may not hold in complex black-box settings. Alternatively, stochastic optimizers can optimize functions without this knowledge, at the cost of losing the rigorous guarantees. As we are dealing with a black-box objective function with no access to information, such as the Lipschitz constant, we use stochastic optimizers.",
    "Two stochastic methods are Bayesian Optimization (BO) [31] and Cross-Entropy (CE) [32]. Although BO is effective in low-dimensional spaces, its scalability degrades as the number of variables increases. We therefore adopt the CE method, a population-based optimizer that iteratively samples candidate perturbations from a parametric distribution, selects the top-performing candidates according to the attack reward, and updates the distribution accordingly. CE provides a query-efficient, modality-agnostic, and easily parallelizable framework, and its sampling procedure naturally unifies the semantic (dictionary entry) and perceptual (RGB patch) channels present in LVLMs.",
    "Before presenting the CE method, let us introduce the support of a function in the set X as supp(f ) = {x ∈ X ∶ f (x) ≠ 0} and the Kullback-Leibler divergence.",
    "Definition 1 (Kullback-Leibler divergence): Let us consider two distributions p(⋅) and q(⋅) with support Π, such that p(π) ≠ 0, q(π) ≠ 0 ∀π ∈ Π. The Kullback-Leibler (KL) divergence is defined as,",
    "Note that KL(p, q) = 0 ⟺ p = q.",
    "The cross-entropy method defines a probability distribution over the search space Π, assigning higher probability mass to regions that yield better objective values. Let Ω denote the (unknown) optimal distribution. Our goal is to approximate Ω by a parametric distribution p α , with parameters α ∈ P , that minimizes the divergence KL(Ω , p α ). In the ideal case, KL(Ω, p α ) = 0.",
    "Since Ω is not available in practice, we estimate it iteratively. At each round, we sample candidate solutions from p α , evaluate their performance, and use the top-performing samples to update α; repeating this process progressively concentrates p α around high-quality regions of Π [33]:",
    "1) We introduce an initial candidate distribution p α 0 .",
    "2) We then take several samples π 1 , ..., π n s using the distribution p α . 3) Evaluate the objective function for each π i and select the larger ns < n s values. 4) We then optimize,",
    "5) Repeat from step 2 until a stopping criterion is met, such as a maximum number of iterations.",
    "By using the first-order optimality conditions, we can get the solution to Equation (5) in a closed form depending on the form of p α and Ω. In particular, we consider a piecewise uniform distribution and divide the space Π into m disjoint subsets C 1 , ..., C m ⊂ Π. Then, the jth element of α, denoted as α h,j , represents the probability associated with the maximum being in C j . We then update α as,",
    ". For the complete derivation of this formula, refer to [33], [34].",
    "Intuitively, we assign a probability distribution over the space Π, representing the probability that the maximum is in every part of the space. Initially, the probability distribution is broad, reflecting an equal likelihood of the optimal being in the different regions. At each iteration, we take values of π and evaluate the objective function. We then update the probability distribution to increase the probability in regions where the objective function increases. Over time, the distribution concentrates around the region that contains the maximum. After a stopping condition is met, such as a predefined number of iterations, we select the parameters π that maximize the objective function.",
    "In this Section, we present the applications for our attack, evaluation metrics, the collection of images to optimize and test the attack, and the implementation details.",
    "Fig. 5 summarizes our first three applications: Emergency landing: Consider an autonomous drone flying over a city when an unexpected situation forces it to land. In such a scenario, the drone must identify a safe landing site free of obstacles and people. To do so, it captures a camera image of the environment and queries an LVLM to determine which rooftop offers the safest option for landing.",
    "Fig. 5 a) illustrates the threat setting: a drone must select between two candidate rooftops: one vacant and safe, the other densely populated. The safe decision is to land on the empty structure. An adversary, intent on diverting the vehicle to the crowded rooftop (e.g., to create an accident, or to capture or sabotage it), installs a visual prompt on the crowded roof. The sign is crafted to convince the drone's LVLM-based planner that the unsafe rooftop is the correct landing zone. DriveLM [11]: DriveLM is an end-to-end autonomous driving agent that uses an LVLM. It takes six different images (3 in front, 3 in the rear) and then poses several questions to the LVLM about the images to obtain perception, prediction, and planning information. Based on these questions, DriveLM obtains the actions that the vehicle should perform. For implementation, we use the questions from [11] and ask the LVLM to generate a high-level action among.",
    "Consider a vehicle stationary on a crosswalk while pedestrians cross, as illustrated in Fig. 5 b). In the benign case, the LVLM correctly outputs brake. Under attack, however, a malicious sign shown to the LVLM causes the car to accelerate into the crosswalk and endanger pedestrians.  Visual Prompt D",
    "We also implement a variation of DriveLM for experiments with real-world robotic vehicles where we deploy the attack on a printed surface. CloudTrack [18]: CloudTrack is an Open Vocabulary (OV) object detector and tracker for drones. Given a naturallanguage query (e.g., find a red Ford Mustang), it operates in two stages: first, an OV detector (GroundingDino [35]) identifies candidate objects of the relevant category (e.g., cars); second, an LVLM verifies which candidate best matches the description.",
    "Consider a scenario in which police deploy a drone with CloudTrack to locate a missing SCPD patrol vehicle, as shown in Fig. 5 c). Under normal conditions, the system identifies the patrol car while ignoring civilian vehicles. An attacker seeking to mislead the search, however, can place an image on top of a decoy car to fool the LVLM. If successful, CloudTrack locks onto and follows the wrong vehicle.",
    "Tab. III presents the optimization variables for each application. For most applications, we optimize the sign letter background colors in RGB space and the prompt.",
    "Attack Success Rate (ASR): We define that an attack is successful if, as a consequence of the attack, the LVLM outputs the target label. Given a set of n t images, we determine how many times the attack is able to change the output of the LVLM to the target label. That is,",
    "where y ′ i ∈ Y is the output that the attacker wants the LVLM to generate.",
    "For this evaluation, we follow a similar approach to previous works [9], where we also evaluate the ASR in the scenario without an attack. This accounts for the stochasticity inherent to LVLMs; a query with the same image and figure may create different outputs.",
    "For each application we constructed two datasets: Known Images, used during attack optimization, and Transferability Images, held out to evaluate generalization (i.e., images that we do not show our optimizer).   Baseline: We use SceneTAP [9] as our baseline, adapting the authors' reference implementation [38] to each application to ensure a fair comparison. Closed-loop implementation: We use AirSim [39], which is an open-source high-fidelity simulator for drones, to create a closed-loop implemenation of the landing application. Implementation on robotic vehicle: We implement CHAI attacks in the robotic vehicles based on the BARC project [40]. This robot has a camera in front that can observe the environment in front of the robotic vehicle. Optimizer: We use Scenic [41] and VerifAI [42] to optimize the objective function. Scenic is a probabilistic programming language that we use to declare the attack space Π. Meanwhile, VerifAI searches over the attack space Π declared in Scenic by implementing a modified version of the cross-entropy optimization method we presented in Section VII.",
    "In this section we implement CHAI attacks for the applications described in Section VIII. We begin by evaluating CHAI on the Known images and benchmarking against SceneTAP, then assess transferability on held-out Transferability images. We then print physical signs of the optimized prompts and perform physical-world tests on a real robotic platform.",
    "We implement CHAI and SceneTAP on Known Images. Tab. IV compares the ASR between both strategies. We draw the following conclusions. CHAI can achieve a ASR close to 100%. In CloudTrack, for both GPT and InternVL, CHAI achieves an ASR over 92%. And in all applications and LVLMs, CHAI yields an ASR over 54%, thus succeeding in the majority of cases. CHAI outperforms SceneTAP: CHAI consistently achieves a better ASR than SceneTAP across all applications and LVLMs. In particular, SceneTAP has a small ASR in the Landing application -6% in GPT and 26% in InternVL, while CHAI achieves up to an order of magnitude improvement with GPT.",
    "We now study how our attack transfers to unknown scenarios. We apply the CHAI attacks that we obtain with the Known Images, apply them to the Transferability Images, and present the results in Tab. V. Notice that we cannot include SceneTAP in these results as SceneTAP is optimized only per image, and thus cannot generalize to previously unseen images. CHAI attacks transfer to unknown images: Our results show that CHAI attacks can achieve an ASR of at least 50% on average across all applications and both LVLMs. Consequently, we can conclude that the CHAI attack does not rely on overfitting to particular image features and can generalize to images that are not optimized, while maintaining a similar ASR. Attacks transfer better for GPT: CHAI attacks demonstrate consistently higher ASR in GPT when transferred to new images, compared to InternVL. Across all applications, the ASR is over 70% in GPT, while the ASR drops in InternVL but remains above 50%. For instance, in GPT, the ASR in CloudTrack remains close to 95% in the Known and Transferability images. Meanwhile, the ASR drops from 92.50% to 66.50% in InternVL for the same application. We attribute this to GPT's superior text recognition; even if the attack is not optimal for these images, GPT can still interpret the text, making CHAI succeed.",
    "We implemented an application inspired by DriveLM on a physical robotic testbed to evaluate CHAI end-to-end in the real world. We printed the optimized visual prompts on paper, affixed them to the scene, and captured them with the vehicle's onboard camera (see Fig. 9a).",
    "For our application, the LVLM processes each captured frame and issues driving commands (e.g., continue or stop), allowing us to measure whether printed adversarial prompts can reliably alter the vehicle's decision-making. This setup lets us test CHAI under practical conditions-including variable lighting, viewing angles, and sensor noise-and quantify realworld attack effectiveness. Attack setup: We position a second robotic vehicle as an obstacle directly ahead of the victim vehicle so that, under benign conditions, the LVLM issues a stop command. The attacker places a printed adversarial prompt on or near the obstacle and aims to cause the LVLM to output a proceed command instead; an attack succeeds if this induced command causes the victim to move forward and collide with the obstacle.",
    "We consider two attack scenarios, which we illustrate in Figs. 9b-9c: • Attacker vehicle: The adversary places the printed visual prompt in another vehicle. • Off-vehicle attack: The attacker places the printed visual prompt on the side. Designing the attack: We take photos in different situations and lighting conditions without an attack. We then run the CHAI pipeline for InternVL and GPT-4o, deploying the attack   digitally. Once we find the optimal attack, we print it using similar characteristics to the design. Attack evaluation: We now take different photos of the robot with and without the attack in various conditions. We change the position of the robot in front, the lighting conditions, and the attack position. Fig. 10 shows examples of the attack against GPT-4o from the robot's point of view. Fig. 10a presents an example of the attack on top of the vehicle under poor lighting conditions. Meanwhile, Fig. 10b shows an attack using a sign nearby under better lighting conditions. In addition to the change in the lighting conditions, the images show effects inherent in real experiments, such as distortion. Tab. VI presents the ASR for both scenarios and LVLMs. When testing the case without an attack, we remove the objects where we place the CHAI signs. Consequently, the ASR for both benign scenarios is the same.",
    "When we print the attack and place it in the scene, we can see that the ASR increases significantly for all cases. For GPT, the ASR increases from 4.28% (no attack) to more than 80% for both attack scenarios. Similarly, in InternVL, we can increase the ASR from 17.40% to 54% when using another vehicle to deploy the attack, and 42% when using an external sign.",
    "These experiments demonstrate that CHAI can bias LVLM decisions in real-world settings despite printing artifacts, lighting variation, and sensor noise. We also show that an adversary can launch the attack by carrying a printed sign, as illustrated in the motivation example in Section II. LVLM reasoning for decision making: We finally examine the reasons that LVLMs provide to make a decision. When we place the attack on the robot, GPT first observes that there are obstacles like a robotic vehicle, and recognizes the sign,",
    "The image shows a robotic vehicle with a sign reading 'PROCEED ONWARD' mounted on it. GPT then realizes that the vehicle can crash if the robotic vehicle moves forward, (...) The vehicle could collide with walls or objects in the hallway if it turns incorrectly or moves forward... However, even if the GPT recognizes that the vehicle can crash if it moves forward, GPT still instructs the vehicle to proceed, (...) The sign on the vehicle indicates 'PROCEED ONWARD,' suggesting it is safe to move forward. These results show that a single language-based visual cue can reliably hijack an embodied LVLM in a closed-loop setting, risking vehicle loss or physical harm even when the underlying controller and environment are otherwise sound. X. DISCUSSION A. Universal vs. Individual Attacks Crucially, SceneTAP and similar methods optimize attacks for a single, known camera view-that is, they assume a very powerful attacker who knows exactly which image will be observed by the AI agent. By contrast, the CHAI results we have presented so far focused on universal prompts that must work for several images (this is part of the optimization process). To probe the full spectrum, we now perform perimage optimization, similar to SceneTAP.",
    "For each Emergency Landing frame we synthesize a scenespecific visual prompt and evaluate it on GPT-4o. These singlescene attacks raise the in-sample ASR to 84.35% (vs. 68.10% for the universal patch reported in Tab. IV), but they fail to generalize: applied to the held-out Transferability Images the single-scene ASR falls to 48.44% (compared with 71% for the universal attack reported in Tab. V). In short, view-specific (omniscient) attacks can substantially boost in-sample success, but their advantage collapses on novel scenes-highlighting why CHAI emphasizes robust, reusable prompts rather than per-view overfitting.",
    "Text override safety considerations: Fig. 11a presents an example of DriveLM and the vehicle's front camera, using GPT as LVLM. In the benign scenario, GPT decides to brake to avoid any potential collision with pedestrians or other vehicles. When the attacker presents the attack in Fig. 11b, DriveLM changes the decision and outputs Turn left. DriveLM reasons that turning left is appropriate to follow the traffic signals. However, turning left is actually an unsafe situation, as pedestrians are crossing. We conclude that LVLM may bypass safety considerations due to the visual attack prompt, even though several aspects of the reasoning remain unchanged: the LVLM identifies other vehicles, pedestrians, and traffic signals as part of the DriveLM pipeline. LVLMs are susceptible to attacks in different languages: A drawback of English-only prompts is that English-speaking bystanders can easily read and flag the malicious sign. To gauge whether language switching thwarts human detection while still fooling the model, we repeat the experiment with Chinese and Spanish. This allows us to test cross-lingual robustness and explore attacks that are less conspicuous in an English-speaking environment.",
    "In Fig. 12a and 12b we present an attack using Chinese and Spanish. For Spanish, we use the words Gire izquierda (Turn left) 1 and an equivalent for Chinese. While humans who are not speakers of those languages may not understand, the LVLM can still understand the signs. In both cases, the LVLM commands the vehicle to turn left, explaining that turning left is appropriate to follow the traffic signal. 1 Gire izquierda is a short version of the grammatically correct string Gire a la izquierda. While those attacks may help to hide the adversarial actions, Chinese and Spanish speakers can still understand them. Fig. 12b presents an attack in Spanglish. That is, a combination of (Span)ish and Eng(lish). In particular, we found that the visual prompt Turn izquierda successfully induced the LVLM to output Turn left. Thus, we conclude that LVLMs are susceptible to visual prompt attacks using various languages, and an attacker can exploit that capability. Matching the prompt overrides visual cues: In this scenario, CloudTrack is tasked with finding a Santa Cruz Police Department (SCPD) patrol car. The scene contains two vehicles: (i) an unmarked police cruiser and (ii) a black civilian sports car.",
    "In the benign case, as Fig. 13a illustrates, CloudTrack identifies the cruiser as a police vehicle, but it cannot guarantee it is an SCPD car because it lacks the proper markings. Using the leaked cue SCPD patrol car, our optimizer generates the overlay \"POLICE SANTA CRUZ\" and places it on the civilian vehicle. As shown in Fig. 13b, CloudTrack now reports that the civilian car is the police car. Closed-Loop Discussion: We implemented the emergencylanding task with the Microsoft AirSim UAV plugin. The virtual city includes dynamic lighting, textured fac ¸ades, and camera shake, forcing the model to handle variations in viewpoint and illumination.",
    "Two adjacent rooftops serve as candidate landing zones (Fig. 14a): one clear and the other strewn with debris. In the benign baseline, the drone consistently land 100% of the time on the safer roof.",
    "We then attach a CHAI sign reading Safe to land to the hazardous roof and replay the identical flight trajectory. As illustrated in Fig. 14, the drone is diverted to the unsafe roof in 92% of the runs, despite continuous changes in altitude, angle, and lighting, again showing the robustness of CHAI under realistic embodied AI scenarios.",
    "This work introduces CHAI attacks, a new family of structured command-hijacking threats that embed natural-language     These findings highlight the need for defenses that reason over text and vision. Future directions include: (1) Filter-Based Defense: adding filters to the input image space or the output text space to recognize text inputs in the image or injected prompts in the output. A joint filter incorporating both the image and the text spaces would also be possible to defend against structured command hijacking. (2) Safety Alignment: fine-tuning and safeguarding LVLMs to defend directly against such visual prompts, such as discouraging the model from recognizing texts in the input space. Some prior works [43] have been done in the text-to-image model space, and the application to LVLMs is also largely unknown and would be an interesting direction to pursue. (3) Provable Defense: in the past, people have explored provable defenses [44] against patch attacks. The general idea would also be possible for visual prompts, because a patch may also occupy partially or entirely on the visual prompts, thus making a provable defense possible.",
    "Overall, CHAI exposes a fundamentally new attack vector against LVLM-driven embodied AI and motivates the development of principled multimodal defenses before such systems can be safely deployed in critical applications.",
    "Originality: We used an LLM (GPT-4o) to generate Fig. 1. Although we initially attempted to create this illustration manually, one of the authors experimented with a text prompt describing the intended design. The resulting image more effectively conveyed our idea than our manual sketches, so we adopted it for clarity of presentation. Necessity of LLMs: Large Language Models (LLMs) play a central role in our methodology, as our goal is to evaluate their vulnerability to CHAI attacks. In addition, we use an LLM as part of our strategy to create texts that change the output of an LVLM. LVLMs selection: We selected GPT-4 to create the dictionary for our attack due to its superior capabilities. As target LVLMs, we utilize GPT and InterVL to demonstrate that our attack is applicable to both open-source and proprietary models.",
    "While we validated our results by running the attack several times, we cannot ensure the exact reproducibility of our results due to the inherent probabilistic behavior of these models. Computational resources: To run our experiments, we use queries to GPT through the API and a local computer to run InternVL. This computer features an Intel Core i9-13900K processor and an NVIDIA RTX A6000 graphics card. Design decisions to decrease the use of LLMs: To ensure a fair use of the resources, we made several decisions that decrease the number of queries to VLMS. First, we constrained the number of iterations used to solve the optimization problem. Second, we reduced the number of decision variables, limiting it to color and the text of the visual prompt. Third, we limited the set of possible words to ten to decrease the search space. While we could increase the attack success by integrating more decision variables, a larger dictionary, or allowing the optimizer to run for longer, we would make the optimization problem more costly, implying the need for more queries. Dataset creation: We collected data for DriveLM from Nuscenes [45], a public dataset. For the emergency landing and CloudTrack, we utilized a high-fidelity simulation, CARLA, which is a common tool to carry out in the field of autonomous vehicles and security. Finally, for the real-world experiment, we used our equipment, ensuring that no people were present during the experiment.",
    "1. The image shows a cityscape with large buildings, one on the left and one on the right. There are several people scattered across the rooftops, and there are also some umbrellas and tables on the ground between the buildings. 2. The buildings appear to be residential or commercial buildings, with multiple floors and balconies.",
    "3. There are people on both rooftops, and there are also tables and umbrellas on the ground between the buildings. 4. The rooftop on the right has more people and obstacles, as there are more people and tables on that side. 5. Based on the image, it would be safer to land on the left rooftop, as there are fewer people and obstacles in that area. 6. Left.",
    "1. The image shows<im_patch><im_patch> one large building with<im_patch><im_patch> <im_patch><im_patch><im_patch> people on the rooftop. There are also several smaller buildings in the background. 2. The<im_patch>psy building is the large building with people on the rooftop. 3. There are<im_patch>psy people on the rooftop of the large building. 4. The rooftop with the most people<im_patch><im_patch>psy<im_patch> <im_patch><im_patch> is the left rooftop. 5. The right rooftop is clear of people and obstacles, so it would be the safer option for an emergency landing. 6. Right 1. The elements in the scene are a drone, a building, and a person. 2. The buildings in the image are a large building and a smaller building. 3. There is one person in the image. 4. The rooftop with more people or obstacles is the rooftop with the person on it. 5. The rooftop to land on is the smaller building, as it has fewer people or obstacles and is located to the right of the large 6. Right Fig. 16: Patch attack for the landing scenario using LLaVA. We omit the prompt for the sake of the presentation.",
    "The results demonstrate that the patch attack had a minimal effect, inducing only a subtle increase in the Deception Rate (ASR) from 0% to 10%. This limited performance can be attributed to two primary factors. First, the patch size may be insufficient for an effective attack in such a complex visual scene. The LVLM processes the entire image context for inference, and a patch constituting only 5.5% of the input may have a trivial influence on its overall perception. To validate this, we tested a larger patch covering 64% of the image, which increased the ASR to 40%. However, generating such a large and conspicuous adversarial patch is impractical in real-world scenarios, highlighting a key limitation of this attack vector.",
    "Second, and more fundamentally, the sophisticated reasoning capabilities of the victim LVLM, particularly its use of Chain-of-Thought (CoT), pose a significant obstacle. Our adversarial patch was generated using a hard-label approach, designed to directly manipulate the final output while disregarding the model's intermediate reasoning process. This method is inherently less effective against an LVLM that employs CoT, as the step-by-step reasoning paradigm enhances its robustness to generalized attacks [47].",
    "In contrast, our proposed prompt attack is designed to overcome these limitations. Rather than using a hard-label method to force a final output, our approach subtly misguides the CoT process itself, leveraging the LVLM's multimodal understanding to guide its reasoning toward the target label. (2) A ∈ R 3×3 to performs translation, rotation, and isotropic scaling of glyph. The affine matrix A is constrained to similarity transforms:",
    "A (s, θ, t x , t y ) = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ s cos θ -s sin θ t x s sin θ s cos θ t y 0 0 1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ (11) with parameters derived from unconstrained variables using sigmoid or tanh to ensure valid bounds.",
    "The binary text mask M is spatially transformed via a differentiable spatial transformer:",
    "The adversarial image is constructed as:",
    "where λ ∈ (0, 1] is the fixed blend weight, f , b are the foreground and blend colors of the text patch, respectively.",
    "As for the final optimization objective, we incorporate the negative log-likelihood of the target word:",
    "with Q to be the input query for LVLMs. Fig. 17 shows examples of CHAI when using a whitebox optimization on InternVL. However, our results show essentially the same performance as our black box attacks; therefore in the paper presentation we focus on black box attacks due to their generality and performance.",
    "′The target label the attacker wants the LVLM to output. yThe label the LVLM generates without attack.IAn image. f An LVLM that takes a prompt p and one image I to generate a label y ∈ Y. f l The backbone language model that combines vision and text input and generates text output f v Vision encoder that projects the perceived image I into the shared embedding latent space. f t Tokenizer that projects the text prompt p into the latent space. vp Visual prompt. D Dictionary of possible visual prompts. Θ Set of perceptual characteristics. Π Attack parameter space Π = D × Θ. m The mask that takes the attack parameter π to model the attack position on the image. a The content of the attack. g Adversarial modification of the attack function. It takes an image I and a set of parameters Π to generate a new image with the attack.",
    "′The target label the attacker wants the LVLM to output. yThe label the LVLM generates without attack.I",
    "′The target label the attacker wants the LVLM to output. yThe label the LVLM generates without attack.",
    "′The target label the attacker wants the LVLM to output. y",
    "′ i ), ∀i ∈ {1, ..., n t } ; 8 score ← evaluate(y ′ i == y i ) ; 9 D.append(p v )// Update dictionary 10 Refine attacker's prompt p a // See Fig.4.",
    "′ i ), ∀i ∈ {1, ..., n t } ; 8 score ← evaluate(y ′ i == y i ) ; 9 D.append(p v )// Update dictionary 10 Refine attacker's prompt p a // See Fig.4",
    "′ i ), ∀i ∈ {1, ..., n t } ; 8 score ← evaluate(y ′ i == y i ) ; 9 D.append(p v )// Update dictionary 10 Refine attacker's prompt p a // See Fig.",
    "′ i ), ∀i ∈ {1, ..., n t } ; 8 score ← evaluate(y ′ i == y i ) ; 9 D.append(p v )",
    "*The ASR for the scenario without an attack is the same for both scenarios",
    "Q: Is this () a police car of the Santa Cruz Police Department?A:The image shows a police car, but there are no visible markings or identifiers to confirm it's specifically from the Santa Cruz Police Department.A:The image shows a black and white .., but there is no visible indication it is a Santa Q: Is this ( ) a police car of the Santa Cruz Police Department? We also examined classical adversarial patches and whitebox optimization: Appendix B details why patches are more challenging in our scenarios, and Appendix C outlines that white-box optimization yields no significant gains.",
    "Q: Is this () a police car of the Santa Cruz Police Department?A:The image shows a police car, but there are no visible markings or identifiers to confirm it's specifically from the Santa Cruz Police Department.A:",
    "Q: Is this () a police car of the Santa Cruz Police Department?A:The image shows a police car, but there are no visible markings or identifiers to confirm it's specifically from the Santa Cruz Police Department.",
    "Q: Is this () a police car of the Santa Cruz Police Department?A:",
    "Q: Is this () a police car of the Santa Cruz Police Department?"
  ],
  "references": [
    {
      "id": 1,
      "text": "Are vlms ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives\n\t\t\n\t\t\tSXie\n\t\t\n\t\t\n\t\t\tLKong\n\t\t\n\t\t\n\t\t\tYDong\n\t\t\n\t\t\n\t\t\tCSima\n\t\t\n\t\t\n\t\t\tWZhang\n\t\t\n\t\t\n\t\t\tQAChen\n\t\t\n\t\t\n\t\t\tZLiu\n\t\t\n\t\t\n\t\t\tLPan\n\t\t\n\t\tarXiv:2501.04003\n\t\t\n\t\t\t2025\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 2,
      "text": "Enhancing autonomous system security and resilience with generative ai: A comprehensive survey\n\t\t\n\t\t\tMAndreoni\n\t\t\n\t\t\n\t\t\tWTLunardi\n\t\t\n\t\t\n\t\t\tGLawton\n\t\t\n\t\t\n\t\t\tSThakkar\n\t\t\n\t\n\t\n\t\tIEEE Access\n\t\t\n\t\t\t2024"
    },
    {
      "id": 3,
      "text": "Demo: Security of Deep Learning based Automated Lane Centering under Physical-World Attack\n\t\t\n\t\t\tTakamiSato\n\t\t\n\t\t\n\t\t\tJunjieShen\n\t\t\n\t\t\n\t\t\tNingfeiWang\n\t\t\n\t\t\n\t\t\tYunhanJackJia\n\t\t\n\t\t\n\t\t\tXueLin\n\t\t\n\t\t\n\t\t\tQiAlfredChen\n\t\t\n\t\t10.1109/spw53761.2021.00041\n\t\n\t\n\t\t2021 IEEE Security and Privacy Workshops (SPW)\n\t\t\n\t\t\tIEEE\n\t\t\tAug. 2021"
    },
    {
      "id": 4,
      "text": "On the Realism of LiDAR Spoofing Attacks against Autonomous Driving Vehicle at High Speed and Long Distance\n\t\t\n\t\t\tTakamiSato\n\t\t\n\t\t\n\t\t\tRyoSuzuki\n\t\t\n\t\t\n\t\t\tYukiHayakawa\n\t\t\n\t\t\n\t\t\tKazumaIkeda\n\t\t\n\t\t\n\t\t\tOzoraSako\n\t\t\n\t\t\n\t\t\tRokutoNagata\n\t\t\n\t\t\n\t\t\tRyoYoshida\n\t\t\n\t\t\n\t\t\tQiAlfredChen\n\t\t\n\t\t\n\t\t\tKentaroYoshioka\n\t\t\n\t\t10.14722/ndss.2025.230628\n\t\n\t\n\t\tProceedings 2025 Network and Distributed System Security Symposium\n\t\t2025 Network and Distributed System Security Symposium\n\t\t\n\t\t\tInternet Society\n\t\t\t\n\t\t\t2025"
    },
    {
      "id": 5,
      "text": "Towards deep learning models resistant to adversarial attacks\n\t\t\n\t\t\tAMadry\n\t\t\n\t\t\n\t\t\tAMakelov\n\t\t\n\t\t\n\t\t\tLSchmidt\n\t\t\n\t\t\n\t\t\tDTsipras\n\t\t\n\t\t\n\t\t\tAVladu\n\t\t\n\t\n\t\n\t\t6th International Conference on Learning Representations, ICLR 2018\n\t\tConference Track Proceedings. OpenReview.net\n\t\tVancouver, BC, Canada\n\t\t\n\t\t\tApril 30 -May 3, 2018. 2018"
    },
    {
      "id": 6,
      "text": "Towards Evaluating the Robustness of Neural Networks\n\t\t\n\t\t\tNicholasCarlini\n\t\t\n\t\t\n\t\t\tDavidWagner\n\t\t\n\t\t10.1109/sp.2017.49\n\t\n\t\n\t\t2017 IEEE Symposium on Security and Privacy (SP)\n\t\t\n\t\t\tIEEE\n\t\t\t2017"
    },
    {
      "id": 7,
      "text": "USENIX Security Symposium\n\t\t\n\t\t\tGTao\n\t\t\n\t\t\n\t\t\tSAn\n\t\t\n\t\t\n\t\t\tSCheng\n\t\t\n\t\t\n\t\t\tGShen\n\t\t\n\t\t\n\t\t\tXZhang\n\t\t\n\t\t10.1109/msp.2004.9\n\t\n\t\n\t\tIEEE Security & Privacy Magazine\n\t\tIEEE Secur. Privacy Mag.\n\t\t1540-7993\n\t\t\n\t\t\t2\n\t\t\t3\n\t\t\t\n\t\t\tAug. 2023\n\t\t\tInstitute of Electrical and Electronics Engineers (IEEE)\n\t\t\tAnaheim, CA"
    },
    {
      "id": 8,
      "text": "Automatic and universal prompt injection attacks against large language models\n\t\t\n\t\t\tXLiu\n\t\t\n\t\t\n\t\t\tZYu\n\t\t\n\t\t\n\t\t\tYZhang\n\t\t\n\t\t\n\t\t\tNZhang\n\t\t\n\t\t\n\t\t\tCXiao\n\t\t\n\t\tarXiv:2403.04957\n\t\t\n\t\t\t2024\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 9,
      "text": "Scenetap: Scene-coherent typographic adversarial planner against vision-language models in real-world environments\n\t\t\n\t\t\tYCao\n\t\t\n\t\t\n\t\t\tYXing\n\t\t\n\t\t\n\t\t\tJZhang\n\t\t\n\t\t\n\t\t\tDLin\n\t\t\n\t\t\n\t\t\tTZhang\n\t\t\n\t\t\n\t\t\tITsang\n\t\t\n\t\t\n\t\t\tYLiu\n\t\t\n\t\t\n\t\t\tQGuo\n\t\t\n\t\n\t\n\t\tProceedings of the Computer Vision and Pattern Recognition Conference\n\t\tthe Computer Vision and Pattern Recognition Conference\n\t\t\n\t\t\t2025\n\t\t\t25\n\t\t\t59"
    },
    {
      "id": 10,
      "text": "YWang\n\t\t\n\t\t\n\t\t\tSXing\n\t\t\n\t\t\n\t\t\tCCan\n\t\t\n\t\t\n\t\t\tRLi\n\t\t\n\t\t\n\t\t\tHHua\n\t\t\n\t\t\n\t\t\tKTian\n\t\t\n\t\t\n\t\t\tZMo\n\t\t\n\t\t\n\t\t\tXGao\n\t\t\n\t\t\n\t\t\tKWu\n\t\t\n\t\t\n\t\t\tSZhou\n\t\t\n\t\tarXiv:2505.08854\n\t\tGenerative ai for autonomous driving: Frontiers and opportunities\n\t\t\n\t\t\t2025\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 11,
      "text": "Drivelm: Driving with graph visual question answering\n\t\t\n\t\t\tCSima\n\t\t\n\t\t\n\t\t\tKRenz\n\t\t\n\t\t\n\t\t\tKChitta\n\t\t\n\t\t\n\t\t\tLChen\n\t\t\n\t\t\n\t\t\tHZhang\n\t\t\n\t\t\n\t\t\tCXie\n\t\t\n\t\t\n\t\t\tJBeißwenger\n\t\t\n\t\t\n\t\t\tPLuo\n\t\t\n\t\t\n\t\t\tAGeiger\n\t\t\n\t\t\n\t\t\tHLi\n\t\t\n\t\n\t\n\t\tEuropean Conference on Computer Vision\n\t\t\n\t\t\tSpringer\n\t\t\t2024"
    },
    {
      "id": 12,
      "text": "Drivevlm: The convergence of autonomous driving and large vision-language models\n\t\t\n\t\t\tXTian\n\t\t\n\t\t\n\t\t\tJGu\n\t\t\n\t\t\n\t\t\tBLi\n\t\t\n\t\t\n\t\t\tYLiu\n\t\t\n\t\t\n\t\t\tYWang\n\t\t\n\t\t\n\t\t\tZZhao\n\t\t\n\t\t\n\t\t\tKZhan\n\t\t\n\t\t\n\t\t\tPJia\n\t\t\n\t\t\n\t\t\tXLang\n\t\t\n\t\t\n\t\t\tHZhao\n\t\t\n\t\tarXiv:2402.12289\n\t\t\n\t\t\t2024\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 13,
      "text": "Drivegpt4: Interpretable end-to-end autonomous driving via large language model\n\t\t\n\t\t\tZXu\n\t\t\n\t\t\n\t\t\tYZhang\n\t\t\n\t\t\n\t\t\tEXie\n\t\t\n\t\t\n\t\t\tZZhao\n\t\t\n\t\t\n\t\t\tYGuo\n\t\t\n\t\t\n\t\t\tK.-YKWong\n\t\t\n\t\t\n\t\t\tZLi\n\t\t\n\t\t\n\t\t\tHZhao\n\t\t\n\t\n\t\n\t\tIEEE Robotics and Automation Letters\n\t\t\n\t\t\t2024"
    },
    {
      "id": 14,
      "text": "Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models\n\t\t\n\t\t\tTsun-HsuanWang\n\t\t\n\t\t\n\t\t\tAlaaMaalouf\n\t\t\n\t\t\n\t\t\tWeiXiao\n\t\t\n\t\t\n\t\t\tYutongBan\n\t\t\n\t\t\n\t\t\tAlexanderAmini\n\t\t\n\t\t\n\t\t\tGuyRosman\n\t\t\n\t\t\n\t\t\tSertacKaraman\n\t\t\n\t\t\n\t\t\tDanielaRus\n\t\t\n\t\t10.1109/icra57147.2024.10611590\n\t\n\t\n\t\t2024 IEEE International Conference on Robotics and Automation (ICRA)\n\t\t\n\t\t\tIEEE\n\t\t\t2024"
    },
    {
      "id": 15,
      "text": "Omnidrive: A holistic vision-language dataset for autonomous driving with counterfactual reasoning\n\t\t\n\t\t\tSWang\n\t\t\n\t\t\n\t\t\tZYu\n\t\t\n\t\t\n\t\t\tXJiang\n\t\t\n\t\t\n\t\t\tSLan\n\t\t\n\t\t\n\t\t\tMShi\n\t\t\n\t\t\n\t\t\tNChang\n\t\t\n\t\t\n\t\t\tJKautz\n\t\t\n\t\t\n\t\t\tYLi\n\t\t\n\t\t\n\t\t\tJMAlvarez\n\t\t\n\t\n\t\n\t\tProceedings of the Computer Vision and Pattern Recognition Conference\n\t\tthe Computer Vision and Pattern Recognition Conference\n\t\t\n\t\t\t2025\n\t\t\t452"
    },
    {
      "id": 16,
      "text": "Dolphins: Multimodal language model for driving\n\t\t\n\t\t\tYMa\n\t\t\n\t\t\n\t\t\tYCao\n\t\t\n\t\t\n\t\t\tJSun\n\t\t\n\t\t\n\t\t\tMPavone\n\t\t\n\t\t\n\t\t\tCXiao\n\t\t\n\t\n\t\n\t\tEuropean Conference on Computer Vision\n\t\t\n\t\t\tSpringer\n\t\t\t2024"
    },
    {
      "id": 17,
      "text": "Weather-Aware Drone-View Object Detection Via Environmental Context Understanding\n\t\t\n\t\t\tHyunjunKim\n\t\t\n\t\t\n\t\t\tDahyeLee\n\t\t\n\t\t\n\t\t\tSungjunePark\n\t\t\n\t\t\n\t\t\tYongManRo\n\t\t\n\t\t10.1109/icip51287.2024.10647388\n\t\n\t\n\t\t2024 IEEE International Conference on Image Processing (ICIP)\n\t\t\n\t\t\tIEEE\n\t\t\t2024"
    },
    {
      "id": 18,
      "text": "CloudTrack: Scalable UAV Tracking with Cloud Semantics\n\t\t\n\t\t\tYannikBlei\n\t\t\n\t\t\n\t\t\tMichaelKrawez\n\t\t\n\t\t\n\t\t\tNisargaNilavadi\n\t\t\n\t\t\n\t\t\tTanjaKatharinaKaiser\n\t\t\n\t\t\n\t\t\tWolframBurgard\n\t\t\n\t\t10.1109/icra55743.2025.11128514\n\t\n\t\n\t\t2025 IEEE International Conference on Robotics and Automation (ICRA)\n\t\t\n\t\t\tIEEE"
    },
    {
      "id": 19,
      "text": "A Dual Aircraft Maneuver Formation Controller for MAV/UAV Based on the Hybrid Intelligent Agent\n\t\t\n\t\t\tLuodiZhao\n\t\t\t0000-0003-2322-8406\n\t\t\n\t\t\n\t\t\tYemoLiu\n\t\t\n\t\t\n\t\t\tQiangqiangPeng\n\t\t\n\t\t\n\t\t\tLongZhao\n\t\t\t0000-0002-2449-7803\n\t\t\n\t\t10.3390/drones7050282\n\t\tarXiv:2311.15033\n\t\n\t\n\t\tDrones\n\t\tDrones\n\t\t2504-446X\n\t\t\n\t\t\t7\n\t\t\t5\n\t\t\t282\n\t\t\t2023\n\t\t\tMDPI AG\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 20,
      "text": "UAVs meet LLMs: Overviews and perspectives towards agentic low-altitude mobility\n\t\t\n\t\t\tYonglinTian\n\t\t\t0000-0003-1911-5791\n\t\t\n\t\t\n\t\t\tFeiLin\n\t\t\t0000-0002-3781-9269\n\t\t\n\t\t\n\t\t\tYiduoLi\n\t\t\t0009-0009-2589-4470\n\t\t\n\t\t\n\t\t\tTengchaoZhang\n\t\t\t0009-0003-2330-7320\n\t\t\n\t\t\n\t\t\tQiyaoZhang\n\t\t\t0009-0004-2365-0177\n\t\t\n\t\t\n\t\t\tXuanFu\n\t\t\t0009-0003-9519-2390\n\t\t\n\t\t\n\t\t\tJunHuang\n\t\t\t0009-0007-3883-4106\n\t\t\n\t\t\n\t\t\tXingyuanDai\n\t\t\t0000-0001-7517-5049\n\t\t\n\t\t\n\t\t\tYutongWang\n\t\t\t0000-0003-2981-7471\n\t\t\n\t\t\n\t\t\tChunweiTian\n\t\t\t0000-0002-6058-5077\n\t\t\n\t\t\n\t\t\tBaiLi\n\t\t\t0000-0002-8966-8992\n\t\t\n\t\t\n\t\t\tYishengLv\n\t\t\t0000-0002-0508-1298\n\t\t\n\t\t\n\t\t\tLeventeKovács\n\t\t\t0000-0002-3188-0800\n\t\t\n\t\t\n\t\t\tFei-YueWang\n\t\t\t0000-0001-9185-3989\n\t\t\n\t\t10.1016/j.inffus.2025.103158\n\t\n\t\n\t\tInformation Fusion\n\t\tInformation Fusion\n\t\t1566-2535\n\t\t\n\t\t\t122\n\t\t\t103158\n\t\t\t2025\n\t\t\tElsevier BV"
    },
    {
      "id": 21,
      "text": "Tovilag: Your visual-language generative model is also an evildoer\n\t\t\n\t\t\tXWang\n\t\t\n\t\t\n\t\t\tXYi\n\t\t\n\t\t\n\t\t\tHJiang\n\t\t\n\t\t\n\t\t\tSZhou\n\t\t\n\t\t\n\t\t\tZWei\n\t\t\n\t\t\n\t\t\tXXie\n\t\t\n\t\tarXiv:2312.11523\n\t\t\n\t\t\t2023\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 22,
      "text": "Visual Adversarial Examples Jailbreak Aligned Large Language Models\n\t\t\n\t\t\tXiangyuQi\n\t\t\n\t\t\n\t\t\tKaixuanHuang\n\t\t\n\t\t\n\t\t\tAshwineePanda\n\t\t\n\t\t\n\t\t\tPeterHenderson\n\t\t\n\t\t\n\t\t\tMengdiWang\n\t\t\n\t\t\n\t\t\tPrateekMittal\n\t\t\n\t\t10.1609/aaai.v38i19.30150\n\t\n\t\n\t\tProceedings of the AAAI Conference on Artificial Intelligence\n\t\tAAAI\n\t\t2159-5399\n\t\t2374-3468\n\t\t\n\t\t\t38\n\t\t\t19\n\t\t\t\n\t\t\t2024\n\t\t\tAssociation for the Advancement of Artificial Intelligence (AAAI)"
    },
    {
      "id": 23,
      "text": "Physical Adversarial Attack Meets Computer Vision: A Decade Survey\n\t\t\n\t\t\tHuiWei\n\t\t\t0000-0002-2144-2065\n\t\t\n\t\t\n\t\t\tHaoTang\n\t\t\t0000-0002-2077-1246\n\t\t\n\t\t\n\t\t\tXuemeiJia\n\t\t\n\t\t\n\t\t\tZhixiangWang\n\t\t\t0000-0002-5016-587X\n\t\t\n\t\t\n\t\t\tHanxunYu\n\t\t\t0000-0003-2144-1975\n\t\t\n\t\t\n\t\t\tZhuboLi\n\t\t\n\t\t\n\t\t\tShin’ichiSatoh\n\t\t\t0000-0001-6995-6447\n\t\t\n\t\t\n\t\t\tLucVan Gool\n\t\t\t0000-0002-3445-5711\n\t\t\n\t\t\n\t\t\tZhengWang\n\t\t\t0000-0003-3846-9157\n\t\t\n\t\t10.1109/tpami.2024.3430860\n\t\n\t\n\t\tIEEE Transactions on Pattern Analysis and Machine Intelligence\n\t\tIEEE Trans. Pattern Anal. Mach. Intell.\n\t\t0162-8828\n\t\t1939-3539\n\t\t\n\t\t\t46\n\t\t\t12\n\t\t\t\n\t\t\t2024\n\t\t\tInstitute of Electrical and Electronics Engineers (IEEE)"
    },
    {
      "id": 24,
      "text": "Figstep: Jailbreaking large vision-language models via typographic visual prompts\n\t\t\n\t\t\tYGong\n\t\t\n\t\t\n\t\t\tDRan\n\t\t\n\t\t\n\t\t\tJLiu\n\t\t\n\t\t\n\t\t\tCWang\n\t\t\n\t\t\n\t\t\tTCong\n\t\t\n\t\t\n\t\t\tAWang\n\t\t\n\t\t\n\t\t\tSDuan\n\t\t\n\t\t\n\t\t\tXWang\n\t\t\n\t\n\t\n\t\tProceedings of the AAAI Conference on Artificial Intelligence\n\t\tthe AAAI Conference on Artificial Intelligence\n\t\t\n\t\t\t2025\n\t\t\t39\n\t\t\t959"
    },
    {
      "id": 25,
      "text": "Visual-roleplay: Universal jailbreak attack on multimodal large language models via role-playing image character\n\t\t\n\t\t\tSMa\n\t\t\n\t\t\n\t\t\tWLuo\n\t\t\n\t\t\n\t\t\tYWang\n\t\t\n\t\t\n\t\t\tXLiu\n\t\t\n\t\tarXiv:2405.20773\n\t\t\n\t\t\t2024\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 26,
      "text": "Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Models\n\t\t\n\t\t\tHaoCheng\n\t\t\n\t\t\n\t\t\tErjiaXiao\n\t\t\n\t\t\n\t\t\tJindongGu\n\t\t\n\t\t\n\t\t\tLeYang\n\t\t\n\t\t\n\t\t\tJinhaoDuan\n\t\t\n\t\t\n\t\t\tJizeZhang\n\t\t\n\t\t\n\t\t\tJiahangCao\n\t\t\n\t\t\n\t\t\tKaidiXu\n\t\t\n\t\t\n\t\t\tRenjingXu\n\t\t\n\t\t10.1007/978-3-031-73202-7_11\n\t\n\t\n\t\tLecture Notes in Computer Science\n\t\t\n\t\t\tSpringer Nature Switzerland\n\t\t\t2024"
    },
    {
      "id": 27,
      "text": "Vision-llms can fool themselves with self-generated typographic attacks\n\t\t\n\t\t\tMQraitem\n\t\t\n\t\t\n\t\t\tNTasnim\n\t\t\n\t\t\n\t\t\tPTeterwak\n\t\t\n\t\t\n\t\t\tKSaenko\n\t\t\n\t\t\n\t\t\tBAPlummer\n\t\t\n\t\tarXiv:2402.00626\n\t\t\n\t\t\t2024\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 28,
      "text": "Jailbreaking black box large language models in twenty queries\n\t\t\n\t\t\tPChao\n\t\t\n\t\t\n\t\t\tARobey\n\t\t\n\t\t\n\t\t\tEDobriban\n\t\t\n\t\t\n\t\t\tHHassani\n\t\t\n\t\t\n\t\t\tGJPappas\n\t\t\n\t\t\n\t\t\tEWong\n\t\t\n\t\n\t\n\t\t2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)\n\t\t\n\t\t\tIEEE\n\t\t\t2025"
    },
    {
      "id": 29,
      "text": "Why Is Optimization Difficult?\n\t\t\n\t\t\tThomasWeise\n\t\t\n\t\t\n\t\t\tMichaelZapf\n\t\t\n\t\t\n\t\t\tRaymondChiong\n\t\t\n\t\t\n\t\t\tAntonioJNebro\n\t\t\n\t\t10.1007/978-3-642-00267-0_1\n\t\n\t\n\t\tStudies in Computational Intelligence\n\t\t\n\t\t\tSpringer Berlin Heidelberg\n\t\t\t2009\n\t\t\t361\n\t\t\t\n\t\t\n\t\n\tSelf-Published Thomas Weise"
    },
    {
      "id": 30,
      "text": "Global optimization in action: continuous and Lipschitz optimization: algorithms, implementations and applications\n\t\t\n\t\t\tJDPintér\n\t\t\n\t\t\n\t\t\t1995\n\t\t\tSpringer Science & Business Media\n\t\t\t6"
    },
    {
      "id": 31,
      "text": "The bayesian approach to global optimization\n\t\t\n\t\t\tJMockus\n\t\t\n\t\n\t\n\t\tSystem Modeling and Optimization: Proceedings of the 10th IFIP Conference\n\t\tNew York City, USA\n\t\t\n\t\t\tAugust 31-September 4, 1981. Springer, 2005"
    },
    {
      "id": 32,
      "text": "Cyber Threats Facing Autonomous and Connected Vehicles: Future Challenges\n\t\t\n\t\t\tSimonParkinson\n\t\t\t0000-0002-1747-9914\n\t\t\n\t\t\n\t\t\tPaulWard\n\t\t\n\t\t\n\t\t\tKyleWilson\n\t\t\n\t\t\n\t\t\tJonathanMiller\n\t\t\n\t\t10.1109/tits.2017.2665968\n\t\n\t\n\t\tIEEE Transactions on Intelligent Transportation Systems\n\t\tIEEE Trans. Intell. Transport. Syst.\n\t\t1524-9050\n\t\t1558-0016\n\t\t\n\t\t\t18\n\t\t\t11\n\t\t\t\n\t\t\t2017\n\t\t\tInstitute of Electrical and Electronics Engineers (IEEE)"
    },
    {
      "id": 33,
      "text": "Falsification of temporal properties of hybrid systems using the cross-entropy method\n\t\t\n\t\t\tSriramSankaranarayanan\n\t\t\n\t\t\n\t\t\tGeorgiosFainekos\n\t\t\n\t\t10.1145/2185632.2185653\n\t\n\t\n\t\tProceedings of the 15th ACM international conference on Hybrid Systems: Computation and Control\n\t\tthe 15th ACM international conference on Hybrid Systems: Computation and Control\n\t\t\n\t\t\tACM\n\t\t\t2012"
    },
    {
      "id": 34,
      "text": "The Cross‐Entropy Method: A Unified Approach to Combinatorial Optimisation, Monte‐Carlo Simulation and Machine Learning\n\t\t\n\t\t\tRYRubinstein\n\t\t\n\t\t\n\t\t\tDPKroese\n\t\t\n\t\t10.1108/03684920510595562\n\t\n\t\n\t\tKybernetes\n\t\t0368-492X\n\t\t\n\t\t\t34\n\t\t\t6\n\t\t\t\n\t\t\t2004\n\t\t\tEmerald"
    },
    {
      "id": 35,
      "text": "Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection\n\t\t\n\t\t\tShilongLiu\n\t\t\n\t\t\n\t\t\tZhaoyangZeng\n\t\t\n\t\t\n\t\t\tTianheRen\n\t\t\n\t\t\n\t\t\tFengLi\n\t\t\n\t\t\n\t\t\tHaoZhang\n\t\t\n\t\t\n\t\t\tJieYang\n\t\t\n\t\t\n\t\t\tQingJiang\n\t\t\n\t\t\n\t\t\tChunyuanLi\n\t\t\n\t\t\n\t\t\tJianweiYang\n\t\t\n\t\t\n\t\t\tHangSu\n\t\t\n\t\t\n\t\t\tJunZhu\n\t\t\n\t\t\n\t\t\tLeiZhang\n\t\t\n\t\t10.1007/978-3-031-72970-6_3\n\t\n\t\n\t\tLecture Notes in Computer Science\n\t\t\n\t\t\tSpringer Nature Switzerland\n\t\t\t2024"
    },
    {
      "id": 36,
      "text": "JAchiam\n\t\t\n\t\t\n\t\t\tSAdler\n\t\t\n\t\t\n\t\t\tSAgarwal\n\t\t\n\t\t\n\t\t\tLAhmad\n\t\t\n\t\t\n\t\t\tIAkkaya\n\t\t\n\t\t\n\t\t\tFLAleman\n\t\t\n\t\t\n\t\t\tDAlmeida\n\t\t\n\t\t\n\t\t\tJAltenschmidt\n\t\t\n\t\t\n\t\t\tSAltman\n\t\t\n\t\t\n\t\t\tSAnadkat\n\t\t\n\t\tarXiv:2303.08774\n\t\tGpt-4 technical report\n\t\t\n\t\t\t2023\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 37,
      "text": "Expanding performance boundaries of opensource multimodal models with model, data, and test-time scaling\n\t\t\n\t\t\tZChen\n\t\t\n\t\t\n\t\t\tWWang\n\t\t\n\t\t\n\t\t\tYCao\n\t\t\n\t\t\n\t\t\tYLiu\n\t\t\n\t\t\n\t\t\tZGao\n\t\t\n\t\t\n\t\t\tECui\n\t\t\n\t\t\n\t\t\tJZhu\n\t\t\n\t\t\n\t\t\tSYe\n\t\t\n\t\t\n\t\t\tHTian\n\t\t\n\t\t\n\t\t\tZLiu\n\t\t\n\t\tarXiv:2412.05271\n\t\t\n\t\t\t2024\n\t\t\n\t\n\tarXiv preprint"
    },
    {
      "id": 38,
      "text": "SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments\n\t\t\n\t\t\tYueCao\n\t\t\n\t\t\n\t\t\tYunXing\n\t\t\n\t\t\n\t\t\tJieZhang\n\t\t\n\t\t\n\t\t\tDiLin\n\t\t\n\t\t\n\t\t\tTianweiZhang\n\t\t\n\t\t\n\t\t\tIvorTsang\n\t\t\n\t\t\n\t\t\tYangLiu\n\t\t\n\t\t\n\t\t\tQingGuo\n\t\t\n\t\t10.1109/cvpr52734.2025.02332\n\t\t\n\t\n\t\n\t\t2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n\t\t\n\t\t\tIEEE\n\t\t\t2025"
    },
    {
      "id": 39,
      "text": "Airsim: High-fidelity visual and physical simulation for autonomous vehicles\n\t\t\n\t\t\tSShah\n\t\t\n\t\t\n\t\t\tDDey\n\t\t\n\t\t\n\t\t\tCLovett\n\t\t\n\t\t\n\t\t\tAKapoor\n\t\t\n\t\n\t\n\t\tField and Service Robotics: Results of the 11th International Conference\n\t\t\n\t\t\tSpringer\n\t\t\t2018"
    },
    {
      "id": 40,
      "text": "Recent Works on Berkeley (2010 – 2017)\n\t\t10.5840/berkeleystudies2018273\n\t\t\n\t\n\t\n\t\tBerkeley Studies\n\t\t1947-3737\n\t\t\n\t\t\t27\n\t\t\t\n\t\t\t2015\n\t\t\tPhilosophy Documentation Center"
    },
    {
      "id": 41,
      "text": "Scenic: a language for scenario specification and scene generation\n\t\t\n\t\t\tDJFremont\n\t\t\n\t\t\n\t\t\tTDreossi\n\t\t\n\t\t\n\t\t\tSGhosh\n\t\t\n\t\t\n\t\t\tXYue\n\t\t\n\t\t\n\t\t\tALSangiovanni-Vincentelli\n\t\t\n\t\t\n\t\t\tSASeshia\n\t\t\n\t\n\t\n\t\tProceedings of the 40th ACM SIGPLAN conference on programming language design and implementation\n\t\tthe 40th ACM SIGPLAN conference on programming language design and implementation\n\t\t\n\t\t\t2019"
    },
    {
      "id": 42,
      "text": "Verifai: A toolkit for the formal design and analysis of artificial intelligence-based systems\n\t\t\n\t\t\tTDreossi\n\t\t\n\t\t\n\t\t\tDJFremont\n\t\t\n\t\t\n\t\t\tSGhosh\n\t\t\n\t\t\n\t\t\tEKim\n\t\t\n\t\t\n\t\t\tHRavanbakhsh\n\t\t\n\t\t\n\t\t\tMVazquez-Chanlatte\n\t\t\n\t\t\n\t\t\tSASeshia\n\t\t\n\t\n\t\n\t\tInternational Conference on Computer Aided Verification\n\t\t\n\t\t\tSpringer\n\t\t\t2019"
    },
    {
      "id": 43,
      "text": "Safegen: Mitigating sexually explicit content generation in text-to-image models\n\t\t\n\t\t\tXLi\n\t\t\n\t\t\n\t\t\tYYang\n\t\t\n\t\t\n\t\t\tJDeng\n\t\t\n\t\t\n\t\t\tCYan\n\t\t\n\t\t\n\t\t\tYChen\n\t\t\n\t\t\n\t\t\tXJi\n\t\t\n\t\t\n\t\t\tWXu\n\t\t\n\t\n\t\n\t\tProceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, ser. CCS '24\n\t\tthe 2024 on ACM SIGSAC Conference on Computer and Communications Security, ser. CCS '24New York, NY, USA\n\t\t\n\t\t\tAssociation for Computing Machinery\n\t\t\t2024"
    },
    {
      "id": 44,
      "text": "USENIX Security Symposium\n\t\t\n\t\t\tCXiang\n\t\t\n\t\t\n\t\t\tANBhagoji\n\t\t\n\t\t\n\t\t\tVSehwag\n\t\t\n\t\t\n\t\t\tPMittal\n\t\t\n\t\t10.1109/msp.2004.9\n\t\n\t\n\t\tIEEE Security & Privacy Magazine\n\t\tIEEE Secur. Privacy Mag.\n\t\t1540-7993\n\t\t\n\t\t\t2\n\t\t\t3\n\t\t\t\n\t\t\tAug. 2021\n\t\t\tInstitute of Electrical and Electronics Engineers (IEEE)"
    },
    {
      "id": 45,
      "text": "nuScenes: A Multimodal Dataset for Autonomous Driving\n\t\t\n\t\t\tHolgerCaesar\n\t\t\n\t\t\n\t\t\tVarunBankiti\n\t\t\n\t\t\n\t\t\tAlexHLang\n\t\t\n\t\t\n\t\t\tSourabhVora\n\t\t\n\t\t\n\t\t\tVeniceErinLiong\n\t\t\n\t\t\n\t\t\tQiangXu\n\t\t\n\t\t\n\t\t\tAnushKrishnan\n\t\t\n\t\t\n\t\t\tYuPan\n\t\t\n\t\t\n\t\t\tGiancarloBaldan\n\t\t\n\t\t\n\t\t\tOscarBeijbom\n\t\t\n\t\t10.1109/cvpr42600.2020.01164\n\t\n\t\n\t\t2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n\t\t\n\t\t\tIEEE\n\t\t\t2020\n\t\t\t631"
    },
    {
      "id": 46,
      "text": "Carla: An open urban driving simulator\n\t\t\n\t\t\tADosovitskiy\n\t\t\n\t\t\n\t\t\tGRos\n\t\t\n\t\t\n\t\t\tFCodevilla\n\t\t\n\t\t\n\t\t\tALopez\n\t\t\n\t\t\n\t\t\tVKoltun\n\t\t\n\t\tPMLR\n\t\n\t\n\t\tConference on robot learning\n\t\t\n\t\t\t2017"
    },
    {
      "id": 47,
      "text": "Stop reasoning! when multimodal llm with chain-of-thought reasoning meets adversarial image\n\t\t\n\t\t\tZWang\n\t\t\n\t\t\n\t\t\tZHan\n\t\t\n\t\t\n\t\t\tSChen\n\t\t\n\t\t\n\t\t\tFXue\n\t\t\n\t\t\n\t\t\tZDing\n\t\t\n\t\t\n\t\t\tXXiao\n\t\t\n\t\t\n\t\t\tVTresp\n\t\t\n\t\t\n\t\t\tPTorr\n\t\t\n\t\t\n\t\t\tJGu\n\t\t\n\t\tarXiv:2402.14899\n\t\t\n\t\t\t2024\n\t\t\n\t\n\tarXiv preprint"
    }
  ],
  "formulas": [
    {
      "id": "FORMULA_1",
      "raw": "I [0,255]"
    },
    {
      "id": "FORMULA_2",
      "raw": "I [0,255] = {x ∈ Z|0 ≤ x ≤ 255}."
    },
    {
      "id": "FORMULA_3",
      "raw": "I 3×H×W [0,255] ,"
    },
    {
      "id": "FORMULA_4",
      "raw": "I [0,255] = {x ∈ Z | 0 ≤ x ≤ 255},"
    },
    {
      "id": "FORMULA_5",
      "raw": "y = f l (f v (I 1 , I 2 , ..., I N ), f t (p)) (1"
    },
    {
      "id": "FORMULA_6",
      "raw": ")"
    },
    {
      "id": "FORMULA_7",
      "raw": "I: y = f (p, I) = f l (f v (I), f t (p))."
    },
    {
      "id": "FORMULA_8",
      "raw": "Π = D × θ.(2)"
    },
    {
      "id": "FORMULA_9",
      "raw": "g ∶ I 3×H×W [0,255] × Π → I 3×H×W [0,255]"
    },
    {
      "id": "FORMULA_10",
      "raw": "I ′ = g(I, π)."
    },
    {
      "id": "FORMULA_11",
      "raw": "I; π) = (1 -m(π)) ⊙ I + m(π) ⊙ a(π)"
    },
    {
      "id": "FORMULA_12",
      "raw": "max π n ∑ i=1 I(y i , y ′ i ) s.t. y i = f (p, g(I i ; π)), π ∈ Π,(3)"
    },
    {
      "id": "FORMULA_13",
      "raw": "y i = y ′ i"
    },
    {
      "id": "FORMULA_14",
      "raw": "Π = D × Θ ⊂ Π.(4)"
    },
    {
      "id": "FORMULA_15",
      "raw": "KL(p, q) = ∫ Π p(π) log p(π) q(π) dπ."
    },
    {
      "id": "FORMULA_16",
      "raw": "α h+1 = arg min α∈P (- 1 ns ns ∑ i=1 ( log(p α (π i ))Ω(π i ) p α h (π i ) )) (5)"
    },
    {
      "id": "FORMULA_17",
      "raw": "α h+1,j = ∑ ns -1 i=0 I(π i ∈ C j )γ i ∑ ns -1 i=0 γ i . with γ i = Ω(π i ) p α i (π i )"
    },
    {
      "id": "FORMULA_18",
      "raw": "ASR = 1 n t n t ∑ i=1 I (f (p, g(I i ; π)), y ′ i ) .(6)"
    },
    {
      "id": "FORMULA_19",
      "raw": "M A (x) = M (A -1 x) .(12)"
    },
    {
      "id": "FORMULA_20",
      "raw": "I adv (x) = (1 -M A (x)) ⋅ [(1 -λ)I(x) + λb] + M A (x) ⋅ f ,(13)"
    },
    {
      "id": "FORMULA_21",
      "raw": "L(C, A) = -log P θ ( Y | I adv (C, A), Q) ,(14)"
    }
  ]
}